<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 4 Generalization | Computational Psychology in Python</title>
<meta name="author" content="Alan Jern">
<meta name="description" content="In the last chapter, you learned that much of cognition is about making inferences. A common inference we‚Äôre faced with involves generalizing examples of things to new cases. A child hears a brand...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 4 Generalization | Computational Psychology in Python">
<meta property="og:type" content="book">
<meta property="og:description" content="In the last chapter, you learned that much of cognition is about making inferences. A common inference we‚Äôre faced with involves generalizing examples of things to new cases. A child hears a brand...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 4 Generalization | Computational Psychology in Python">
<meta name="twitter:description" content="In the last chapter, you learned that much of cognition is about making inferences. A common inference we‚Äôre faced with involves generalizing examples of things to new cases. A child hears a brand...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.9/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><link rel="apple-touch-icon" sizes="180x180" href="images/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicons/favicon-16x16.png">
<link rel="manifest" href="images/favicons/site.webmanifest">
<link rel="shortcut icon" href="images/favicons/favicon.ico">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="images/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Computational Psychology in Python</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Preface</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">2</span> Why computational modeling?</a></li>
<li><a class="" href="bayes.html"><span class="header-section-number">3</span> Bayesian inference</a></li>
<li><a class="active" href="generalization.html"><span class="header-section-number">4</span> Generalization</a></li>
<li><a class="" href="categorization.html"><span class="header-section-number">5</span> Categorization</a></li>
<li><a class="" href="hierarchical-generalization.html"><span class="header-section-number">6</span> Hierarchical generalization</a></li>
<li><a class="" href="sampling-assumptions.html"><span class="header-section-number">7</span> Sampling assumptions</a></li>
<li><a class="" href="pragmatics.html"><span class="header-section-number">8</span> Language pragmatics</a></li>
<li><a class="" href="social-cognition.html"><span class="header-section-number">9</span> Social cognition</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="generalization" class="section level1" number="4">
<h1>
<span class="header-section-number">4</span> Generalization<a class="anchor" aria-label="anchor" href="#generalization"><i class="fas fa-link"></i></a>
</h1>
<p>In the last chapter, you learned that much of cognition is about making inferences. A common inference we‚Äôre faced with involves generalizing examples of things to new cases.</p>
<ul>
<li>A child hears a brand new word and they have to figure out which objects to apply that word to.</li>
<li>You eat one candy from an assorted box and then try to guess what the others might taste like.</li>
<li>You remember that your friend liked doing crosswords during a weekend trip at the cabin one time, so you guess they might like a book of puzzles as a gift (generalizing their interests).</li>
</ul>
<p>How can we apply Bayesian inference to these kinds of problems?</p>
<div id="hormones" class="section level2" number="4.1">
<h2>
<span class="header-section-number">4.1</span> Healthy hormone levels üíâ<a class="anchor" aria-label="anchor" href="#hormones"><i class="fas fa-link"></i></a>
</h2>
<p>This example comes from a <a href="https://cocosci.princeton.edu/tom/papers/TGbbs.pdf">2001 paper by Josh Tenenbaum and Thomas Griffiths</a><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Tenenbaum, J. B. &amp;amp; Griffiths, T. L. (2001). Generalization, similarity, and Bayesian inference. Behavioral and Brain Sciences, 24, 629-640.&lt;/p&gt;"><sup>1</sup></a></p>
<p><strong>The basic problem</strong>: You learn the value of a healthy hormone level (say, 60) that varies on a scale from 1 to 100 (integers only). What is the probability that another value (say, 70) is also healthy?</p>
<div id="setting-up-a-model" class="section level3" number="4.1.1">
<h3>
<span class="header-section-number">4.1.1</span> Setting up a model<a class="anchor" aria-label="anchor" href="#setting-up-a-model"><i class="fas fa-link"></i></a>
</h3>
<div id="the-hypothesis-space" class="section level4" number="4.1.1.1">
<h4>
<span class="header-section-number">4.1.1.1</span> The hypothesis space<a class="anchor" aria-label="anchor" href="#the-hypothesis-space"><i class="fas fa-link"></i></a>
</h4>
<p>To start with, we‚Äôll assume that healthy values lie in a contiguous interval. Using the term from the paper, this interval is the <em>consequential region</em> <span class="math inline">\(C\)</span>.</p>
<p>The hypothesis space consists of all possible consequential regions. For example, [0,100], [10,19], and [44,45], are all valid hypotheses. The full hypothesis space is every valid interval between 0 and 100.</p>
</div>
<div id="prior" class="section level4" number="4.1.1.2">
<h4>
<span class="header-section-number">4.1.1.2</span> Prior<a class="anchor" aria-label="anchor" href="#prior"><i class="fas fa-link"></i></a>
</h4>
<p>How much weight should we assign to each hypothesis? We might have reason to favor shorter intervals over longer ones, for example. In the paper, they use an <a href="https://en.wikipedia.org/wiki/Erlang_distribution">Erlang prior</a>. Alternatively, for simplicity of calculation, we could again assume a uniform prior distribution, placing equal weight on all hypotheses, like we did in the previous chapter. This is tantamount to making no prior assumptions about which intervals are most probable.</p>
</div>
<div id="likelihood" class="section level4" number="4.1.1.3">
<h4>
<span class="header-section-number">4.1.1.3</span> Likelihood<a class="anchor" aria-label="anchor" href="#likelihood"><i class="fas fa-link"></i></a>
</h4>
<p>Suppose you learn that a healthy patient has a hormone level of 60. What was the likelihood of observing this value, assuming we know which hypothesis is correct? That is, what is <span class="math inline">\(P(x = 60 | h)\)</span>. It depends on how we assume the patient was chosen.</p>
<div id="weak-strong-sampling" class="section level5" number="4.1.1.3.1">
<h5>
<span class="header-section-number">4.1.1.3.1</span> Weak vs.¬†strong sampling<a class="anchor" aria-label="anchor" href="#weak-strong-sampling"><i class="fas fa-link"></i></a>
</h5>
<p>Under <em>weak sampling</em>, we assume that each observation was sampled from the full range of possibilities, and it was just a coincidence that we happened to get one from the consequential region (a healthy patient). If that‚Äôs true, then the probability of getting any particular value doesn‚Äôt depend on which hypothesis is true:</p>
<p><span class="math display">\[
P(x|h) = \frac{1}{L}
\]</span></p>
<p>where <span class="math inline">\(L\)</span> is the length of the range of possible values (100 in our case).</p>
<p>Under <em>strong sampling</em>, we assume that each observation was specifically chosen as an example of the consequential region <span class="math inline">\(C\)</span>. In other words, someone chose a healthy person and tested their hormone levels as an example for you. In this case, the probability of seeing a particular value depends on the size of the region:</p>
<p><span class="math display">\[
P(x|h) = \begin{cases} 
  \frac{1}{|h|} &amp; \text{if } x \in h \\
  0 &amp; \text{otherwise}
  \end{cases}
\]</span>
where <span class="math inline">\(|h|\)</span> is the <em>size</em> of <span class="math inline">\(h\)</span>, i.e., the number of values contained in <span class="math inline">\(h\)</span>. If you have multiple observations <span class="math inline">\(X = \{x_1, x_2, \ldots, x_n \}\)</span>, then <span class="math inline">\(P(X|h) = (1/|h|)^n\)</span>. This is because we will assume each sample is independent like a coin flip.</p>
<p>The result of the strong sampling assumption is the <em>size principle</em>: among hypotheses that include all of the observed examples, those that are smallest will receive higher posterior probability because they will have higher likelihoods.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-20"></span>
<img src="images/03/hypotheses.png" alt="Sample hypotheses. The thickness of the lines indicates their likelihood, depicting the size principle. Image from Griffiths &amp; Tenenbaum (2001)." width="90%"><p class="caption">
Figure 4.1: Sample hypotheses. The thickness of the lines indicates their likelihood, depicting the size principle. Image from Griffiths &amp; Tenenbaum (2001).
</p>
</div>
</div>
</div>
<div id="posterior" class="section level4" number="4.1.1.4">
<h4>
<span class="header-section-number">4.1.1.4</span> Posterior<a class="anchor" aria-label="anchor" href="#posterior"><i class="fas fa-link"></i></a>
</h4>
<p>We can now simply apply Bayes‚Äôs rule to compute the probability of each hypothesis, given an observation (or set of observations).</p>
<p><span class="math display">\[
P(h|X) = \frac{P(X|h) P(h)}{\sum_{h_i} P(X|h_i) P(h_i)}
\]</span></p>
</div>
</div>
<div id="generalizing" class="section level3" number="4.1.2">
<h3>
<span class="header-section-number">4.1.2</span> Generalizing<a class="anchor" aria-label="anchor" href="#generalizing"><i class="fas fa-link"></i></a>
</h3>
<p>We aren‚Äôt quite finished. Remember that we really want to know the probability of some new value <span class="math inline">\(y\)</span> also being a healthy hormone level. But at this point all we have done is assigned a probability to each <em>interval</em> being the consequential region.</p>
<p>What we want to do is essentially a two-step process:</p>
<ol style="list-style-type: decimal">
<li>For each hypothesis <span class="math inline">\(h\)</span>, check to see if <span class="math inline">\(y\)</span> is in it.</li>
<li>If it is, check how probable it is that <span class="math inline">\(h\)</span> is the consequential region <span class="math inline">\(C\)</span>, given our observations <span class="math inline">\(X\)</span>.</li>
</ol>
<p>This basic idea is sometimes known as hypothesis averaging because we don‚Äôt actually care which hypothesis is the right one, so we‚Äôll just average over <em>all</em> hypotheses, weighted by how probable they are. Specifically, we‚Äôll compute:</p>
<p><span class="math display">\[
P(y \in C|X) = \sum_h P(y \in C | h) P(h | X)
\]</span>
The second term on the right is what we computed earlier using Bayes‚Äôs rule.</p>
<p>What about the first term? This time, we‚Äôll assume <em>weak sampling</em> because there‚Äôs no reason to assume that this new value <span class="math inline">\(y\)</span> was chosen to be a healthy value or not.</p>
</div>
<div id="homework-2-finish-the-details" class="section level3" number="4.1.3">
<h3>
<span class="header-section-number">4.1.3</span> Homework 2: Finish the details<a class="anchor" aria-label="anchor" href="#homework-2-finish-the-details"><i class="fas fa-link"></i></a>
</h3>
<p><a href="https://colab.research.google.com/drive/1PQMz6W9d1VoRi471NLdxCsV-XGp4Rs-j?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.svg"></a></p>
<p>I haven‚Äôt provided all the details for this model because your assignment is to finish the implementation yourself, run some simulations, and collect a small amount of real data to compare the model to.</p>
</div>
</div>
<div id="the-number-game" class="section level2" number="4.2">
<h2>
<span class="header-section-number">4.2</span> The number game üî¢<a class="anchor" aria-label="anchor" href="#the-number-game"><i class="fas fa-link"></i></a>
</h2>
<p>In most domains, requiring concepts to be restricted to contiguous intervals is not realistic. Numbers are one example. Consider the space of possible number concepts you could make up for integers between 1 to 100. In addition to concepts like ‚Äúnumbers between 20 and 50,‚Äù there are many other plausible concepts like ‚Äúmultiples of 10,‚Äù ‚Äúeven numbers,‚Äù or ‚Äúpowers of 3.‚Äù</p>
<p>Consider the following problem: You are given one or more examples <span class="math inline">\(X\)</span> of numbers that fit some rule and you want to know how probable it is that a new number <span class="math inline">\(y\)</span> also fits the rule.</p>
<p>The model we discussed before can be naturally extended to this problem. For the likelihood, we can make the same <a href="generalization.html#weak-strong-sampling">strong sampling</a> assumption as before.</p>
<p>The prior is where things get a little trickier. Intuitively some concepts like ‚Äúeven numbers‚Äù seem more probable even before seeing any examples than concepts like ‚Äúmultiples of 7.‚Äù This now becomes a psychological question: Which rules will people find to be more intuitively plausible? There is no single way to decide this, but we could run a survey to find out: Give people a long list of rules and ask them to judge how intuitively natural they seem. We could then construct a prior probability distribution using this data.</p>
<p>Alternatively, we could come up with some definition of ‚Äúcomplexity‚Äù in hypotheses and assume that less complex hypotheses will receive higher prior probability.</p>
<p>Once we have chosen a prior probability distribution <span class="math inline">\(P(h)\)</span>, we can now proceed just as we did before.</p>
</div>
<div id="inductive-generalizations-about-animal-properties" class="section level2" number="4.3">
<h2>
<span class="header-section-number">4.3</span> Inductive generalizations about animal properties üê¥<a class="anchor" aria-label="anchor" href="#inductive-generalizations-about-animal-properties"><i class="fas fa-link"></i></a>
</h2>
<p>Now let‚Äôs consider an even more complex generalization problem, based on a <a href="http://sanjanalab.org/reprints/Sanjana_NIPS_2002.pdf">2002 paper by Neville Sanjana and Josh Tenenbaum</a><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Sanjana, N. &amp;amp; Tenenbaum, J. (2002). Bayesian models of inductive generalization. Advances in Neural Information Processing Systems 15&lt;/p&gt;"><sup>2</sup></a>. This is the problem of generalizing properties from a set of example animals to other animals. The paper uses the following example:</p>
<pre><code>Chimps have blicketitis
Squirrels have blicketitis
--------------------------
Horses have blicketitis</code></pre>
<p>The way to read this is as follows: The premises state that <strong>chimps and squirrels have blicketitis</strong>. The conclusion is that <strong>horses have blicketitis</strong>. The inductive generalization question is how probable is the conclusion given the premises? Intuitively, the conclusion in this example seems more plausible than the conclusion in the following example:</p>
<pre><code>Chimps have blicketitis
Gorillas have blicketitis
--------------------------
Horses have blicketitis</code></pre>
<p>The interesting psychological question is why is it that some generalizations seem more intuitively plausible than others?</p>
<div id="hypothesis-space" class="section level3" number="4.3.1">
<h3>
<span class="header-section-number">4.3.1</span> Hypothesis space<a class="anchor" aria-label="anchor" href="#hypothesis-space"><i class="fas fa-link"></i></a>
</h3>
<p>This problem is conceptually similar to the ones we‚Äôve already been discussing. You want to infer what animals have blicketitis after seeing some examples of animals that blicketitis. The first question to answer is what is the analogue of a consequential region for this problem. Animals don‚Äôt naturally fall on a one-dimensional interval so we‚Äôll need to define a different hypothesis space. One possibility is a hierarchy, which naturally captures the knowledge people have about animals.</p>
<div id="clustering" class="section level4" number="4.3.1.1">
<h4>
<span class="header-section-number">4.3.1.1</span> Clustering<a class="anchor" aria-label="anchor" href="#clustering"><i class="fas fa-link"></i></a>
</h4>
<p>The paper first creates a hierarchy of eight animals using similarity data collected from people. Specifically, they asked people to judge how similar all pairs of eight animals were and then calculated the average similarity judgment for each animal.</p>
<p>These similarity judgments can be used to construct a tree using a simple clustering algorithm. The algorithm works as follows:</p>
<ol style="list-style-type: decimal">
<li>Put all animals in their own cluster.</li>
<li>Where there is more than one cluster that hasn‚Äôt been placed in a group, do the following:
<ol style="list-style-type: decimal">
<li>Identify the pair of clusters with the greatest similarity between them.</li>
<li>Group those clusters into their own new cluster.</li>
</ol>
</li>
</ol>
<p>There are several approaches for computing the similarity between two clusters that contain multiple animals. For example, you might use the maximum similarity between any pair of individual animals in the two clusters.</p>
<p>The results of this algorithm can be represented as a tree, shown below. Each node in the tree represents a cluster. The hypotheses we will consider will be any combination of 1, 2, or 3 of the clusters determined using the clustering algorithm.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-21"></span>
<img src="images/03/animal_clusters.png" alt="The tree of animal species. Image from Sanjana &amp; Tenenbaum (2002)." width="90%"><p class="caption">
Figure 4.2: The tree of animal species. Image from Sanjana &amp; Tenenbaum (2002).
</p>
</div>
</div>
</div>
<div id="the-model" class="section level3" number="4.3.2">
<h3>
<span class="header-section-number">4.3.2</span> The model<a class="anchor" aria-label="anchor" href="#the-model"><i class="fas fa-link"></i></a>
</h3>
<p>We can now define the model. First, let‚Äôs define <span class="math inline">\(P(h)\)</span> where <span class="math inline">\(h\)</span> is a set of clusters. The authors make an assumption analogous to the following:</p>
<p><span class="math display">\[
P(h) \propto \frac{1}{\phi^k}
\]</span>
where <span class="math inline">\(k\)</span> is the number of clusters in <span class="math inline">\(h\)</span>. <span class="math inline">\(\phi\)</span> is a parameter that we can choose. As long as <span class="math inline">\(\phi &gt; 1\)</span>, <span class="math inline">\(P(h)\)</span> will be smaller for hypotheses consisting of more clusters. This has the effect of assigning higher weight to ‚Äúsimpler‚Äù hypotheses.</p>
<p>Once again, we will make the <a href="sampling-assumptions.html#sampling-assumptions">strong sampling assumption</a> for the likelihood. This time, <span class="math inline">\(|h|\)</span> is the number of animal species in <span class="math inline">\(h\)</span> and <span class="math inline">\(n\)</span> is the number of examples in the premises.</p>
<p>A consequence of both of these assumptions is something like <a href="https://en.wikipedia.org/wiki/Occam%27s_razor">Occam‚Äôs razor</a> which says that the simplest explanation should be preferred. This model will assign 0 likelihood to any hypotheses that don‚Äôt include <span class="math inline">\(X\)</span>, thus narrowing the hypothesis space down to just hypotheses that are possible. Of those, it will favor hypotheses with fewer clusters and with fewer animals. In other words, it will favor the simplest hypotheses that are consistent with the examples we‚Äôve seen.</p>
</div>
<div id="try-out-the-model-yourself" class="section level3" number="4.3.3">
<h3>
<span class="header-section-number">4.3.3</span> Try out the model yourself<a class="anchor" aria-label="anchor" href="#try-out-the-model-yourself"><i class="fas fa-link"></i></a>
</h3>
<p>You can try out a running version of the model by making a copy of the <a href="https://colab.research.google.com/drive/1QIhYLYMcNXjrRQBXKRz9CZ4xMirzqcsp?usp=sharing">code here</a>. Here, let‚Äôs look at how the model handles a few specific cases.</p>
<p>Let‚Äôs start with a single example: horses can get blicketitis.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="generalization.html#cb38-1" aria-hidden="true" tabindex="-1"></a>animalGeneralization([<span class="st">"horse"</span>])</span></code></pre></div>
<pre><code>## array([1.        , 0.52984834, 0.30628906, 0.30628906, 0.19579428,
##        0.19579428, 0.12893614, 0.12893614, 0.0842413 , 0.0842413 ])</code></pre>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-23-1.png" width="672"></div>
<p>Here, we see a standard generalization curve. Now, let‚Äôs add a few more animals.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="generalization.html#cb40-1" aria-hidden="true" tabindex="-1"></a>animalGeneralization([<span class="st">"horse"</span>, <span class="st">"cow"</span>, <span class="st">"mouse"</span>])</span></code></pre></div>
<pre><code>## array([1.        , 1.        , 0.57379051, 0.57379051, 0.47852518,
##        0.47852518, 1.        , 0.60980466, 0.1707609 , 0.1707609 ])</code></pre>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-24-3.png" width="672"></div>
<p>Now the model has increased probability for most other animals. This makes sense because there is more reason to think blicketitis might affect lots of different animals.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="generalization.html#cb42-1" aria-hidden="true" tabindex="-1"></a>animalGeneralization([<span class="st">"horse"</span>, <span class="st">"cow"</span>, <span class="st">"mouse"</span>, <span class="st">"squirrel"</span>])</span></code></pre></div>
<pre><code>## array([1.        , 1.        , 0.64983602, 0.64983602, 0.58531333,
##        0.58531333, 1.        , 1.        , 0.18405475, 0.18405475])</code></pre>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-25-5.png" width="672"></div>
<p>Adding a squirrel further supported this idea. But what if we add additional examples of animals we‚Äôve already seen?</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="generalization.html#cb44-1" aria-hidden="true" tabindex="-1"></a>animalGeneralization([<span class="st">"horse"</span>, <span class="st">"cow"</span>, <span class="st">"mouse"</span>, <span class="st">"squirrel"</span>, <span class="st">"horse"</span>, <span class="st">"squirrel"</span>])</span></code></pre></div>
<pre><code>## array([1.        , 1.        , 0.32551484, 0.32551484, 0.26938358,
##        0.26938358, 1.        , 1.        , 0.06883892, 0.06883892])</code></pre>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-26-7.png" width="672"></div>
<p>Now the probabilities for other animals drop, because it‚Äôs starting to look like maybe this disease only affects the four animals we‚Äôve seen so far.</p>
<p>To sum up, when we‚Äôve seen a small number of examples, like a single horse, the model will generally prefer simpler hypotheses. But once we‚Äôve seen more data, it will favor more complex hypotheses (like a group of animals from two separate evolutionary clusters) if the data support it.</p>
<p>As one final example, let‚Äôs look at the specific impact of multiple examples of a single animal.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="generalization.html#cb46-1" aria-hidden="true" tabindex="-1"></a>animalGeneralization([<span class="st">"gorilla"</span>])</span></code></pre></div>
<pre><code>## array([0.22011594, 0.22011594, 0.22011594, 0.22011594, 0.46663258,
##        1.        , 0.13668495, 0.13668495, 0.08643809, 0.08643809])</code></pre>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-27-9.png" width="672"></div>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="generalization.html#cb48-1" aria-hidden="true" tabindex="-1"></a>animalGeneralization([<span class="st">"gorilla"</span>, <span class="st">"gorilla"</span>])</span></code></pre></div>
<pre><code>## array([0.06236149, 0.06236149, 0.06236149, 0.06236149, 0.2473422 ,
##        1.        , 0.03994508, 0.03994508, 0.02971748, 0.02971748])</code></pre>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-27-10.png" width="672"></div>
<div class="sourceCode" id="cb50"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="generalization.html#cb50-1" aria-hidden="true" tabindex="-1"></a>animalGeneralization([<span class="st">"gorilla"</span>, <span class="st">"gorilla"</span>, <span class="st">"gorilla"</span>])</span></code></pre></div>
<pre><code>## array([0.01730482, 0.01730482, 0.01730482, 0.01730482, 0.1259028 ,
##        1.        , 0.01270155, 0.01270155, 0.01111944, 0.01111944])</code></pre>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-27-11.png" width="672"></div>
<p>Here, we see that the model becomes increasingly confident that the property is unique to gorillas. This makes intuitive sense and it‚Äôs something that people seem to exhibit in their judgments. But, as they point out in the paper, it‚Äôs not something that non-probabilistic models can easily explain.</p>
</div>
<div id="results" class="section level3" number="4.3.4">
<h3>
<span class="header-section-number">4.3.4</span> Results<a class="anchor" aria-label="anchor" href="#results"><i class="fas fa-link"></i></a>
</h3>
<p>The results in the paper show that this model predicts people‚Äôs judgments quite well, better than alternative models that do not rely on Bayesian inference. These results suggest that the assumptions of the model are very similar to the assumptions that people make when making inductive generalizations.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-28"></span>
<img src="images/03/st_results.png" alt="Comparison between model results and human judgments. Image from Sanjana &amp; Tenenbaum (2002)." width="90%"><p class="caption">
Figure 4.3: Comparison between model results and human judgments. Image from Sanjana &amp; Tenenbaum (2002).
</p>
</div>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="bayes.html"><span class="header-section-number">3</span> Bayesian inference</a></div>
<div class="next"><a href="categorization.html"><span class="header-section-number">5</span> Categorization</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#generalization"><span class="header-section-number">4</span> Generalization</a></li>
<li>
<a class="nav-link" href="#hormones"><span class="header-section-number">4.1</span> Healthy hormone levels üíâ</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#setting-up-a-model"><span class="header-section-number">4.1.1</span> Setting up a model</a></li>
<li><a class="nav-link" href="#generalizing"><span class="header-section-number">4.1.2</span> Generalizing</a></li>
<li><a class="nav-link" href="#homework-2-finish-the-details"><span class="header-section-number">4.1.3</span> Homework 2: Finish the details</a></li>
</ul>
</li>
<li><a class="nav-link" href="#the-number-game"><span class="header-section-number">4.2</span> The number game üî¢</a></li>
<li>
<a class="nav-link" href="#inductive-generalizations-about-animal-properties"><span class="header-section-number">4.3</span> Inductive generalizations about animal properties üê¥</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#hypothesis-space"><span class="header-section-number">4.3.1</span> Hypothesis space</a></li>
<li><a class="nav-link" href="#the-model"><span class="header-section-number">4.3.2</span> The model</a></li>
<li><a class="nav-link" href="#try-out-the-model-yourself"><span class="header-section-number">4.3.3</span> Try out the model yourself</a></li>
<li><a class="nav-link" href="#results"><span class="header-section-number">4.3.4</span> Results</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Computational Psychology in Python</strong>" was written by Alan Jern. It was last built on 2022-04-01.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
