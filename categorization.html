<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 5 Categorization | Computational Psychology in Python</title>
<meta name="author" content="Alan Jern">
<meta name="description" content="The last chapter introduced the problem of inductive generalization. This chapter will focus on a specific case of generalization that is particular interest to psychologists, cognitive...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 5 Categorization | Computational Psychology in Python">
<meta property="og:type" content="book">
<meta property="og:description" content="The last chapter introduced the problem of inductive generalization. This chapter will focus on a specific case of generalization that is particular interest to psychologists, cognitive...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 5 Categorization | Computational Psychology in Python">
<meta name="twitter:description" content="The last chapter introduced the problem of inductive generalization. This chapter will focus on a specific case of generalization that is particular interest to psychologists, cognitive...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.9/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Computational Psychology in Python</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Preface</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">2</span> Why computational modeling?</a></li>
<li><a class="" href="bayes.html"><span class="header-section-number">3</span> Bayesian inference</a></li>
<li><a class="" href="generalization.html"><span class="header-section-number">4</span> Generalization</a></li>
<li><a class="active" href="categorization.html"><span class="header-section-number">5</span> Categorization</a></li>
<li><a class="" href="hierarchical-generalization.html"><span class="header-section-number">6</span> Hierarchical generalization</a></li>
<li><a class="" href="sampling-assumptions.html"><span class="header-section-number">7</span> Sampling assumptions</a></li>
<li><a class="" href="the-rational-speech-act-model.html"><span class="header-section-number">8</span> The Rational Speech Act model</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="categorization" class="section level1" number="5">
<h1>
<span class="header-section-number">5</span> Categorization<a class="anchor" aria-label="anchor" href="#categorization"><i class="fas fa-link"></i></a>
</h1>
<p>The last chapter introduced the problem of inductive generalization. This chapter will focus on a specific case of generalization that is particular interest to psychologists, cognitive scientists, and people who use AI and machine learning: <em>classification</em> or <em>categorization</em>. Basically, assigning labels to things.</p>
<p>People constantly classify things in the world into categories (chairs, cats, friends, enemies, edible things, and so on). Doing this helps us to communicate and function in novel situations.</p>
<p>In machine learning, it’s often useful to classify inputs into different categories like whether a social media post violates community standards or not, whether an image contains a human face or not, or whether an MRI contains a tumor.</p>
<p>Psychologists study how people form categories because it provides a window into how we organize our knowledge. But the basic problem is the same as the one studied in machine learning.</p>
<p>I’m going to intrdouce a fairly simple psychological model of categorization introduced by Robert Nosofsky called the Generalized Context Model. It assumes that people make classification judgments using the following general algorithm:</p>
<ol style="list-style-type: decimal">
<li>Remember all the examples of categories you’ve seen before.</li>
<li>When you want to classify a new instance, compare it to all previous examples of different categories and rate how similar it is to each one.</li>
<li>Assign it to the category that has the highest average similarity.</li>
</ol>
<p>This model makes some outlandish assumptions (like the idea that people would remember every example they had seen before). But, to a first approximation, it does a decent job of predicting people’s classification judgments in a lot of situations. And for learning purposes, it has the advantage of being pretty easy to understand and implement.</p>
<div id="a-typical-category-learning-experiment" class="section level2" number="5.1">
<h2>
<span class="header-section-number">5.1</span> A typical category learning experiment 🟢🟨<a class="anchor" aria-label="anchor" href="#a-typical-category-learning-experiment"><i class="fas fa-link"></i></a>
</h2>
<p>In psychology, category learning experiments have a pretty similar structure. People see some unfamiliar stimuli that differ on several dimensions and can be sorted into different categories. The task is to learn what distinguishes one category from another. The experiments usually consist of two phases: a training phase, and a testing phase.</p>
<ul>
<li>
<em>Training phase</em>: People see many examples of each category and have to classify them, often simply by guessing at first. They get feedback and gradually learn to tell the different categories apart.</li>
<li>
<em>Testing phase</em>: People see more examples, usually some brand new ones, and have to classify them again. This time they don’t get feedback. The point of this phase is to test what people actually learned about the meaning of the categories.</li>
</ul>
<p>The stimuli could be abstract shapes, cartoon insects, race cars with different features, or anything else. What matters is that they have clearly distinguishable features.</p>
</div>
<div id="representing-stimuli-in-a-model" class="section level2" number="5.2">
<h2>
<span class="header-section-number">5.2</span> Representing stimuli in a model 🧩<a class="anchor" aria-label="anchor" href="#representing-stimuli-in-a-model"><i class="fas fa-link"></i></a>
</h2>
<p>For modeling purposes, all of these types of stimuli (with discrete-valued features) can be represented in a matrix, with the following properties:</p>
<ol style="list-style-type: decimal">
<li>Each dimension of the matrix represents a different feature of the stimuli.</li>
<li>The length of each dimension represents how many different values that feature can have.</li>
<li>A single item can then be represented by a binary matrix with 1s in the cells indicating which feature values it has and 0s everywhere else.</li>
</ol>
<p>For example, in Robert Nosofsky’s (1986) test of the GCM<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Nosofsky, R. M. (1986). Attention, similarity, and the identification-categorization relationship. Journal of Experimental Psychology: General, 115(1), 39-57.&lt;/p&gt;"><sup>3</sup></a>, the stimuli were semicircles with lines through them. The two feature dimensions were (1) circle size and (2) line orientation. Each feature had four possible values.</p>
<p>Using our binary matrix representation, here is one possible stimulus from the Nosofsky experiment:</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="categorization.html#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb52-2"><a href="categorization.html#cb52-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-3"><a href="categorization.html#cb52-3" aria-hidden="true" tabindex="-1"></a>stimulus <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb52-4"><a href="categorization.html#cb52-4" aria-hidden="true" tabindex="-1"></a>                     [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb52-5"><a href="categorization.html#cb52-5" aria-hidden="true" tabindex="-1"></a>                     [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb52-6"><a href="categorization.html#cb52-6" aria-hidden="true" tabindex="-1"></a>                     [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]], dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb52-7"><a href="categorization.html#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(stimulus)</span></code></pre></div>
<pre><code>## [[0 0 0 0]
##  [0 0 0 0]
##  [0 0 1 0]
##  [0 0 0 0]]</code></pre>
<p>The Nosofksy stimuli are pretty simple: They can have exactly one value on each dimension, so this representation will always only have a single 1 in the matrix.</p>
<p>If we wanted to know how many of each stimulus to present our model, it would be pretty cumbersome to maintain a long list of arrays like this one. So we could instead maintain a single matrix that stores counts of stimuli. That is, the values at each element will represent the number of stimuli observed with those feature values.</p>
<p>For example, let’s consider the “dimensional” condition of Nosofsky’s experiment, in which subjects saw only the stimuli on the “diagonals” of the matrix. Let’s assume they saw each one 100 times (actually, they did a mind-numbing 1200 trials after 2600 practice trials 🥴). We could represent that like so:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="categorization.html#cb54-1" aria-hidden="true" tabindex="-1"></a>training_set <span class="op">=</span> np.array([[<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">100</span>],</span>
<span id="cb54-2"><a href="categorization.html#cb54-2" aria-hidden="true" tabindex="-1"></a>                         [<span class="dv">0</span>, <span class="dv">100</span>, <span class="dv">100</span>, <span class="dv">0</span>],</span>
<span id="cb54-3"><a href="categorization.html#cb54-3" aria-hidden="true" tabindex="-1"></a>                         [<span class="dv">0</span>, <span class="dv">100</span>, <span class="dv">100</span>, <span class="dv">0</span>],</span>
<span id="cb54-4"><a href="categorization.html#cb54-4" aria-hidden="true" tabindex="-1"></a>                         [<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">100</span>]], dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb54-5"><a href="categorization.html#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(training_set)</span></code></pre></div>
<pre><code>## [[100   0   0 100]
##  [  0 100 100   0]
##  [  0 100 100   0]
##  [100   0   0 100]]</code></pre>
<p>This still isn’t ideal though. Because it doesn’t represent the category labels that subjects got during their training. We don’t know which examples belonged to which categories.</p>
<p>Thinking ahead to how the GCM works, let’s instead represent the training examples more like a list, so that we can iterate through them:</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="categorization.html#cb56-1" aria-hidden="true" tabindex="-1"></a>tDimensional <span class="op">=</span> np.zeros((<span class="dv">8</span>,<span class="dv">3</span>), dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb56-2"><a href="categorization.html#cb56-2" aria-hidden="true" tabindex="-1"></a>tDimensional[<span class="dv">0</span>] <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>]</span>
<span id="cb56-3"><a href="categorization.html#cb56-3" aria-hidden="true" tabindex="-1"></a>tDimensional[<span class="dv">1</span>] <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>]</span>
<span id="cb56-4"><a href="categorization.html#cb56-4" aria-hidden="true" tabindex="-1"></a>tDimensional[<span class="dv">2</span>] <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">2</span>]</span>
<span id="cb56-5"><a href="categorization.html#cb56-5" aria-hidden="true" tabindex="-1"></a>tDimensional[<span class="dv">3</span>] <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">1</span>]</span>
<span id="cb56-6"><a href="categorization.html#cb56-6" aria-hidden="true" tabindex="-1"></a>tDimensional[<span class="dv">4</span>] <span class="op">=</span> [<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">4</span>]</span>
<span id="cb56-7"><a href="categorization.html#cb56-7" aria-hidden="true" tabindex="-1"></a>tDimensional[<span class="dv">5</span>] <span class="op">=</span> [<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>]</span>
<span id="cb56-8"><a href="categorization.html#cb56-8" aria-hidden="true" tabindex="-1"></a>tDimensional[<span class="dv">6</span>] <span class="op">=</span> [<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">3</span>]</span>
<span id="cb56-9"><a href="categorization.html#cb56-9" aria-hidden="true" tabindex="-1"></a>tDimensional[<span class="dv">7</span>] <span class="op">=</span> [<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">4</span>]</span>
<span id="cb56-10"><a href="categorization.html#cb56-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-11"><a href="categorization.html#cb56-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tDimensional)</span></code></pre></div>
<pre><code>## [[1 1 1]
##  [1 2 2]
##  [1 3 2]
##  [1 4 1]
##  [2 1 4]
##  [2 2 3]
##  [2 3 3]
##  [2 4 4]]</code></pre>
<p>In this format, each row of the matrix is an array with three elements. The first element is the category label: 1 or 2. The second two elements are the indices into the stimulus matrix. <strong>Note: Here I’ve deviated from Python norms to be consistent with the numbering in the table in the original Nosofsky paper, shown below. When working with the model, remember that indexing in Python starts with 0.</strong></p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-32"></span>
<img src="images/04/dimensional_table.png" alt="Training examples from the dimensional condition. Image from Nosofsky (1986)." width="50%"><p class="caption">
Figure 5.1: Training examples from the dimensional condition. Image from Nosofsky (1986).
</p>
</div>
</div>
<div id="the-generalized-context-model-gcm" class="section level2" number="5.3">
<h2>
<span class="header-section-number">5.3</span> The Generalized Context Model (GCM)<a class="anchor" aria-label="anchor" href="#the-generalized-context-model-gcm"><i class="fas fa-link"></i></a>
</h2>
<p>Here, I will describe a special case of the model here that applies to tasks (like the one in the paper) with only two categories. However, this model can easily be extended to any number of categories. The model is defined by two key equations.</p>
<p>The first equation defines the probability of classifying Stimulus <span class="math inline">\(S_i\)</span> into Category <span class="math inline">\(C_1\)</span> (i.e., Category 1):</p>
<p><span class="math display" id="eq:gcm1">\[
\begin{equation}
P(C_1|S_i) = \frac{b_1 \sum_{j \in C_1} \eta_{ij}}{b_1 \sum_{j \in C_1} \eta_{ij} + (1-b_1) \sum_{k \in C_2} \eta_{ik}}
\tag{5.1}
\end{equation}
\]</span></p>
<p>The sum <span class="math inline">\(\sum_{j \in C_1}\)</span> is a sum over all stimuli <span class="math inline">\(S_j\)</span> that belong to Category 1. The sum <span class="math inline">\(\sum_{j \in C_1}\)</span> is a sum over all stimuli <span class="math inline">\(S_k\)</span> that belong to Category 2.</p>
<p>Equation <a href="categorization.html#eq:gcm1">(5.1)</a> has a parameter <span class="math inline">\(b_1\)</span> which is defined as the response bias for Category 1. <span class="math inline">\(b_1\)</span> can range from 0 to 1 and captures the possibility that a subject is biased to respond with one category or another. This is similar but not identical to a prior distribution. When <span class="math inline">\(b_1\)</span> is small, the model is biased toward Category 2; when <span class="math inline">\(b_1\)</span> is large, the model is biased toward Category 1. Note that when <span class="math inline">\(b = 0.5\)</span>, it cancels out of the equation.</p>
<p>We could also include a response bias for Category 2, but the model assumes that <span class="math inline">\(\sum_i b_i = 1\)</span>. Therefore, when there are only two categories, <span class="math inline">\(b_2 = 1 - b_1\)</span> which is what you see in the second term of the denominator of Equation <a href="categorization.html#eq:gcm1">(5.1)</a>.</p>
<p><span class="math inline">\(\eta_{ij}\)</span> is a function that defines how similar to <span class="math inline">\(S_i\)</span> and <span class="math inline">\(S_j\)</span> are. The GCM assumes that stimuli can be represented as points in a multi-dimensional space (in this case, a two-dimensional space) and the similarity is defined as a function of the distance between those two points:</p>
<p><span class="math display">\[
\eta_{ij} = e^{-c^2 \left[w_1 (f_{i1} - f_{j1})^2 + (1-w_1)(f_{i2} - f_{j2})^2 \right]}
\]</span></p>
<p>where <span class="math inline">\(f_{i1}\)</span> and <span class="math inline">\(f_{i2}\)</span> refer to the feature values on dimensions 1 and 2 of <span class="math inline">\(S_i\)</span>. This equation has two parameters: <span class="math inline">\(c\)</span> and <span class="math inline">\(w_1\)</span>. <span class="math inline">\(c\)</span> is a scaling parameter that affects how steeply the exponential curve is. This will allow us to account for how different people might have different ideas of how close two stimuli have must be to be called similar. <span class="math inline">\(w_1\)</span> is called the attentional weight for dimension 1. This parameter captures how much weight is placed on dimension 1 over dimension 2. Just like <span class="math inline">\(b_1\)</span>, <span class="math inline">\(w_1\)</span> can range from 0 to 1, and the larger it is, the more weight is placed on dimension 1. Similarly, we could add a <span class="math inline">\(w_2\)</span>, but the attention weights are constrained to sum to 1.</p>
<div id="generating-model-predictions" class="section level3" number="5.3.1">
<h3>
<span class="header-section-number">5.3.1</span> Generating model predictions<a class="anchor" aria-label="anchor" href="#generating-model-predictions"><i class="fas fa-link"></i></a>
</h3>
<p>In order to generate predictions from this model, we need two things. First, we need to define what the training stimuli are, and what their feature values are. This is what will allow us to compute the sums in Equation <a href="categorization.html#eq:gcm1">(5.1)</a>.</p>
<p>Next, we need to specify the values of the 3 parameters <span class="math inline">\(b_1\)</span>, <span class="math inline">\(c\)</span>, and <span class="math inline">\(w_1\)</span>. Nosofsky does this by first collecting data from people in the experiment. That is, for every stimulus <span class="math inline">\(S_i\)</span> we can record the proportion of times people classified it as Category 1. In other words, we can collect empirical estimates of <span class="math inline">\(P(C_1|S_i)\)</span> for all values of <span class="math inline">\(S_i\)</span>.</p>
<p>Then Nosofsky uses a maximum likelihood procedure for fitting the model to the data. Conceptually, the idea is to find the set of values of the three parameters that produce model predictions that are as close as possible to the empirical data. (This is exactly how linear regression works, where you have some data <span class="math inline">\((\bar{x},\bar{y})\)</span> and want to find the best-fitting values of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> for the function <span class="math inline">\(y = ax + b\)</span> that describes the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.)</p>
<p>We can define model fit by mean squared error (MSE). MSE is defined as</p>
<p><span class="math display">\[
\frac{1}{n} \sum_i (y_i - x_i)^2
\]</span></p>
<p>where <span class="math inline">\(y\)</span> are the data and <span class="math inline">\(x\)</span> are the model predictions. A lower MSE means a better model fit. Finding the best values of the parameters can be achieved through an exhaustive search of all values. For example, we might consider all possible values of each parameter in increments of 0.1. We could then write a program that performs the following basic algorithm:</p>
<ol style="list-style-type: decimal">
<li>Set minimum MSE to <span class="math inline">\(\inf\)</span>.</li>
<li>For all possible values of <span class="math inline">\(b_1\)</span>, <span class="math inline">\(c\)</span>, and <span class="math inline">\(w_1\)</span>:
<ol style="list-style-type: decimal">
<li>Generate model predictions <span class="math inline">\(P(R_1|S_i)\)</span> for all <span class="math inline">\(S_i\)</span>.</li>
<li>Compute MSE between the empirical data and model predictions generated in the previous step.</li>
<li>If this MSE is smaller than minimum MSE, set minimum MSE to the current value and store the current values of <span class="math inline">\(b_1\)</span>, <span class="math inline">\(c\)</span>, and <span class="math inline">\(w_1\)</span>.</li>
</ol>
</li>
<li>Return the stored values of <span class="math inline">\(b_1\)</span>, <span class="math inline">\(c\)</span>, and <span class="math inline">\(w_1\)</span>.</li>
</ol>
</div>
<div id="homework-3" class="section level3" number="5.3.2">
<h3>
<span class="header-section-number">5.3.2</span> Homework 3<a class="anchor" aria-label="anchor" href="#homework-3"><i class="fas fa-link"></i></a>
</h3>
<p><a href="https://colab.research.google.com/drive/166ptmuGWwqxLvRfbXaoZ7qI0B5326lh7?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.svg"></a></p>
<p>In your next homework you will implement the GCM described above. In the homework, you will have to encode training and test stimuli and implement the equations and model-fitting algorithm above. Additionally, to get some experience with the behavioral side of cognitive science, you will create a working version of a category learning experiment in order to collect some data that you can use to compare with the GCM’s predictions.</p>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="generalization.html"><span class="header-section-number">4</span> Generalization</a></div>
<div class="next"><a href="hierarchical-generalization.html"><span class="header-section-number">6</span> Hierarchical generalization</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#categorization"><span class="header-section-number">5</span> Categorization</a></li>
<li><a class="nav-link" href="#a-typical-category-learning-experiment"><span class="header-section-number">5.1</span> A typical category learning experiment 🟢🟨</a></li>
<li><a class="nav-link" href="#representing-stimuli-in-a-model"><span class="header-section-number">5.2</span> Representing stimuli in a model 🧩</a></li>
<li>
<a class="nav-link" href="#the-generalized-context-model-gcm"><span class="header-section-number">5.3</span> The Generalized Context Model (GCM)</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#generating-model-predictions"><span class="header-section-number">5.3.1</span> Generating model predictions</a></li>
<li><a class="nav-link" href="#homework-3"><span class="header-section-number">5.3.2</span> Homework 3</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Computational Psychology in Python</strong>" was written by Alan Jern. It was last built on 2022-03-26.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
