<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 3 Bayesian inference | Computational Psychology in Python</title>
<meta name="author" content="Alan Jern">
<meta name="description" content="This chapter reviews some basic probability and Bayesian inference. You might be asking yourself: What does this have to do with psychology? The answer becomes clear when you recognize that most...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 3 Bayesian inference | Computational Psychology in Python">
<meta property="og:type" content="book">
<meta property="og:description" content="This chapter reviews some basic probability and Bayesian inference. You might be asking yourself: What does this have to do with psychology? The answer becomes clear when you recognize that most...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 3 Bayesian inference | Computational Psychology in Python">
<meta name="twitter:description" content="This chapter reviews some basic probability and Bayesian inference. You might be asking yourself: What does this have to do with psychology? The answer becomes clear when you recognize that most...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.9/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Computational Psychology in Python</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Preface</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">2</span> Why computational modeling?</a></li>
<li><a class="active" href="bayes.html"><span class="header-section-number">3</span> Bayesian inference</a></li>
<li><a class="" href="generalization.html"><span class="header-section-number">4</span> Generalization</a></li>
<li><a class="" href="categorization.html"><span class="header-section-number">5</span> Categorization</a></li>
<li><a class="" href="hierarchical-generalization.html"><span class="header-section-number">6</span> Hierarchical generalization</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="bayes" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> Bayesian inference<a class="anchor" aria-label="anchor" href="#bayes"><i class="fas fa-link"></i></a>
</h1>
<p>This chapter reviews some basic probability and Bayesian inference. You might be asking yourself: <em>What does this have to do with psychology?</em> The answer becomes clear when you recognize that most of what we do when we‚Äôre making sense of the world is drawing inferences. When you see an ambiguous image, is it a rabbit or a duck? When someone mumbles something, did they say ‚Äúhello‚Äù or ‚Äúgo to hell?‚Äù When you take a pill and your headache goes away, did the pill eliminate your headache or did the headache go away on its own?</p>
<div class="figure">
<span style="display:block;" id="fig:unnamed-chunk-3"></span>
<img src="images/02/duck-rabbit.png" alt="The duck-rabbit illusion." width="70%"><p class="caption">
Figure 3.1: The duck-rabbit illusion.
</p>
</div>
<p>In all of these examples, there is more than one hypothesis about what we observed. Probability and Bayesian inference provide the tools for optimally determining how probable these different hypotheses are. One of the claims of this book is that when people are making inferences in situations like these, their inferences are often well predicted by the optimal inferences dictated by probability theory.</p>
<div id="basic-probability" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> Basic probability üé≤<a class="anchor" aria-label="anchor" href="#basic-probability"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>Conditional probability: <span class="math inline">\(P(b|a) = \frac{P(a,b)}{P(a)}\)</span>
</li>
<li>Chain rule: <span class="math inline">\(P(a,b) = P(b|a)P(a)\)</span>
</li>
<li>Marginalization: <span class="math inline">\(P(d) = \sum_h P(d,h) = \sum_h P(d|h) P(h)\)</span>
</li>
<li>Bayes‚Äôs rule: <span class="math inline">\(P(h|d) = \frac{P(d|h) P(h)}{P(d)} = \frac{P(d|h) P(h)}{\sum_h P(d|h) P(h)}\)</span>. <span class="math inline">\(P(d|h)\)</span> is referred to as the <em>likelihood</em>, <span class="math inline">\(P(h)\)</span> is the <em>prior</em>, and <span class="math inline">\(P(h|d)\)</span> is the <em>posterior</em>.</li>
</ul>
</div>
<div id="a-motivating-example-sampling-from-a-bag" class="section level2" number="3.2">
<h2>
<span class="header-section-number">3.2</span> A motivating example: Sampling from a bag üëù<a class="anchor" aria-label="anchor" href="#a-motivating-example-sampling-from-a-bag"><i class="fas fa-link"></i></a>
</h2>
<p>Suppose you have a bag full of black and red balls. You can‚Äôt see inside the bag and you don‚Äôt know how many black and red balls are inside, but you know that there are nine total balls in the bag.</p>
<p>You want to know how many black balls and red balls there are. There are a finite number of hypotheses: {0 black balls, 1 black ball, 2 black balls, ‚Ä¶, 9 black balls}. Let‚Äôs call these hypotheses <span class="math inline">\(B_0\)</span>, <span class="math inline">\(B_1\)</span>, etc., respectively.</p>
<p>You don‚Äôt know which hypothesis is true, but you might have some idea which hypotheses are more likely than others. Therefore, it is natural to represent your uncertainty with a probability distribution over the possible unknown states that the world could be in ‚Äì in this case, the 10 hypotheses. Each hypothesis gets assigned a probability, and the probabilities sum to 1.</p>
<p>For simplicity, let‚Äôs assume that you <em>don‚Äôt</em> have any idea which hypotheses are more likely. In other words, you give every hypothesis the same probability: 1/10 = 0.1. This is also called a uniform distribution over hypotheses. This distribution is your prior.</p>
<p>Now suppose you put your hand in the bag and pull out a ball at random. The possible observations are: <code>{black, red}</code>, Let‚Äôs call them <span class="math inline">\(B\)</span> and <span class="math inline">\(R\)</span>, respectively. The probability of observing each color depends on which hypothesis is true, i.e., how many balls of each color are in the bag. For instance, if <span class="math inline">\(B_0\)</span> is true (there are 0 black balls in the bag), then the probability of observing a red ball is 1 (<span class="math inline">\(P(R|B_0)=1\)</span>), and the probability of observing a black ball is 0 (<span class="math inline">\(P(B|B_0)=0\)</span>). These expressions that tell us how probable our observations are, given a specific hypothesis, are your likelihoods.</p>
<div id="sampling-from-the-generative-model" class="section level3" number="3.2.1">
<h3>
<span class="header-section-number">3.2.1</span> Sampling from the generative model<a class="anchor" aria-label="anchor" href="#sampling-from-the-generative-model"><i class="fas fa-link"></i></a>
</h3>
<p>Now we have a distribution over hypotheses (a prior), <span class="math inline">\(P(h)\)</span>, and a distribution over observations given each hypothesis (a likelihood), <span class="math inline">\(P(d|h)\)</span>. These two things allow us to create a <em>generative model</em>, a model for sampling new data.</p>
<p>How do we sample from the generative model? Note that which hypothesis <span class="math inline">\(h\)</span> is true does not depend on the data, while the data <span class="math inline">\(d\)</span> depends on which hypothesis is true. Therefore, we can sample from the generative model using the following two-step process:</p>
<ol style="list-style-type: decimal">
<li>Sample a hypothesis from the prior.</li>
<li>Sample data given the hypothesis, using the likelihood.</li>
</ol>
<p>Let‚Äôs first create a vector with the probability of each hypothesis:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="bayes.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="bayes.html#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-3"><a href="bayes.html#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="bayes.html#cb1-4" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">2022</span>) <span class="co"># set random seed to get same results every time</span></span>
<span id="cb1-5"><a href="bayes.html#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="bayes.html#cb1-6" aria-hidden="true" tabindex="-1"></a>h_priors <span class="op">=</span> np.repeat(<span class="fl">0.1</span>,<span class="dv">10</span>)</span>
<span id="cb1-7"><a href="bayes.html#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(h_priors)</span></code></pre></div>
<pre><code>## [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]</code></pre>
<p>Now we‚Äôll do the first step: create a vector of 10000 hypotheses sampled from the prior:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="bayes.html#cb3-1" aria-hidden="true" tabindex="-1"></a>prior_samples <span class="op">=</span> np.array(random.choices(np.arange(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">1</span>),</span>
<span id="cb3-2"><a href="bayes.html#cb3-2" aria-hidden="true" tabindex="-1"></a>                   weights <span class="op">=</span> h_priors, </span>
<span id="cb3-3"><a href="bayes.html#cb3-3" aria-hidden="true" tabindex="-1"></a>                   k <span class="op">=</span> <span class="dv">10000</span>))</span>
<span id="cb3-4"><a href="bayes.html#cb3-4" aria-hidden="true" tabindex="-1"></a>                   </span>
<span id="cb3-5"><a href="bayes.html#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prior_samples[<span class="dv">0</span>:<span class="dv">9</span>]) <span class="co"># printing out just a few</span></span></code></pre></div>
<pre><code>## [5 4 3 0 7 9 4 6 8]</code></pre>
<p>Here, each number corresponds to one hypothesis: 0 corresponds to <span class="math inline">\(B_0\)</span>, 1 to <span class="math inline">\(B_1\)</span>, and so on. Each sample represents one possible way (a hypothesis) the world could be. Since the prior was uniform (each hypothesis had the same probability), each hypothesis appears about equally often. We can plot all the samples to verify:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="bayes.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-2"><a href="bayes.html#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="bayes.html#cb5-3" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb5-4"><a href="bayes.html#cb5-4" aria-hidden="true" tabindex="-1"></a>n, bins, patches <span class="op">=</span> ax.hist(prior_samples, bins<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb5-5"><a href="bayes.html#cb5-5" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Prior sample'</span>)</span>
<span id="cb5-6"><a href="bayes.html#cb5-6" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Number of samples'</span>)</span></code></pre></div>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-7-1.png" width="672"></div>
<p>Now for the next step. For each sample in <code>prior_samples</code>, we want to sample an observation. To do that, let‚Äôs pause for a second and think about the probability of pulling a black ball given that hypothesis <span class="math inline">\(B_3\)</span> is true, for example. This means that there are 3 black balls and 6 red balls in the bag. So the probability of pulling a black ball from the bag at random will be 3/9.</p>
<p>Generalizing this idea, we can get the probability of pulling a black ball from the bag by dividing the elements of <code>prior_samples</code> by 9:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="bayes.html#cb6-1" aria-hidden="true" tabindex="-1"></a>p_black <span class="op">=</span> prior_samples <span class="op">/</span> <span class="dv">9</span></span>
<span id="cb6-2"><a href="bayes.html#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(p_black[<span class="dv">0</span>:<span class="dv">4</span>]) <span class="co"># print out just a few</span></span></code></pre></div>
<pre><code>## [0.55555556 0.44444444 0.33333333 0.        ]</code></pre>
<p>Now, to complete our generative model, we just need to sample one value for each element of <code>p_black</code>. Each sample represents a draw from a bag.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="bayes.html#cb8-1" aria-hidden="true" tabindex="-1"></a>ball_samples <span class="op">=</span> np.random.binomial(n <span class="op">=</span> np.repeat(<span class="dv">1</span>,<span class="bu">len</span>(p_black)),</span>
<span id="cb8-2"><a href="bayes.html#cb8-2" aria-hidden="true" tabindex="-1"></a>                                  p <span class="op">=</span> p_black)</span>
<span id="cb8-3"><a href="bayes.html#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ball_samples[<span class="dv">0</span>:<span class="dv">9</span>])</span></code></pre></div>
<pre><code>## [0 0 0 0 1 1 1 1 1]</code></pre>
<p><code>ball_samples</code> is 1 for black and 0 for red. Once again, let‚Äôs plot all our samples.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="bayes.html#cb10-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb10-2"><a href="bayes.html#cb10-2" aria-hidden="true" tabindex="-1"></a>n, bins, patches <span class="op">=</span> ax.hist(ball_samples, bins<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb10-3"><a href="bayes.html#cb10-3" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb10-4"><a href="bayes.html#cb10-4" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Number of samples'</span>)</span></code></pre></div>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-10-3.png" width="672"></div>
<p>You can think of this plot representing our overall beliefs about the number of red and black balls in the bag, averaged over all possible hypotheses.</p>
<p>Not surprisingly, we got about equal numbers of red and black balls. This makes sense: We didn‚Äôt have any prior expectations about whether red or black balls were more likely in the bag.</p>
<p>How should our beliefs change after we pull a ball out of the bag? That is, how should we respond to evidence?</p>
</div>
</div>
<div id="bayesian-updating-learning-from-evidence" class="section level2" number="3.3">
<h2>
<span class="header-section-number">3.3</span> Bayesian updating: Learning from evidence ü§î<a class="anchor" aria-label="anchor" href="#bayesian-updating-learning-from-evidence"><i class="fas fa-link"></i></a>
</h2>
<p>Let‚Äôs apply <a href="bayes.html#basic-probability">Bayes‚Äôs rule</a> to see how to optimally incorporate new data into your beliefs.</p>
<div id="applying-bayess-rule-to-the-bag-case" class="section level3" number="3.3.1">
<h3>
<span class="header-section-number">3.3.1</span> Applying Bayes‚Äôs rule to the bag case<a class="anchor" aria-label="anchor" href="#applying-bayess-rule-to-the-bag-case"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose you have a uniform prior distribution over the 10 hypotheses about balls in the bag. Now you pick a ball and it‚Äôs black. Given this observation <span class="math inline">\(B\)</span>, how should you change the probabilities you give to each hypothesis?</p>
<p>Intuitively, you should now give a little bit more probability to those hypotheses that have more black balls than red balls, because those are the hypotheses that make your observations more likely. Moreover, you can safely exclude hypothesis <span class="math inline">\(B_0\)</span>, because your observation would be impossible if <span class="math inline">\(B_0\)</span> were true. Let‚Äôs calculate this with Bayes‚Äôs rule.</p>
<p>The prior is the vector <code>h_priors</code> defined above. Given that we have observed <span class="math inline">\(B\)</span>, the likelihood should tell us, for each hypothesis, the probability of <span class="math inline">\(B\)</span> given that hypothesis. For example, for <span class="math inline">\(B_9\)</span>, the likelihood <span class="math inline">\(P(B|B_9) = 1\)</span>. For <span class="math inline">\(B_8\)</span>, <span class="math inline">\(P(B|B_8) = 8/9\)</span>, because 8 of the 9 balls are black.</p>
<p>Generalizing this idea, <span class="math inline">\(P(B|B_n) = n/9\)</span>. We can therefore compute the likelihoods for all hypotheses in a vector:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="bayes.html#cb11-1" aria-hidden="true" tabindex="-1"></a>likelihoods <span class="op">=</span> np.arange(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">1</span>) <span class="op">/</span> <span class="dv">9</span></span>
<span id="cb11-2"><a href="bayes.html#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="bayes.html#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(likelihoods)</span></code></pre></div>
<pre><code>## [0.         0.11111111 0.22222222 0.33333333 0.44444444 0.55555556
##  0.66666667 0.77777778 0.88888889 1.        ]</code></pre>
<p>Now suppose we want to find the probability of hypothesis <span class="math inline">\(B_5\)</span> after observing one draw <span class="math inline">\(B\)</span>. Let‚Äôs apply Bayes‚Äôs rule:</p>
<p><span class="math display">\[P(B_5 | B) = \frac{P(B|B_5) P(B_5)}{\sum_h{p(B|h) P(h)}}\]</span></p>
<p>Let‚Äôs compute the parts we need to calculate <span class="math inline">\(P(B_5 | B)\)</span>.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="bayes.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior</span></span>
<span id="cb13-2"><a href="bayes.html#cb13-2" aria-hidden="true" tabindex="-1"></a>p_B5 <span class="op">=</span> h_priors[<span class="dv">3</span>]</span>
<span id="cb13-3"><a href="bayes.html#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="bayes.html#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Likelihood</span></span>
<span id="cb13-5"><a href="bayes.html#cb13-5" aria-hidden="true" tabindex="-1"></a>likelihood_B5 <span class="op">=</span> likelihoods[<span class="dv">5</span>]</span>
<span id="cb13-6"><a href="bayes.html#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="bayes.html#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Data</span></span>
<span id="cb13-8"><a href="bayes.html#cb13-8" aria-hidden="true" tabindex="-1"></a>p_B <span class="op">=</span> <span class="bu">sum</span>(likelihoods<span class="op">*</span>h_priors)</span>
<span id="cb13-9"><a href="bayes.html#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="bayes.html#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior</span></span>
<span id="cb13-11"><a href="bayes.html#cb13-11" aria-hidden="true" tabindex="-1"></a>p_B5_given_B <span class="op">=</span> p_B5 <span class="op">*</span> likelihood_B5 <span class="op">/</span> p_B</span>
<span id="cb13-12"><a href="bayes.html#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="bayes.html#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Print out results</span></span>
<span id="cb13-14"><a href="bayes.html#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"P(B5) = "</span> <span class="op">+</span> <span class="bu">str</span>(p_B5)) <span class="co"># Prior</span></span></code></pre></div>
<pre><code>## P(B5) = 0.1</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="bayes.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"P(B|B5) = "</span> <span class="op">+</span> <span class="bu">str</span>(likelihood_B5)) <span class="co"># Likelihood</span></span></code></pre></div>
<pre><code>## P(B|B5) = 0.5555555555555556</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="bayes.html#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"P(B) = "</span> <span class="op">+</span> <span class="bu">str</span>(p_B)) <span class="co"># Data</span></span></code></pre></div>
<pre><code>## P(B) = 0.5</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="bayes.html#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"P(B5|B) = "</span> <span class="op">+</span> <span class="bu">str</span>(p_B5_given_B)) <span class="co"># Posterior</span></span></code></pre></div>
<pre><code>## P(B5|B) = 0.11111111111111112</code></pre>
<p>Let‚Äôs update the probabilities for all hypotheses in a more compact way.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="bayes.html#cb21-1" aria-hidden="true" tabindex="-1"></a>posteriors <span class="op">=</span> (likelihoods <span class="op">*</span> h_priors) <span class="op">/</span> <span class="bu">sum</span>(likelihoods <span class="op">*</span> h_priors)</span>
<span id="cb21-2"><a href="bayes.html#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="bayes.html#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(posteriors)):</span>
<span id="cb21-4"><a href="bayes.html#cb21-4" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"P(B"</span> <span class="op">+</span> <span class="bu">str</span>(i) <span class="op">+</span> <span class="st">"|B) = "</span> <span class="op">+</span> <span class="bu">str</span>(posteriors[i]))</span></code></pre></div>
<pre><code>## P(B0|B) = 0.0
## P(B1|B) = 0.022222222222222223
## P(B2|B) = 0.044444444444444446
## P(B3|B) = 0.06666666666666667
## P(B4|B) = 0.08888888888888889
## P(B5|B) = 0.11111111111111112
## P(B6|B) = 0.13333333333333333
## P(B7|B) = 0.15555555555555556
## P(B8|B) = 0.17777777777777778
## P(B9|B) = 0.2</code></pre>
<p>As expected, Bayes‚Äôs rule says we should increase the probability we assign to hypotheses with more black balls than red balls. Additionally, let‚Äôs double-check that the posterior probabilities sum to 1 (a requirement for a valid probability distribution).</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="bayes.html#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="bu">sum</span>(posteriors)</span></code></pre></div>
<pre><code>## 1.0</code></pre>
<p>Finally, let‚Äôs plot the posterior probabilities.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="bayes.html#cb25-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb25-2"><a href="bayes.html#cb25-2" aria-hidden="true" tabindex="-1"></a>hypotheses <span class="op">=</span> (<span class="st">'B0'</span>, <span class="st">'B1'</span>, <span class="st">'B2'</span>, <span class="st">'B3'</span>, <span class="st">'B4'</span>,</span>
<span id="cb25-3"><a href="bayes.html#cb25-3" aria-hidden="true" tabindex="-1"></a>              <span class="st">'B5'</span>, <span class="st">'B6'</span>, <span class="st">'B7'</span>, <span class="st">'B8'</span>, <span class="st">'B9'</span>)</span>
<span id="cb25-4"><a href="bayes.html#cb25-4" aria-hidden="true" tabindex="-1"></a>y_pos <span class="op">=</span> np.arange(<span class="bu">len</span>(hypotheses))</span>
<span id="cb25-5"><a href="bayes.html#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="bayes.html#cb25-6" aria-hidden="true" tabindex="-1"></a>ax.barh(y_pos, posteriors, align<span class="op">=</span><span class="st">'center'</span>)</span></code></pre></div>
<pre><code>## &lt;BarContainer object of 10 artists&gt;</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="bayes.html#cb27-1" aria-hidden="true" tabindex="-1"></a>ax.set_yticks(y_pos)</span>
<span id="cb27-2"><a href="bayes.html#cb27-2" aria-hidden="true" tabindex="-1"></a>ax.set_yticklabels(hypotheses)</span>
<span id="cb27-3"><a href="bayes.html#cb27-3" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Probability'</span>)</span>
<span id="cb27-4"><a href="bayes.html#cb27-4" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Hypothesis'</span>)</span></code></pre></div>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-15-5.png" width="672"></div>
</div>
<div id="how-to-avoid-calculating-pd" class="section level3" number="3.3.2">
<h3>
<span class="header-section-number">3.3.2</span> How to avoid calculating P(d)<a class="anchor" aria-label="anchor" href="#how-to-avoid-calculating-pd"><i class="fas fa-link"></i></a>
</h3>
<p>In practice, we generally do not need to calculate the <span class="math inline">\(P(d)\)</span> (the denominator in Bayes‚Äôs rule) explicitly. I‚Äôll give you the general idea why in this section.</p>
<p>First, we create a vector of prior probabilities, which has as many components as there are hypotheses. We‚Äôll just reuse <code>h_priors</code>. Note that the probabilities sum to 1, as they should because it‚Äôs a probability distribution.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="bayes.html#cb28-1" aria-hidden="true" tabindex="-1"></a>h_priors</span></code></pre></div>
<pre><code>## array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])</code></pre>
<p>Next, we create a likelihood array. When we did calculations above, we only had a vector with the likelihoods for a specific observation. However, we would like to have something that encodes the likelihood function for each possible observation given each possible hypothesis, rather than just for a specific observation.</p>
<p>In this example, there are two possible observations: <span class="math inline">\(B\)</span> and <span class="math inline">\(R\)</span>. We can encode the likelihood as an <span class="math inline">\(m \times n\)</span> array where <span class="math inline">\(m\)</span> is the number of hypotheses and <span class="math inline">\(n\)</span> is the number of possible observations. In our case: <span class="math inline">\(10 \times 2\)</span>.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="bayes.html#cb30-1" aria-hidden="true" tabindex="-1"></a>likelihood_array <span class="op">=</span> np.array((np.arange(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">1</span>) <span class="op">/</span> <span class="dv">9</span>,</span>
<span id="cb30-2"><a href="bayes.html#cb30-2" aria-hidden="true" tabindex="-1"></a>                             <span class="dv">1</span><span class="op">-</span>(np.arange(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">1</span>) <span class="op">/</span> <span class="dv">9</span>))).T</span>
<span id="cb30-3"><a href="bayes.html#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(likelihood_array)</span></code></pre></div>
<pre><code>## [[0.         1.        ]
##  [0.11111111 0.88888889]
##  [0.22222222 0.77777778]
##  [0.33333333 0.66666667]
##  [0.44444444 0.55555556]
##  [0.55555556 0.44444444]
##  [0.66666667 0.33333333]
##  [0.77777778 0.22222222]
##  [0.88888889 0.11111111]
##  [1.         0.        ]]</code></pre>
<p>Now we multiply the prior and likelihoods together (the numerator of Bayes‚Äôs rule) <em>element-wise</em> (first element gets multiplied with first element, second element by second element, etc.):</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="bayes.html#cb32-1" aria-hidden="true" tabindex="-1"></a>prior_array <span class="op">=</span> np.array((h_priors, h_priors)).T</span>
<span id="cb32-2"><a href="bayes.html#cb32-2" aria-hidden="true" tabindex="-1"></a>bayes_numerator <span class="op">=</span> likelihood_array <span class="op">*</span> prior_array</span>
<span id="cb32-3"><a href="bayes.html#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="bayes.html#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(bayes_numerator)</span></code></pre></div>
<pre><code>## [[0.         0.1       ]
##  [0.01111111 0.08888889]
##  [0.02222222 0.07777778]
##  [0.03333333 0.06666667]
##  [0.04444444 0.05555556]
##  [0.05555556 0.04444444]
##  [0.06666667 0.03333333]
##  [0.07777778 0.02222222]
##  [0.08888889 0.01111111]
##  [0.1        0.        ]]</code></pre>
<p>Finally, we want a distribution for each column, i.e., a distribution over hypotheses given each observation. Therefore, we sum each column and then divide each element by the sum of its column:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="bayes.html#cb34-1" aria-hidden="true" tabindex="-1"></a>posteriors <span class="op">=</span> bayes_numerator <span class="op">/</span> np.<span class="bu">sum</span>(bayes_numerator, axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb34-2"><a href="bayes.html#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(posteriors)</span></code></pre></div>
<pre><code>## [[0.         0.2       ]
##  [0.02222222 0.17777778]
##  [0.04444444 0.15555556]
##  [0.06666667 0.13333333]
##  [0.08888889 0.11111111]
##  [0.11111111 0.08888889]
##  [0.13333333 0.06666667]
##  [0.15555556 0.04444444]
##  [0.17777778 0.02222222]
##  [0.2        0.        ]]</code></pre>
<p>And that gives us the posterior without us having to explicitly calculate the evidence for each observation!</p>
<div class="rmdnote">
<p>The general idea is this. Because the denominator of Bayes‚Äôs rule, for a fixed observation, is a constant, you can usually get away with computing <span class="math inline">\(P(d|h) P(h)\)</span> for every possible hypothesis <span class="math inline">\(h\)</span> and then ‚Äúnormalize‚Äù the resulting values so that they sum to 1 (remember that they have to in order for it to be a valid probability distribution).</p>
</div>
</div>
</div>
<div id="exercises" class="section level2" number="3.4">
<h2>
<span class="header-section-number">3.4</span> Exercises üìù<a class="anchor" aria-label="anchor" href="#exercises"><i class="fas fa-link"></i></a>
</h2>
<div id="taxi-cabs" class="section level3" number="3.4.1">
<h3>
<span class="header-section-number">3.4.1</span> Taxi cabs<a class="anchor" aria-label="anchor" href="#taxi-cabs"><i class="fas fa-link"></i></a>
</h3>
<p>80% of the taxi cabs in Simpletown are green and 20% are yellow. An hit-and-run accident happened at night involving a taxi. A witness claimed that the taxi was yellow. After extensive testing, it is determined that the witness can correctly identify the color of a taxi only 75% of the time under conditions like the ones present during the accident. What is the probability that the taxi was yellow?</p>
</div>
<div id="flipping-coins" class="section level3" number="3.4.2">
<h3>
<span class="header-section-number">3.4.2</span> Flipping coins<a class="anchor" aria-label="anchor" href="#flipping-coins"><i class="fas fa-link"></i></a>
</h3>
<p>You observe a sequence of coin flips and want to determine if the coin is a trick coin (always comes up heads) or a normal coin. Let <span class="math inline">\(P(\text{heads}) = \theta\)</span>. Let <span class="math inline">\(h_1\)</span> be the hypothesis that <span class="math inline">\(\theta = 0.5\)</span> (fair coin). Let <span class="math inline">\(h_2\)</span> be the hypothesis that <span class="math inline">\(\theta = 1\)</span> (trick coin).</p>
<p>For this problem, we will define something called prior odds, which is the ratio of prior probabilities assigned to two hypotheses: <span class="math inline">\(\frac{P(h_1)}{P(h_2)}\)</span>. Because most coins aren‚Äôt trick coins, we assume that <span class="math inline">\(\frac{P(h_1)}{P(h_2)} = 999\)</span>, indicating a very strong (999 to 1) prior probability in favor of fair coins. We can now compute the posterior odds, the ratio of posterior probabilities for the two hypotheses after observing some data <span class="math inline">\(d\)</span>: <span class="math inline">\(\frac{P(h_1|d)}{P(h_2|d)}\)</span>.</p>
<p>Compute the posterior odds after observing the following sequences of coin flips:</p>
<ol style="list-style-type: decimal">
<li>HHTHT</li>
<li>HHHHH</li>
<li>HHHHHHHHHH</li>
</ol>
</div>
</div>
<div id="solutions" class="section level2" number="3.5">
<h2>
<span class="header-section-number">3.5</span> Solutions<a class="anchor" aria-label="anchor" href="#solutions"><i class="fas fa-link"></i></a>
</h2>
<div id="taxi-cabs-1" class="section level3" number="3.5.1">
<h3>
<span class="header-section-number">3.5.1</span> Taxi cabs<a class="anchor" aria-label="anchor" href="#taxi-cabs-1"><i class="fas fa-link"></i></a>
</h3>
<p>Let <span class="math inline">\(h_1\)</span> be the hypothesis that the taxi is yellow. Let <span class="math inline">\(h_2\)</span> be the hypothesis that the taxi is green. Let data <span class="math inline">\(d\)</span> be the witness report that the taxi was yellow. Given the problem statement, <span class="math inline">\(P(h_1) = 0.2\)</span> and <span class="math inline">\(P(h_2) = 0.8\)</span>. The witness is only accurate 75% of the time, so <span class="math inline">\(P(d|h1) = 0.75\)</span> (the witness saw a yellow taxi and correctly identified it) and <span class="math inline">\(P(d|h2) = 0.25\)</span> (the witness saw a green taxi but identified it as yellow). Now we apply Bayes‚Äôs rule:</p>
<p><span class="math display">\[\begin{align}
P(h_1|d) &amp;= \frac{P(d|h_1) P(h_1)}{P(d)} \\
&amp;= \frac{P(d|h_1) P(h_1)}{P(d|h_1) P(h_1) + P(d|h_2) P(h_2)} \\
&amp;= \frac{(0.75) (0.2)}{(0.75)(0.2) + (0.25)(0.8)} \approx 0.43
\end{align}\]</span></p>
<p>Because yellow cabs are rare (have low prior probability), it is actually more probable that the cab was green, even though the witness is 75% accurate.</p>
</div>
<div id="flipping-coins-1" class="section level3" number="3.5.2">
<h3>
<span class="header-section-number">3.5.2</span> Flipping coins<a class="anchor" aria-label="anchor" href="#flipping-coins-1"><i class="fas fa-link"></i></a>
</h3>
<div id="hhtht" class="section level4 unnumbered">
<h4>HHTHT<a class="anchor" aria-label="anchor" href="#hhtht"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[
\begin{align}
\frac{P(h_1|d)}{P(h_2|d)} &amp;= \frac{P(d|h_1)}{P(d|h_2)} \frac{P(h_1)}{P(h_2)} \\
&amp;= \frac{(1/2)^5}{0} \times 999 = \inf
\end{align}
\]</span>
This sequence isn‚Äôt even possible under <span class="math inline">\(h_2\)</span> so we have infinite evidence in favor of <span class="math inline">\(h_1\)</span>.</p>
</div>
<div id="hhhhh" class="section level4 unnumbered">
<h4>HHHHH<a class="anchor" aria-label="anchor" href="#hhhhh"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[
\begin{align}
\frac{P(h_1|d)}{P(h_2|d)} &amp;= \frac{P(d|h_1)}{P(d|h_2)} \frac{P(h_1)}{P(h_2)} \\
&amp;= \frac{(1/2)^5}{1^5} \times 999 = 31.2
\end{align}
\]</span></p>
<p>This sequence favors <span class="math inline">\(h_1\)</span> by a factor of about 31. Even five heads in a row can‚Äôt overcome our strong prior favoring <span class="math inline">\(h_1\)</span>.</p>
</div>
<div id="hhhhhhhhhh" class="section level4 unnumbered">
<h4>HHHHHHHHHH<a class="anchor" aria-label="anchor" href="#hhhhhhhhhh"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[
\begin{align}
\frac{P(h_1|d)}{P(h_2|d)} &amp;= \frac{P(d|h_1)}{P(d|h_2)} \frac{P(h_1)}{P(h_2)} \\
&amp;= \frac{(1/2)^{10}}{1^{10}} \times 999 = 0.98
\end{align}
\]</span></p>
<p>Now the evidence favors <span class="math inline">\(h_2\)</span> (trick coin) just barely.</p>

</div>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="intro.html"><span class="header-section-number">2</span> Why computational modeling?</a></div>
<div class="next"><a href="generalization.html"><span class="header-section-number">4</span> Generalization</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#bayes"><span class="header-section-number">3</span> Bayesian inference</a></li>
<li><a class="nav-link" href="#basic-probability"><span class="header-section-number">3.1</span> Basic probability üé≤</a></li>
<li>
<a class="nav-link" href="#a-motivating-example-sampling-from-a-bag"><span class="header-section-number">3.2</span> A motivating example: Sampling from a bag üëù</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#sampling-from-the-generative-model"><span class="header-section-number">3.2.1</span> Sampling from the generative model</a></li></ul>
</li>
<li>
<a class="nav-link" href="#bayesian-updating-learning-from-evidence"><span class="header-section-number">3.3</span> Bayesian updating: Learning from evidence ü§î</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#applying-bayess-rule-to-the-bag-case"><span class="header-section-number">3.3.1</span> Applying Bayes‚Äôs rule to the bag case</a></li>
<li><a class="nav-link" href="#how-to-avoid-calculating-pd"><span class="header-section-number">3.3.2</span> How to avoid calculating P(d)</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#exercises"><span class="header-section-number">3.4</span> Exercises üìù</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#taxi-cabs"><span class="header-section-number">3.4.1</span> Taxi cabs</a></li>
<li><a class="nav-link" href="#flipping-coins"><span class="header-section-number">3.4.2</span> Flipping coins</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#solutions"><span class="header-section-number">3.5</span> Solutions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#taxi-cabs-1"><span class="header-section-number">3.5.1</span> Taxi cabs</a></li>
<li><a class="nav-link" href="#flipping-coins-1"><span class="header-section-number">3.5.2</span> Flipping coins</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Computational Psychology in Python</strong>" was written by Alan Jern. It was last built on 2022-03-05.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
