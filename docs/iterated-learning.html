<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 9 Iterated learning | Introduction to Computational Psychology</title>
<meta name="author" content="Alan Jern">
<meta name="description" content="If someone told you they defecated their water before drinking it, you probably wouldn‚Äôt be too impressed. Unless you lived in the 1600s, when ‚Äúdefecate‚Äù meant ‚Äúto purify something‚Äù. Language...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 9 Iterated learning | Introduction to Computational Psychology">
<meta property="og:type" content="book">
<meta property="og:url" content="https://alanjern.github.io/computational-psych-book/iterated-learning.html">
<meta property="og:image" content="https://alanjern.github.io/computational-psych-book/images/cover/marble_notext.jpg">
<meta property="og:description" content="If someone told you they defecated their water before drinking it, you probably wouldn‚Äôt be too impressed. Unless you lived in the 1600s, when ‚Äúdefecate‚Äù meant ‚Äúto purify something‚Äù. Language...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 9 Iterated learning | Introduction to Computational Psychology">
<meta name="twitter:description" content="If someone told you they defecated their water before drinking it, you probably wouldn‚Äôt be too impressed. Unless you lived in the 1600s, when ‚Äúdefecate‚Äù meant ‚Äúto purify something‚Äù. Language...">
<meta name="twitter:image" content="https://alanjern.github.io/computational-psych-book/images/cover/marble_notext.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.9/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><link rel="apple-touch-icon" sizes="180x180" href="images/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicons/favicon-16x16.png">
<link rel="manifest" href="images/favicons/site.webmanifest">
<link rel="shortcut icon" href="images/favicons/favicon.ico">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="images/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Introduction to Computational Psychology</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Why computational modeling?</a></li>
<li><a class="" href="bayes.html"><span class="header-section-number">2</span> Bayesian inference</a></li>
<li><a class="" href="generalization.html"><span class="header-section-number">3</span> Generalization</a></li>
<li><a class="" href="categorization.html"><span class="header-section-number">4</span> Categorization</a></li>
<li><a class="" href="hierarchical-generalization.html"><span class="header-section-number">5</span> Hierarchical generalization</a></li>
<li><a class="" href="sampling-assumptions.html"><span class="header-section-number">6</span> Sampling assumptions</a></li>
<li><a class="" href="pragmatics.html"><span class="header-section-number">7</span> Language pragmatics</a></li>
<li><a class="" href="social-cognition.html"><span class="header-section-number">8</span> Social cognition</a></li>
<li><a class="active" href="iterated-learning.html"><span class="header-section-number">9</span> Iterated learning</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/alanjern/computational-psych-book">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="iterated-learning" class="section level1" number="9">
<h1>
<span class="header-section-number">9</span> Iterated learning<a class="anchor" aria-label="anchor" href="#iterated-learning"><i class="fas fa-link"></i></a>
</h1>
<p>If someone told you they defecated their water before drinking it, you probably wouldn‚Äôt be too impressed. Unless you lived in the 1600s, when ‚Äúdefecate‚Äù <a href="https://www.mentalfloss.com/article/54770/15-words-dont-mean-what-they-used">meant ‚Äúto purify something‚Äù</a>.</p>
<p>Language changes. Words change. Concepts change. Is there any way to predict the conceptual drift that is always happening in our culture?</p>
<p>Yeah, maybe, kinda! But first, some math.</p>
<div id="markov-chains" class="section level2" number="9.1">
<h2>
<span class="header-section-number">9.1</span> Markov chains üîó<a class="anchor" aria-label="anchor" href="#markov-chains"><i class="fas fa-link"></i></a>
</h2>
<p>Words and concepts change over <em>time</em>. <strong>Markov chains</strong> provide a useful modeling framework for time-dependent data.</p>
<p>As the name suggests, a Markov chain is a chain, specifically of states. At each time step, it moves to a new state. A Markov chain meets the following conditions:</p>
<ol style="list-style-type: decimal">
<li>A system can be in a finite number of states.</li>
<li>The state at each step of the chain depends only on the previous state. (This dependence can be probabilistic.)</li>
</ol>
<p>For example, consider the game Telephone, where one person whispers a word to the next person, they whisper what they hear to the next person, and so on. The set of possible words are the states (finite but admittedly large).</p>
<p>The word that Person <span class="math inline">\(n+1\)</span> hears clearly only depends on the word that Person <span class="math inline">\(n\)</span> whispered. But there might be some miscommunication: the probability that Person <span class="math inline">\(n+1\)</span> accurately hears the word is probably less than 1, with similar-sounding words getting higher probability than dissimilar words.</p>
<div id="examples" class="section level3" number="9.1.1">
<h3>
<span class="header-section-number">9.1.1</span> Examples<a class="anchor" aria-label="anchor" href="#examples"><i class="fas fa-link"></i></a>
</h3>
<p>Consider a very simple chain of colors that are either red or blue. Here, the state space <span class="math inline">\(S = \{ \text{red, blue} \}\)</span>. To specify this Markov chain, we need to define the <em>transition probabilities</em>: the probabilities of moving from each state to every other state.</p>
<p>Let <span class="math inline">\(t_{ij}\)</span> be the probability of transitioning from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span>. For example, if <span class="math inline">\(t_{RB} = 0.3\)</span> then the probability of transitioning from the red state to the blue state is 0.3. We can then write the transition probabilities in a matrix.</p>
<div id="markov-exmaple1" class="section level4" number="9.1.1.1">
<h4>
<span class="header-section-number">9.1.1.1</span> Example 1<a class="anchor" aria-label="anchor" href="#markov-exmaple1"><i class="fas fa-link"></i></a>
</h4>
<p>Here‚Äôs our transition probability matrix.</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th></th>
<th>R</th>
<th>B</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>R</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>B</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table></div>
<p>In this example, <span class="math inline">\(t_{RB} = t_{BR} = 1\)</span>. This means that the chain will deterministically alternate between red and blue. Note that the rows of the transition matrix must sum to 1 because they represent all the possible states that the chain might transition to given a current state. Although the columns also sum to 1 in this example, this is not a requirement.</p>
<p><em>Example sequence</em>: <span style="color: red;">R</span> <span style="color: blue;">B</span> <span style="color: red;">R</span> <span style="color: blue;">B</span> <span style="color: red;">R</span> <span style="color: blue;">B</span> ‚Ä¶</p>
</div>
<div id="markov-example2" class="section level4" number="9.1.1.2">
<h4>
<span class="header-section-number">9.1.1.2</span> Example 2<a class="anchor" aria-label="anchor" href="#markov-example2"><i class="fas fa-link"></i></a>
</h4>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th></th>
<th>R</th>
<th>B</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>R</td>
<td>0.25</td>
<td>0.75</td>
</tr>
<tr class="even">
<td>B</td>
<td>0.75</td>
<td>0.25</td>
</tr>
</tbody>
</table></div>
<p>In this example, we‚Äôd expect to the sequence to flip back and forth between red and blue, but not deterministically. At every step, the sequence is more likely to flip than to stay in the current color.</p>
<p><em>Example sequence</em>: <span style="color: red;">R</span> <span style="color: blue;">B</span> <span style="color: red;">R</span> <span style="color: red;">R</span> <span style="color: blue;">B</span> <span style="color: red;">R</span> <span style="color: red;">R</span> <span style="color: blue;">B</span> <span style="color: red;">R</span> <span style="color: blue;">B</span> <span style="color: red;">R</span> ‚Ä¶</p>
</div>
<div id="markov-example3" class="section level4" number="9.1.1.3">
<h4>
<span class="header-section-number">9.1.1.3</span> Example 3<a class="anchor" aria-label="anchor" href="#markov-example3"><i class="fas fa-link"></i></a>
</h4>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th></th>
<th>R</th>
<th>B</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>R</td>
<td>0.75</td>
<td>0.25</td>
</tr>
<tr class="even">
<td>B</td>
<td>0.75</td>
<td>0.25</td>
</tr>
</tbody>
</table></div>
<p>In this example, no matter what state we‚Äôre in, there is a 0.75 probability of moving to red and a 0.25 probability of moving to blue. Therefore, in this example, we‚Äôd expect the chain to spend about 3/4 of its time in the red state and 1/4 of its time in the blue state. This example makes clear that the columns do not need to sum to 1.</p>
<p><em>Example sequence</em>: <span style="color: red;">R</span> <span style="color: red;">R</span> <span style="color: blue;">B</span> <span style="color: blue;">B</span> <span style="color: red;">R</span> <span style="color: red;">R</span> <span style="color: blue;">B</span> <span style="color: blue;">B</span> <span style="color: red;">R</span> <span style="color: red;">R</span> <span style="color: red;">R</span> ‚Ä¶</p>
</div>
</div>
<div id="stationary-distributions" class="section level3" number="9.1.2">
<h3>
<span class="header-section-number">9.1.2</span> Stationary distributions<a class="anchor" aria-label="anchor" href="#stationary-distributions"><i class="fas fa-link"></i></a>
</h3>
<p>A common question for a given Markov chain is what is its so-called <em>stationary distribution</em>, the proportion of time the chain will spend in each state if it runs for a very long time. For Markov chains a small number of states, computing the stationary distribution is easy to do with some algebra. Consider the following example.</p>
<p>Let‚Äôs define a transition matrix for Markov chain with two states, <span class="math inline">\(s_1\)</span> and <span class="math inline">\(s_2\)</span>:</p>
<p><span class="math display">\[
\begin{equation}
T = \left(
\begin{matrix}
t_{11} &amp; t_{12} \\
t_{21} &amp; t_{22}
\end{matrix} \right)
\end{equation}
\]</span></p>
<p>The stationary distribution corresponds to the probability of the chain being in state <span class="math inline">\(s_1\)</span> versus <span class="math inline">\(s_2\)</span>. We will define <span class="math inline">\(\theta_1\)</span> as the probability of the chain being in state <span class="math inline">\(s_1\)</span> and <span class="math inline">\(\theta_2\)</span> as the probability of the chain being in state <span class="math inline">\(s_2\)</span>. We can define <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span> recursively as follows:</p>
<p><span class="math display">\[
\begin{eqnarray}
\theta_1 = t_{11} \theta_1 + t_{21} \theta_2 \\
\theta_2 = t_{22} \theta_2 + t_{12} \theta_1 \\
\end{eqnarray}
\]</span></p>
<p>These equations essentially say that the probability of being in a state is the probability of being in that state and staying in that state plus the probability of being in the other state and switching states. We will then use the following facts to solve for the <span class="math inline">\(\theta\)</span>s:</p>
<p><span class="math display">\[
\begin{eqnarray}
\theta_1 + \theta_2 = 1 \\
t_{11} + t_{12} = 1 \\
t_{21} + t_{22} = 1
\end{eqnarray}
\]</span></p>
<p>Using some algebra, we get the following expressions:</p>
<p><span class="math display">\[
\begin{eqnarray}
\theta_1 = \frac{t_{12}}{t_{12}+t_{21}} \\
\theta_2 = \frac{t_{21}}{t_{21}+t_{12}}
\end{eqnarray}
\]</span></p>
<p>We can see that these expressions give sensible results by applying them to the red-blue examples above.</p>
<p>In <a href="#markov-example1">Example 1</a>, unsurprisingly, <span class="math inline">\(\theta_R = \theta_B = 0.5\)</span>. Because the sequence alternates, it will be red half the time and blue half the time. In <a href="iterated-learning.html#markov-example2">Example 2</a>, again <span class="math inline">\(\theta_R = \theta_B = 0.5\)</span>. Even though the alternating is not deterministic, no matter what state the sequence is in, there is an equal probability that it will switch to the other state. In <a href="iterated-learning.html#markov-example3">Example 3</a>, <span class="math inline">\(\theta_R = 0.75\)</span> and <span class="math inline">\(\theta_B = 0.25\)</span>.</p>
</div>
</div>
<div id="hmms" class="section level2" number="9.2">
<h2>
<span class="header-section-number">9.2</span> Hidden Markov models üôà<a class="anchor" aria-label="anchor" href="#hmms"><i class="fas fa-link"></i></a>
</h2>
<p>Now let‚Äôs consider a more complicated situation in which each step in a Markov chain generates an observation. A learner gets to see the observations, but not the states themselves. In this situation, we say that the states are ‚Äúhidden,‚Äù hence the name <strong>hidden Markov model</strong>.</p>
<p>Hidden Markov models are sometimes used to model language production, where the hidden states are the parts of speech (like noun, verb, adjective, or more complex things like noun phrases, objects, and subjects). What we actually get to observe as readers and listeners, however, are words (like ‚Äúdog,‚Äù ‚Äúeats,‚Äù and ‚Äúplay‚Äù). In English, many words are ambiguous and can potentially belong to different parts of speech. ‚ÄúPlay,‚Äù for example could be a noun (‚ÄúThe actors performed in a play‚Äù) or a verb (‚ÄúThe children play in the sand‚Äù). We are constantly decoding the language we observe to infer the parts of speech of the words.</p>
<div id="formal-definition" class="section level3" number="9.2.1">
<h3>
<span class="header-section-number">9.2.1</span> Formal definition<a class="anchor" aria-label="anchor" href="#formal-definition"><i class="fas fa-link"></i></a>
</h3>
<p>A hidden Markov model includes the following pieces:</p>
<ol style="list-style-type: decimal">
<li>A set of states <span class="math inline">\(S = \{s_1, \ldots, s_N\}\)</span>
</li>
<li>A set of observations <span class="math inline">\(O = \{o_1, \ldots, o_M\}\)</span>
</li>
<li>A set of initial state probabilities <span class="math inline">\(\Pi = \{\pi_1, \ldots, \pi_N\}\)</span>, where <span class="math inline">\(\pi_i\)</span> is the probability of state <span class="math inline">\(i\)</span> begin the first state in the sequence</li>
<li>A matrix of state transition probabilities <span class="math inline">\(T\)</span>, where <span class="math inline">\(t_{ij}\)</span> in the matrix is the probability of transitioning from state <span class="math inline">\(s_i\)</span> to state <span class="math inline">\(s_j\)</span>
</li>
<li>A matrix of ‚Äúemission‚Äù probabilities <span class="math inline">\(B\)</span>, where <span class="math inline">\(b_{ik}\)</span> in the matrix is the probability of producing observation <span class="math inline">\(o_k\)</span> while in state <span class="math inline">\(s_i\)</span>
</li>
<li>An observation sequence <span class="math inline">\(Y = (y_1, \ldots, y_T)\)</span>, where <span class="math inline">\(T\)</span> is the length of the observed sequence of data</li>
<li>An state sequence that is not observed <span class="math inline">\(X = (x_1, \ldots, x_T)\)</span>
</li>
</ol>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-71"></span>
<img src="images/09/hmm.png" alt="A hidden Markov model. Source: Wikipedia (public domain)." width="85%"><p class="caption">
Figure 9.1: A hidden Markov model. Source: Wikipedia (public domain).
</p>
</div>
</div>
<div id="inference" class="section level3" number="9.2.2">
<h3>
<span class="header-section-number">9.2.2</span> Inference<a class="anchor" aria-label="anchor" href="#inference"><i class="fas fa-link"></i></a>
</h3>
<p>Usually the purpose of using an HMM is to infer <span class="math inline">\(X\)</span> from <span class="math inline">\(Y\)</span>. There is a standard algorithm for doing this called <a href="https://en.wikipedia.org/wiki/Viterbi_algorithm">the Viterbi algorithm</a>, the details of which are beyond the scope of this book.</p>
<p>There are many existing programming packages available for working with HMMs. On Python package is <a href="https://github.com/hmmlearn/hmmlearn">hmmlearn</a>, which includes a <a href="https://hmmlearn.readthedocs.io/en/stable/api.html#hmmlearn.base.BaseHMM.predict"><code>predict()</code></a> function, which will predict the most probable sequence of hidden states <span class="math inline">\(X\)</span>, given a sequence of observations <span class="math inline">\(Y\)</span> and a fully specified Hidden Markov model. Note that this function will not always be correct. If the model is probabilistic and there are some observations that can be produced in more than one state (for example, the word ‚Äúplay‚Äù could be produced by either the noun or verb state), there will be some cases where it is impossible to be certain which state an observation came from.</p>
</div>
</div>
<div id="cultural-transmission" class="section level2" number="9.3">
<h2>
<span class="header-section-number">9.3</span> Cultural transmission üó£<a class="anchor" aria-label="anchor" href="#cultural-transmission"><i class="fas fa-link"></i></a>
</h2>
<p>So what does all this have to do with ‚Äúdefecate‚Äù changing meaning over time?</p>
<p>Well you can think of communication over time as one giant long-term game of Telephone. And an important question you can then ask is: What is the stationary distribution of these chains?</p>
<p>This is the subject of a line research in computational cognitive science. The basic model combines some assumptions we‚Äôve seen before with some assumptions of hidden Markov models you just learned about in this chapter.</p>
<p>First, assume that people are trying to learn some concept by seeing an example <span class="math inline">\(x_i\)</span> and getting a label for it <span class="math inline">\(y_i\)</span> and then trying to generalize that label to other instances by forming a hypothesis <span class="math inline">\(h\)</span> about what other things <span class="math inline">\(y_i\)</span> applies to. This is the problem we encountered in the <a href="generalization.html#generalization">generalization chapter</a>, and we can assume that each person forms their beliefs about <span class="math inline">\(h\)</span> using Bayesian inference.</p>
<p>Now let‚Äôs add in the communication factor. Each person <span class="math inline">\(n+1\)</span> gets a label <span class="math inline">\(y_n\)</span> from someone else Person <span class="math inline">\(n\)</span>, forms their own hypothesis <span class="math inline">\(h_{n+1}\)</span> then labels a new object <span class="math inline">\(x_{n+1}\)</span> for the next person. This process repeats, creating a Markov chain.</p>
<p>To simplify this chain, what we end up with is a sequence of states where each state is a hypothesis about the concept.</p>
<p><em>What will this chain converge to?</em></p>
<p><strong>It turns out that the stationary distribution of this chain is the prior <span class="math inline">\(P(h)\)</span></strong>. Because people make mistakes and communication is noisy, over time, the labels <span class="math inline">\(y_n\)</span> become virtually useless and people will converge to their shared prior beliefs.</p>
<div id="using-iterated-learning-to-identify-peoples-priors" class="section level3" number="9.3.1">
<h3>
<span class="header-section-number">9.3.1</span> Using iterated learning to identify people‚Äôs priors<a class="anchor" aria-label="anchor" href="#using-iterated-learning-to-identify-peoples-priors"><i class="fas fa-link"></i></a>
</h3>
<p>For this reason, we can use a kind of Telephone game as a way to find out what people‚Äôs shared prior beliefs are.</p>
<p><a href="https://link.springer.com/content/pdf/10.3758/BF03194066.pdf">A study lead by Michael Kalish</a> tested this idea for function learning: learning to predict <span class="math inline">\(y\)</span> values from <span class="math inline">\(x\)</span> values.</p>
<p>In the task, people were given <span class="math inline">\(x\)</span> values and had to predict the corresponding <span class="math inline">\(y\)</span> values using a slider. They got feedback on 50 training examples. Then they did an additional 25 trials without feedback.</p>
<p>Those 25 test trials were doubled and used as the 50 training examples for the next person, and so on. What functions would people eventually learn?</p>
<p>The figure below shows several representative chains. The researchers found that regardless of what the initial training data was, the chains usually converged to a linear increasing function, suggesting that this is what most people were expecting.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-72"></span>
<img src="images/09/kalishetal_functionlearning_results.png" alt="Iterated function learning results from Kalish et al. (2007)." width="95%"><p class="caption">
Figure 9.2: Iterated function learning results from Kalish et al.¬†(2007).
</p>
</div>
<p>This same method has been applied in other domains as well. For example, in <a href="https://suchow.io/assets/docs/suchow2016dzp.pdf">a study lead by Jordan Suchow</a>, people had to reproduce as accurately as possible the positions of characters in a word. The initial positions were randomly positioned, and what each person reproduced would be passed to the next person in the chain.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-73"></span>
<img src="images/09/suchowetal_letterspacing.png" alt="Iterated typestting results from Suchow et al. (2016)." width="70%"><p class="caption">
Figure 9.3: Iterated typestting results from Suchow et al.¬†(2016).
</p>
</div>
<p>The researchers found that, over time, people drifted much closer to equal spaced letters, which was much more legible. Moreover, if they averaged the responses across all chains (DZP<sub>2</sub> in the figure), they found that people‚Äôs responses were even better than equal spacing, getting closer to Linotype, recommended by designers.</p>

</div>
</div>
</div>









  <div class="chapter-nav">
<div class="prev"><a href="social-cognition.html"><span class="header-section-number">8</span> Social cognition</a></div>
<div class="empty"></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#iterated-learning"><span class="header-section-number">9</span> Iterated learning</a></li>
<li>
<a class="nav-link" href="#markov-chains"><span class="header-section-number">9.1</span> Markov chains üîó</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#examples"><span class="header-section-number">9.1.1</span> Examples</a></li>
<li><a class="nav-link" href="#stationary-distributions"><span class="header-section-number">9.1.2</span> Stationary distributions</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#hmms"><span class="header-section-number">9.2</span> Hidden Markov models üôà</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#formal-definition"><span class="header-section-number">9.2.1</span> Formal definition</a></li>
<li><a class="nav-link" href="#inference"><span class="header-section-number">9.2.2</span> Inference</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#cultural-transmission"><span class="header-section-number">9.3</span> Cultural transmission üó£</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#using-iterated-learning-to-identify-peoples-priors"><span class="header-section-number">9.3.1</span> Using iterated learning to identify people‚Äôs priors</a></li></ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/alanjern/computational-psych-book/blob/main/09-iterated-learning.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/alanjern/computational-psych-book/edit/main/09-iterated-learning.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Introduction to Computational Psychology</strong>" was written by Alan Jern. It was last built on 2022-04-24.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
