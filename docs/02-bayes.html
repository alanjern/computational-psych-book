<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>bayes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        
    <div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="02-bayes.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Bayesian inference</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>This chapter reviews some basic probability and Bayesian inference. You might be asking yourself: <em>What does this have to do with psychology?</em> The answer becomes clear when you recognize that most of what we do when we‚Äôre making sense of the world is drawing inferences. When you see an ambiguous image, is it a rabbit or a duck? When someone mumbles something, did they say ‚Äúhello‚Äù or ‚Äúgo to hell‚Äù? When you take a pill and your headache goes away, did the pill eliminate your headache or did the headache go away on its own?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/02/duck-rabbit.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">The duck-rabbit illusion</figcaption>
</figure>
</div>
<p>In all of these examples, there is more than one hypothesis about what we observed. Probability and Bayesian inference provide the tools for optimally determining how probable these different hypotheses are. One of the claims of this book is that when people are making inferences in situations like these, their inferences are often well predicted by the optimal inferences dictated by probability theory.</p>
<section id="basic-probability" class="level2">
<h2 class="anchored" data-anchor-id="basic-probability">Basic probability üé≤</h2>
<ul>
<li>Conditional probability: <span class="math inline">\(P(b|a) = \frac{P(a,b)}{P(a)}\)</span></li>
<li>Chain rule: <span class="math inline">\(P(a,b) = P(b|a)P(a)\)</span></li>
<li>Marginalization: <span class="math inline">\(P(d) = \sum_h P(d,h) = \sum_h P(d|h) P(h)\)</span></li>
<li>Bayes‚Äôs rule: <span class="math inline">\(P(h|d) = \frac{P(d|h) P(h)}{P(d)} = \frac{P(d|h) P(h)}{\sum_h P(d|h) P(h)}\)</span>. <span class="math inline">\(P(d|h)\)</span> is referred to as the <em>likelihood</em>, <span class="math inline">\(P(h)\)</span> is the <em>prior</em>, and <span class="math inline">\(P(h|d)\)</span> is the <em>posterior</em>.</li>
</ul>
</section>
<section id="a-motivating-example-sampling-from-a-bag" class="level2">
<h2 class="anchored" data-anchor-id="a-motivating-example-sampling-from-a-bag">A motivating example: Sampling from a bag üëù</h2>
<p>Suppose you have a bag full of black and red balls. You can‚Äôt see inside the bag and you don‚Äôt know how many black and red balls are inside, but you know that there are nine total balls in the bag.</p>
<p>You want to know how many black balls and red balls there are. There are a finite number of hypotheses: {0 black balls, 1 black ball, 2 black balls, ‚Ä¶, 9 black balls}. Let‚Äôs call these hypotheses <span class="math inline">\(B_0\)</span>, <span class="math inline">\(B_1\)</span>, etc., respectively.</p>
<p>You don‚Äôt know which hypothesis is true, but you might have some idea which hypotheses are more likely than others. Therefore, it is natural to represent your uncertainty with a probability distribution over the possible unknown states that the world could be in ‚Äì in this case, the 10 hypotheses. Each hypothesis gets assigned a probability, and the probabilities sum to 1.</p>
<p>For simplicity, let‚Äôs assume that you <em>don‚Äôt</em> have any idea which hypotheses are more likely. In other words, you give every hypothesis the same probability: 1/10 = 0.1. This is also called a uniform distribution over hypotheses. This distribution is your prior.</p>
<p>Now suppose you put your hand in the bag and pull out a ball at random. The possible observations are: <code>{black, red}</code>, Let‚Äôs call them <span class="math inline">\(B\)</span> and <span class="math inline">\(R\)</span>, respectively. The probability of observing each color depends on which hypothesis is true, i.e., how many balls of each color are in the bag. For instance, if <span class="math inline">\(B_0\)</span> is true (there are 0 black balls in the bag), then the probability of observing a red ball is 1 (<span class="math inline">\(P(R|B_0)=1\)</span>), and the probability of observing a black ball is 0 (<span class="math inline">\(P(B|B_0)=0\)</span>). These expressions that tell us how probable our observations are, given a specific hypothesis, are your likelihoods.</p>
<section id="sampling-from-the-generative-model" class="level3">
<h3 class="anchored" data-anchor-id="sampling-from-the-generative-model">Sampling from the generative model</h3>
<p>Now we have a distribution over hypotheses (a prior), <span class="math inline">\(P(h)\)</span>, and a distribution over observations given each hypothesis (a likelihood), <span class="math inline">\(P(d|h)\)</span>. These two things allow us to create a <em>generative model</em>, a model for sampling new data.</p>
<p>How do we sample from the generative model? Note that which hypothesis <span class="math inline">\(h\)</span> is true does not depend on the data, while the data <span class="math inline">\(d\)</span> depends on which hypothesis is true. Therefore, we can sample from the generative model using the following two-step process:</p>
<ol type="1">
<li>Sample a hypothesis from the prior.</li>
<li>Sample data given the hypothesis, using the likelihood.</li>
</ol>
<p>Let‚Äôs first create a vector with the probability of each hypothesis:</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">2022</span>) <span class="co"># set random seed to get same results every time</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>h_priors <span class="op">=</span> np.repeat(<span class="fl">0.1</span>,<span class="dv">10</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(h_priors)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]</code></pre>
</div>
</div>
<p>Now we‚Äôll do the first step: create a vector of 10000 hypotheses sampled from the prior:</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>prior_samples <span class="op">=</span> np.array(random.choices(np.arange(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">1</span>),</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>                   weights <span class="op">=</span> h_priors, </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>                   k <span class="op">=</span> <span class="dv">10000</span>))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>                   </span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prior_samples[<span class="dv">0</span>:<span class="dv">9</span>]) <span class="co"># printing out just a few</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[5 4 3 0 7 9 4 6 8]</code></pre>
</div>
</div>
<p>Here, each number corresponds to one hypothesis: 0 corresponds to <span class="math inline">\(B_0\)</span>, 1 to <span class="math inline">\(B_1\)</span>, and so on. Each sample represents one possible way (a hypothesis) the world could be. Since the prior was uniform (each hypothesis had the same probability), each hypothesis appears about equally often. We can plot all the samples to verify:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>n, bins, patches <span class="op">=</span> ax.hist(prior_samples, bins<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Prior sample'</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Number of samples'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>Text(0, 0.5, 'Number of samples')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="02-bayes_files/figure-html/cell-4-output-2.png" width="601" height="422"></p>
</div>
</div>
<p>Now for the next step. For each sample in <code>prior_samples</code>, we want to sample an observation. To do that, let‚Äôs pause for a second and think about the probability of pulling a black ball given that hypothesis <span class="math inline">\(B_3\)</span> is true, for example. This means that there are 3 black balls and 6 red balls in the bag. So the probability of pulling a black ball from the bag at random will be 3/9.</p>
<p>Generalizing this idea, we can get the probability of pulling a black ball from the bag by dividing the elements of <code>prior_samples</code> by 9:</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>p_black <span class="op">=</span> prior_samples <span class="op">/</span> <span class="dv">9</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(p_black[<span class="dv">0</span>:<span class="dv">4</span>]) <span class="co"># print out just a few</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.55555556 0.44444444 0.33333333 0.        ]</code></pre>
</div>
</div>
<p>Now, to complete our generative model, we just need to sample one value for each element of <code>p_black</code>. Each sample represents a draw from a bag.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>ball_samples <span class="op">=</span> np.random.binomial(n <span class="op">=</span> np.repeat(<span class="dv">1</span>,<span class="bu">len</span>(p_black)),</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>                                  p <span class="op">=</span> p_black)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ball_samples[<span class="dv">0</span>:<span class="dv">9</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1 0 1 0 1 1 0 1 1]</code></pre>
</div>
</div>
<p><code>ball_samples</code> is 1 for black and 0 for red. Once again, let‚Äôs plot all our samples.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>n, bins, patches <span class="op">=</span> ax.hist(ball_samples, bins<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Number of samples'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>Text(0, 0.5, 'Number of samples')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="02-bayes_files/figure-html/cell-7-output-2.png" width="601" height="404"></p>
</div>
</div>
<p>You can think of this plot representing our overall beliefs about the number of red and black balls in the bag, averaged over all possible hypotheses.</p>
<p>Not surprisingly, we got about equal numbers of red and black balls. This makes sense: We didn‚Äôt have any prior expectations about whether red or black balls were more likely in the bag.</p>
<p>How should our beliefs change after we pull a ball out of the bag? That is, how should we respond to evidence?</p>
</section>
</section>
<section id="bayesian-updating-learning-from-evidence" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-updating-learning-from-evidence">Bayesian updating: Learning from evidence ü§î</h2>
<p>Let‚Äôs apply <a href="#basic-probability">Bayes‚Äôs rule</a> to see how to optimally incorporate new data into your beliefs.</p>
<section id="applying-bayess-rule-to-the-bag-case" class="level3">
<h3 class="anchored" data-anchor-id="applying-bayess-rule-to-the-bag-case">Applying Bayes‚Äôs rule to the bag case</h3>
<p>Suppose you have a uniform prior distribution over the 10 hypotheses about balls in the bag. Now you pick a ball and it‚Äôs black. Given this observation <span class="math inline">\(B\)</span>, how should you change the probabilities you give to each hypothesis?</p>
<p>Intuitively, you should now give a little bit more probability to those hypotheses that have more black balls than red balls, because those are the hypotheses that make your observations more likely. Moreover, you can safely exclude hypothesis <span class="math inline">\(B_0\)</span>, because your observation would be impossible if <span class="math inline">\(B_0\)</span> were true. Let‚Äôs calculate this with Bayes‚Äôs rule.</p>
<p>The prior is the vector <code>h_priors</code> defined above. Given that we have observed <span class="math inline">\(B\)</span>, the likelihood should tell us, for each hypothesis, the probability of <span class="math inline">\(B\)</span> given that hypothesis. For example, for <span class="math inline">\(B_9\)</span>, the likelihood <span class="math inline">\(P(B|B_9) = 1\)</span>. For <span class="math inline">\(B_8\)</span>, <span class="math inline">\(P(B|B_8) = 8/9\)</span>, because 8 of the 9 balls are black.</p>
<p>Generalizing this idea, <span class="math inline">\(P(B|B_n) = n/9\)</span>. We can therefore compute the likelihoods for all hypotheses in a vector:</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>likelihoods <span class="op">=</span> np.arange(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">1</span>) <span class="op">/</span> <span class="dv">9</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(likelihoods)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.         0.11111111 0.22222222 0.33333333 0.44444444 0.55555556
 0.66666667 0.77777778 0.88888889 1.        ]</code></pre>
</div>
</div>
<p>Now suppose we want to find the probability of hypothesis <span class="math inline">\(B_5\)</span> after observing one draw <span class="math inline">\(B\)</span>. Let‚Äôs apply Bayes‚Äôs rule:</p>
<p><span class="math display">\[P(B_5 | B) = \frac{P(B|B_5) P(B_5)}{\sum_h{p(B|h) P(h)}}\]</span></p>
<p>Let‚Äôs compute the parts we need to calculate <span class="math inline">\(P(B_5 | B)\)</span>.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>p_B5 <span class="op">=</span> h_priors[<span class="dv">3</span>]</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Likelihood</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>likelihood_B5 <span class="op">=</span> likelihoods[<span class="dv">5</span>]</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Data</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>p_B <span class="op">=</span> <span class="bu">sum</span>(likelihoods<span class="op">*</span>h_priors)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>p_B5_given_B <span class="op">=</span> p_B5 <span class="op">*</span> likelihood_B5 <span class="op">/</span> p_B</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Print out results</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"P(B5) = "</span> <span class="op">+</span> <span class="bu">str</span>(p_B5)) <span class="co"># Prior</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"P(B|B5) = "</span> <span class="op">+</span> <span class="bu">str</span>(likelihood_B5)) <span class="co"># Likelihood</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"P(B) = "</span> <span class="op">+</span> <span class="bu">str</span>(p_B)) <span class="co"># Data</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"P(B5|B) = "</span> <span class="op">+</span> <span class="bu">str</span>(p_B5_given_B)) <span class="co"># Posterior</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>P(B5) = 0.1
P(B|B5) = 0.5555555555555556
P(B) = 0.5
P(B5|B) = 0.11111111111111112</code></pre>
</div>
</div>
<p>Let‚Äôs update the probabilities for all hypotheses in a more compact way.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>posteriors <span class="op">=</span> (likelihoods <span class="op">*</span> h_priors) <span class="op">/</span> <span class="bu">sum</span>(likelihoods <span class="op">*</span> h_priors)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(posteriors)):</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"P(B"</span> <span class="op">+</span> <span class="bu">str</span>(i) <span class="op">+</span> <span class="st">"|B) = "</span> <span class="op">+</span> <span class="bu">str</span>(posteriors[i]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>P(B0|B) = 0.0
P(B1|B) = 0.022222222222222223
P(B2|B) = 0.044444444444444446
P(B3|B) = 0.06666666666666667
P(B4|B) = 0.08888888888888889
P(B5|B) = 0.11111111111111112
P(B6|B) = 0.13333333333333333
P(B7|B) = 0.15555555555555556
P(B8|B) = 0.17777777777777778
P(B9|B) = 0.2</code></pre>
</div>
</div>
<p>As expected, Bayes‚Äôs rule says we should increase the probability we assign to hypotheses with more black balls than red balls. Additionally, let‚Äôs double-check that the posterior probabilities sum to 1 (a requirement for a valid probability distribution).</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="bu">sum</span>(posteriors)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>1.0</code></pre>
</div>
</div>
<p>Finally, let‚Äôs plot the posterior probabilities.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>hypotheses <span class="op">=</span> (<span class="st">'B0'</span>, <span class="st">'B1'</span>, <span class="st">'B2'</span>, <span class="st">'B3'</span>, <span class="st">'B4'</span>,</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>              <span class="st">'B5'</span>, <span class="st">'B6'</span>, <span class="st">'B7'</span>, <span class="st">'B8'</span>, <span class="st">'B9'</span>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>y_pos <span class="op">=</span> np.arange(<span class="bu">len</span>(hypotheses))</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>ax.barh(y_pos, posteriors, align<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>ax.set_yticks(y_pos)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>ax.set_yticklabels(hypotheses)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Probability'</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Hypothesis'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>Text(0, 0.5, 'Hypothesis')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="02-bayes_files/figure-html/cell-12-output-2.png" width="585" height="422"></p>
</div>
</div>
</section>
<section id="normalization" class="level3">
<h3 class="anchored" data-anchor-id="normalization">How to avoid calculating P(d)</h3>
<p>In practice, we generally do not need to calculate the <span class="math inline">\(P(d)\)</span> (the denominator in Bayes‚Äôs rule) explicitly. I‚Äôll give you the general idea why in this section.</p>
<p>First, we create a vector of prior probabilities, which has as many components as there are hypotheses. We‚Äôll just reuse <code>h_priors</code>. Note that the probabilities sum to 1, as they should because it‚Äôs a probability distribution.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>h_priors</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])</code></pre>
</div>
</div>
<p>Next, we create a likelihood array. When we did calculations above, we only had a vector with the likelihoods for a specific observation. However, we would like to have something that encodes the likelihood function for each possible observation given each possible hypothesis, rather than just for a specific observation.</p>
<p>In this example, there are two possible observations: <span class="math inline">\(B\)</span> and <span class="math inline">\(R\)</span>. We can encode the likelihood as an <span class="math inline">\(m \times n\)</span> array where <span class="math inline">\(m\)</span> is the number of hypotheses and <span class="math inline">\(n\)</span> is the number of possible observations. In our case: <span class="math inline">\(10 \times 2\)</span>.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>likelihood_array <span class="op">=</span> np.array((np.arange(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">1</span>) <span class="op">/</span> <span class="dv">9</span>,</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>                             <span class="dv">1</span><span class="op">-</span>(np.arange(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">1</span>) <span class="op">/</span> <span class="dv">9</span>))).T</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(likelihood_array)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[0.         1.        ]
 [0.11111111 0.88888889]
 [0.22222222 0.77777778]
 [0.33333333 0.66666667]
 [0.44444444 0.55555556]
 [0.55555556 0.44444444]
 [0.66666667 0.33333333]
 [0.77777778 0.22222222]
 [0.88888889 0.11111111]
 [1.         0.        ]]</code></pre>
</div>
</div>
<p>Now we multiply the prior and likelihoods together (the numerator of Bayes‚Äôs rule) <em>element-wise</em> (first element gets multiplied with first element, second element by second element, etc.):</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>prior_array <span class="op">=</span> np.array((h_priors, h_priors)).T</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>bayes_numerator <span class="op">=</span> likelihood_array <span class="op">*</span> prior_array</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(bayes_numerator)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[0.         0.1       ]
 [0.01111111 0.08888889]
 [0.02222222 0.07777778]
 [0.03333333 0.06666667]
 [0.04444444 0.05555556]
 [0.05555556 0.04444444]
 [0.06666667 0.03333333]
 [0.07777778 0.02222222]
 [0.08888889 0.01111111]
 [0.1        0.        ]]</code></pre>
</div>
</div>
<p>Finally, we want a distribution for each column, i.e., a distribution over hypotheses given each observation. Therefore, we sum each column and then divide each element by the sum of its column:</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>posteriors <span class="op">=</span> bayes_numerator <span class="op">/</span> np.<span class="bu">sum</span>(bayes_numerator, axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(posteriors)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[0.         0.2       ]
 [0.02222222 0.17777778]
 [0.04444444 0.15555556]
 [0.06666667 0.13333333]
 [0.08888889 0.11111111]
 [0.11111111 0.08888889]
 [0.13333333 0.06666667]
 [0.15555556 0.04444444]
 [0.17777778 0.02222222]
 [0.2        0.        ]]</code></pre>
</div>
</div>
<p>And that gives us the posterior without us having to explicitly calculate the evidence for each observation!</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The general idea is this. Because the denominator of Bayes‚Äôs rule, for a fixed observation, is a constant, you can usually get away with computing <span class="math inline">\(P(d|h) P(h)\)</span> for every possible hypothesis <span class="math inline">\(h\)</span> and then ‚Äúnormalize‚Äù the resulting values so that they sum to 1 (remember that they have to in order for it to be a valid probability distribution).</p>
</div>
</div>
</section>
</section>
<section id="bayes-exercises" class="level2">
<h2 class="anchored" data-anchor-id="bayes-exercises">Exercises üìù</h2>
<section id="taxi-cabs" class="level3">
<h3 class="anchored" data-anchor-id="taxi-cabs">Taxi cabs</h3>
<p>80% of the taxi cabs in Simpletown are green and 20% are yellow. An hit-and-run accident happened at night involving a taxi. A witness claimed that the taxi was yellow. After extensive testing, it is determined that the witness can correctly identify the color of a taxi only 75% of the time under conditions like the ones present during the accident. What is the probability that the taxi was yellow?</p>
</section>
<section id="flipping-coins" class="level3">
<h3 class="anchored" data-anchor-id="flipping-coins">Flipping coins</h3>
<p>You observe a sequence of coin flips and want to determine if the coin is a trick coin (always comes up heads) or a normal coin. Let <span class="math inline">\(P(\text{heads}) = \theta\)</span>. Let <span class="math inline">\(h_1\)</span> be the hypothesis that <span class="math inline">\(\theta = 0.5\)</span> (fair coin). Let <span class="math inline">\(h_2\)</span> be the hypothesis that <span class="math inline">\(\theta = 1\)</span> (trick coin).</p>
<p>For this problem, we will define something called prior odds, which is the ratio of prior probabilities assigned to two hypotheses: <span class="math inline">\(\frac{P(h_1)}{P(h_2)}\)</span>. Because most coins aren‚Äôt trick coins, we assume that <span class="math inline">\(\frac{P(h_1)}{P(h_2)} = 999\)</span>, indicating a very strong (999 to 1) prior probability in favor of fair coins. We can now compute the posterior odds, the ratio of posterior probabilities for the two hypotheses after observing some data <span class="math inline">\(d\)</span>: <span class="math inline">\(\frac{P(h_1|d)}{P(h_2|d)}\)</span>.</p>
<p>Compute the posterior odds after observing the following sequences of coin flips:</p>
<ol type="1">
<li>HHTHT</li>
<li>HHHHH</li>
<li>HHHHHHHHHH</li>
</ol>
</section>
</section>
<section id="solutions" class="level2">
<h2 class="anchored" data-anchor-id="solutions">Solutions</h2>
<section id="taxi-cabs-1" class="level3">
<h3 class="anchored" data-anchor-id="taxi-cabs-1">Taxi cabs</h3>
<p>Let <span class="math inline">\(h_1\)</span> be the hypothesis that the taxi is yellow. Let <span class="math inline">\(h_2\)</span> be the hypothesis that the taxi is green. Let data <span class="math inline">\(d\)</span> be the witness report that the taxi was yellow. Given the problem statement, <span class="math inline">\(P(h_1) = 0.2\)</span> and <span class="math inline">\(P(h_2) = 0.8\)</span>. The witness is only accurate 75% of the time, so <span class="math inline">\(P(d|h1) = 0.75\)</span> (the witness saw a yellow taxi and correctly identified it) and <span class="math inline">\(P(d|h2) = 0.25\)</span> (the witness saw a green taxi but identified it as yellow). Now we apply Bayes‚Äôs rule:</p>
<p><span class="math display">\[\begin{align}
P(h_1|d) &amp;= \frac{P(d|h_1) P(h_1)}{P(d)} \\
&amp;= \frac{P(d|h_1) P(h_1)}{P(d|h_1) P(h_1) + P(d|h_2) P(h_2)} \\
&amp;= \frac{(0.75) (0.2)}{(0.75)(0.2) + (0.25)(0.8)} \approx 0.43
\end{align}\]</span></p>
<p>Because yellow cabs are rare (have low prior probability), it is actually more probable that the cab was green, even though the witness is 75% accurate.</p>
</section>
<section id="flipping-coins-1" class="level3">
<h3 class="anchored" data-anchor-id="flipping-coins-1">Flipping coins</h3>
<section id="hhtht" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="hhtht">HHTHT</h4>
<p><span class="math display">\[
\begin{align}
\frac{P(h_1|d)}{P(h_2|d)} &amp;= \frac{P(d|h_1)}{P(d|h_2)} \frac{P(h_1)}{P(h_2)} \\
&amp;= \frac{(1/2)^5}{0} \times 999 = \inf
\end{align}
\]</span> This sequence isn‚Äôt even possible under <span class="math inline">\(h_2\)</span> so we have infinite evidence in favor of <span class="math inline">\(h_1\)</span>.</p>
</section>
<section id="hhhhh" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="hhhhh">HHHHH</h4>
<p><span class="math display">\[
\begin{align}
\frac{P(h_1|d)}{P(h_2|d)} &amp;= \frac{P(d|h_1)}{P(d|h_2)} \frac{P(h_1)}{P(h_2)} \\
&amp;= \frac{(1/2)^5}{1^5} \times 999 = 31.2
\end{align}
\]</span></p>
<p>This sequence favors <span class="math inline">\(h_1\)</span> by a factor of about 31. Even five heads in a row can‚Äôt overcome our strong prior favoring <span class="math inline">\(h_1\)</span>.</p>
</section>
<section id="hhhhhhhhhh" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="hhhhhhhhhh">HHHHHHHHHH</h4>
<p><span class="math display">\[
\begin{align}
\frac{P(h_1|d)}{P(h_2|d)} &amp;= \frac{P(d|h_1)}{P(d|h_2)} \frac{P(h_1)}{P(h_2)} \\
&amp;= \frac{(1/2)^{10}}{1^{10}} \times 999 = 0.98
\end{align}
\]</span></p>
<p>Now the evidence favors <span class="math inline">\(h_2\)</span> (trick coin) just barely.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>