[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"first version digital book Computational Psychology course ‚Äì cognitive modeling course. includes homework assignments written Python.","code":""},{"path":"index.html","id":"thanks","chapter":"Preface","heading":"Thanks","text":"book borrows inspiration content (especially Chapters 2 7) Fausto Carcassi‚Äôs Introduction Cognitive Modelling R book tremendously grateful sharing book publicly. Hopefully resource equally helpful others.number modeling examples homework assignments adapted code written Danielle Navarro previous iterations Computational Cognitive Science course Andy Perfors. grateful making materials public (well documented).feedback welcome encouraged.","code":""},{"path":"index.html","id":"license","chapter":"Preface","heading":"License","text":"Anyone free reuse repurpose book non-commercial purposes, attribution.","code":""},{"path":"intro.html","id":"intro","chapter":"1 Why computational modeling?","heading":"1 Why computational modeling?","text":"field psychology full theories. book focused specific type theory: type can formalized computational model.advantage computational (mathematical) theory one can‚Äôt expressed computational terms? biggest advantage precision. example, suppose say attention like spotlight üî¶: can attend things currently within light, can control light shining, things outside light outside awareness. kind theory ‚Äì analogy-based one ‚Äì ‚Äôs good start making general qualitative predictions attention works.‚Äôll quickly run problems want make precise quantitative predictions attention. big spotlight? size expand contract? quickly move around? attention completely absent outside spotlight ramp get near edge light? words, wanted build computer model theory, simple analogy doesn‚Äôt cut .Computational models, nothing else, force us explicit assumptions.","code":""},{"path":"intro.html","id":"representations","chapter":"1 Why computational modeling?","heading":"1.1 Representations üî∏","text":"don‚Äôt perceive world truly . give one example, visible spectrum eyes can detect just fraction full electromagnetic spectrum. words, ‚Äôre seeing incomplete picture surrounding world.\nFigure 1.1: visible light spectrum. Source: Philip Ronan/Wikipedia.\nSimilarly, constantly making assumptions things see hear using assumptions fill gaps.heads kind model world around us ‚Äì cognitive scientists call mental representation. representations help us reach rapid conclusions things involving language, causes effects, concepts, mental states, many aspects cognition.key questions cognitive scientists use computational models :mental representations rely ?minds use representations learn get new information?kind information get expectations kind information ‚Äôre getting affect use ?book elaborate, examples, questions.","code":""},{"path":"intro.html","id":"homework-1-build-your-first-computational-model","chapter":"1 Why computational modeling?","heading":"1.2 Homework 1: Build your first computational model üíª","text":"get initial experience computational modeling, ‚Äôll build experiment simple model classical conditioning developed Robert Rescorla Allan Wagner ‚Äì now called Rescorla-Wagner model.homework assignments book done Google Colab. Click button top section view Homework 1.‚Äôre unfamiliar Colab (Jupyter Notebooks), watch brief introduction video.Note: ‚Äôll make copy notebook saved Drive order edit .","code":""},{"path":"bayes.html","id":"bayes","chapter":"2 Bayesian inference","heading":"2 Bayesian inference","text":"chapter reviews basic probability Bayesian inference. might asking : psychology? answer becomes clear recognize ‚Äôre making sense world drawing inferences. see ambiguous image, rabbit duck? someone mumbles something, say ‚Äúhello‚Äù ‚Äúgo hell?‚Äù take pill headache goes away, pill eliminate headache headache go away ?\nFigure 2.1: duck-rabbit illusion.\nexamples, one hypothesis observed. Probability Bayesian inference provide tools optimally determining probable different hypotheses . One claims book people making inferences situations like , inferences often well predicted optimal inferences dictated probability theory.","code":""},{"path":"bayes.html","id":"basic-probability","chapter":"2 Bayesian inference","heading":"2.1 Basic probability üé≤","text":"Conditional probability: \\(P(b|) = \\frac{P(,b)}{P()}\\)Chain rule: \\(P(,b) = P(b|)P()\\)Marginalization: \\(P(d) = \\sum_h P(d,h) = \\sum_h P(d|h) P(h)\\)Bayes‚Äôs rule: \\(P(h|d) = \\frac{P(d|h) P(h)}{P(d)} = \\frac{P(d|h) P(h)}{\\sum_h P(d|h) P(h)}\\). \\(P(d|h)\\) referred likelihood, \\(P(h)\\) prior, \\(P(h|d)\\) posterior.","code":""},{"path":"bayes.html","id":"a-motivating-example-sampling-from-a-bag","chapter":"2 Bayesian inference","heading":"2.2 A motivating example: Sampling from a bag üëù","text":"Suppose bag full black red balls. can‚Äôt see inside bag don‚Äôt know many black red balls inside, know nine total balls bag.want know many black balls red balls . finite number hypotheses: {0 black balls, 1 black ball, 2 black balls, ‚Ä¶, 9 black balls}. Let‚Äôs call hypotheses \\(B_0\\), \\(B_1\\), etc., respectively.don‚Äôt know hypothesis true, might idea hypotheses likely others. Therefore, natural represent uncertainty probability distribution possible unknown states world ‚Äì case, 10 hypotheses. hypothesis gets assigned probability, probabilities sum 1.simplicity, let‚Äôs assume don‚Äôt idea hypotheses likely. words, give every hypothesis probability: 1/10 = 0.1. also called uniform distribution hypotheses. distribution prior.Now suppose put hand bag pull ball random. possible observations : {black, red}, Let‚Äôs call \\(B\\) \\(R\\), respectively. probability observing color depends hypothesis true, .e., many balls color bag. instance, \\(B_0\\) true (0 black balls bag), probability observing red ball 1 (\\(P(R|B_0)=1\\)), probability observing black ball 0 (\\(P(B|B_0)=0\\)). expressions tell us probable observations , given specific hypothesis, likelihoods.","code":""},{"path":"bayes.html","id":"sampling-from-the-generative-model","chapter":"2 Bayesian inference","heading":"2.2.1 Sampling from the generative model","text":"Now distribution hypotheses (prior), \\(P(h)\\), distribution observations given hypothesis (likelihood), \\(P(d|h)\\). two things allow us create generative model, model sampling new data.sample generative model? Note hypothesis \\(h\\) true depend data, data \\(d\\) depends hypothesis true. Therefore, can sample generative model using following two-step process:Sample hypothesis prior.Sample data given hypothesis, using likelihood.Let‚Äôs first create vector probability hypothesis:Now ‚Äôll first step: create vector 10000 hypotheses sampled prior:, number corresponds one hypothesis: 0 corresponds \\(B_0\\), 1 \\(B_1\\), . sample represents one possible way (hypothesis) world . Since prior uniform (hypothesis probability), hypothesis appears equally often. can plot samples verify:Now next step. sample prior_samples, want sample observation. , let‚Äôs pause second think probability pulling black ball given hypothesis \\(B_3\\) true, example. means 3 black balls 6 red balls bag. probability pulling black ball bag random 3/9.Generalizing idea, can get probability pulling black ball bag dividing elements prior_samples 9:Now, complete generative model, just need sample one value element p_black. sample represents draw bag.ball_samples 1 black 0 red. , let‚Äôs plot samples.can think plot representing overall beliefs number red black balls bag, averaged possible hypotheses.surprisingly, got equal numbers red black balls. makes sense: didn‚Äôt prior expectations whether red black balls likely bag.beliefs change pull ball bag? , respond evidence?","code":"import numpy as np\nimport random\n\nrandom.seed(2022) # set random seed to get same results every time\n\nh_priors = np.repeat(0.1,10)\nprint(h_priors)## [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]prior_samples = np.array(random.choices(np.arange(0,10,1),\n                   weights = h_priors, \n                   k = 10000))\n                   \nprint(prior_samples[0:9]) # printing out just a few## [5 4 3 0 7 9 4 6 8]import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nn, bins, patches = ax.hist(prior_samples, bins=10)\nax.set_xlabel('Prior sample')\nax.set_ylabel('Number of samples')p_black = prior_samples / 9\nprint(p_black[0:4]) # print out just a few## [0.55555556 0.44444444 0.33333333 0.        ]ball_samples = np.random.binomial(n = np.repeat(1,len(p_black)),\n                                  p = p_black)\nprint(ball_samples[0:9])## [1 0 0 0 1 1 0 1 1]fig, ax = plt.subplots()\nn, bins, patches = ax.hist(ball_samples, bins=2)\nax.set_xticks([0,1])\nax.set_ylabel('Number of samples')"},{"path":"bayes.html","id":"bayesian-updating-learning-from-evidence","chapter":"2 Bayesian inference","heading":"2.3 Bayesian updating: Learning from evidence ü§î","text":"Let‚Äôs apply Bayes‚Äôs rule see optimally incorporate new data beliefs.","code":""},{"path":"bayes.html","id":"applying-bayess-rule-to-the-bag-case","chapter":"2 Bayesian inference","heading":"2.3.1 Applying Bayes‚Äôs rule to the bag case","text":"Suppose uniform prior distribution 10 hypotheses balls bag. Now pick ball ‚Äôs black. Given observation \\(B\\), change probabilities give hypothesis?Intuitively, now give little bit probability hypotheses black balls red balls, hypotheses make observations likely. Moreover, can safely exclude hypothesis \\(B_0\\), observation impossible \\(B_0\\) true. Let‚Äôs calculate Bayes‚Äôs rule.prior vector h_priors defined . Given observed \\(B\\), likelihood tell us, hypothesis, probability \\(B\\) given hypothesis. example, \\(B_9\\), likelihood \\(P(B|B_9) = 1\\). \\(B_8\\), \\(P(B|B_8) = 8/9\\), 8 9 balls black.Generalizing idea, \\(P(B|B_n) = n/9\\). can therefore compute likelihoods hypotheses vector:Now suppose want find probability hypothesis \\(B_5\\) observing one draw \\(B\\). Let‚Äôs apply Bayes‚Äôs rule:\\[P(B_5 | B) = \\frac{P(B|B_5) P(B_5)}{\\sum_h{p(B|h) P(h)}}\\]Let‚Äôs compute parts need calculate \\(P(B_5 | B)\\).Let‚Äôs update probabilities hypotheses compact way.expected, Bayes‚Äôs rule says increase probability assign hypotheses black balls red balls. Additionally, let‚Äôs double-check posterior probabilities sum 1 (requirement valid probability distribution).Finally, let‚Äôs plot posterior probabilities.","code":"likelihoods = np.arange(0,10,1) / 9\n\nprint(likelihoods)## [0.         0.11111111 0.22222222 0.33333333 0.44444444 0.55555556\n##  0.66666667 0.77777778 0.88888889 1.        ]# Prior\np_B5 = h_priors[3]\n\n# Likelihood\nlikelihood_B5 = likelihoods[5]\n\n# Data\np_B = sum(likelihoods*h_priors)\n\n# Posterior\np_B5_given_B = p_B5 * likelihood_B5 / p_B\n\n# Print out results\nprint(\"P(B5) = \" + str(p_B5)) # Prior## P(B5) = 0.1print(\"P(B|B5) = \" + str(likelihood_B5)) # Likelihood## P(B|B5) = 0.5555555555555556print(\"P(B) = \" + str(p_B)) # Data## P(B) = 0.5print(\"P(B5|B) = \" + str(p_B5_given_B)) # Posterior## P(B5|B) = 0.11111111111111112posteriors = (likelihoods * h_priors) / sum(likelihoods * h_priors)\n\nfor i in range(len(posteriors)):\n  print(\"P(B\" + str(i) + \"|B) = \" + str(posteriors[i]))## P(B0|B) = 0.0\n## P(B1|B) = 0.022222222222222223\n## P(B2|B) = 0.044444444444444446\n## P(B3|B) = 0.06666666666666667\n## P(B4|B) = 0.08888888888888889\n## P(B5|B) = 0.11111111111111112\n## P(B6|B) = 0.13333333333333333\n## P(B7|B) = 0.15555555555555556\n## P(B8|B) = 0.17777777777777778\n## P(B9|B) = 0.2sum(posteriors)## 1.0fig, ax = plt.subplots()\nhypotheses = ('B0', 'B1', 'B2', 'B3', 'B4',\n              'B5', 'B6', 'B7', 'B8', 'B9')\ny_pos = np.arange(len(hypotheses))\n\nax.barh(y_pos, posteriors, align='center')## <BarContainer object of 10 artists>ax.set_yticks(y_pos)\nax.set_yticklabels(hypotheses)\nax.set_xlabel('Probability')\nax.set_ylabel('Hypothesis')"},{"path":"bayes.html","id":"how-to-avoid-calculating-pd","chapter":"2 Bayesian inference","heading":"2.3.2 How to avoid calculating P(d)","text":"practice, generally need calculate \\(P(d)\\) (denominator Bayes‚Äôs rule) explicitly. ‚Äôll give general idea section.First, create vector prior probabilities, many components hypotheses. ‚Äôll just reuse h_priors. Note probabilities sum 1, ‚Äôs probability distribution.Next, create likelihood array. calculations , vector likelihoods specific observation. However, like something encodes likelihood function possible observation given possible hypothesis, rather just specific observation.example, two possible observations: \\(B\\) \\(R\\). can encode likelihood \\(m \\times n\\) array \\(m\\) number hypotheses \\(n\\) number possible observations. case: \\(10 \\times 2\\).Now multiply prior likelihoods together (numerator Bayes‚Äôs rule) element-wise (first element gets multiplied first element, second element second element, etc.):Finally, want distribution column, .e., distribution hypotheses given observation. Therefore, sum column divide element sum column:gives us posterior without us explicitly calculate evidence observation!general idea . denominator Bayes‚Äôs rule, fixed observation, constant, can usually get away computing \\(P(d|h) P(h)\\) every possible hypothesis \\(h\\) ‚Äúnormalize‚Äù resulting values sum 1 (remember order valid probability distribution).","code":"h_priors## array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])likelihood_array = np.array((np.arange(0,10,1) / 9,\n                             1-(np.arange(0,10,1) / 9))).T\nprint(likelihood_array)## [[0.         1.        ]\n##  [0.11111111 0.88888889]\n##  [0.22222222 0.77777778]\n##  [0.33333333 0.66666667]\n##  [0.44444444 0.55555556]\n##  [0.55555556 0.44444444]\n##  [0.66666667 0.33333333]\n##  [0.77777778 0.22222222]\n##  [0.88888889 0.11111111]\n##  [1.         0.        ]]prior_array = np.array((h_priors, h_priors)).T\nbayes_numerator = likelihood_array * prior_array\n\nprint(bayes_numerator)## [[0.         0.1       ]\n##  [0.01111111 0.08888889]\n##  [0.02222222 0.07777778]\n##  [0.03333333 0.06666667]\n##  [0.04444444 0.05555556]\n##  [0.05555556 0.04444444]\n##  [0.06666667 0.03333333]\n##  [0.07777778 0.02222222]\n##  [0.08888889 0.01111111]\n##  [0.1        0.        ]]posteriors = bayes_numerator / np.sum(bayes_numerator, axis = 0)\nprint(posteriors)## [[0.         0.2       ]\n##  [0.02222222 0.17777778]\n##  [0.04444444 0.15555556]\n##  [0.06666667 0.13333333]\n##  [0.08888889 0.11111111]\n##  [0.11111111 0.08888889]\n##  [0.13333333 0.06666667]\n##  [0.15555556 0.04444444]\n##  [0.17777778 0.02222222]\n##  [0.2        0.        ]]"},{"path":"bayes.html","id":"exercises","chapter":"2 Bayesian inference","heading":"2.4 Exercises üìù","text":"","code":""},{"path":"bayes.html","id":"taxi-cabs","chapter":"2 Bayesian inference","heading":"2.4.1 Taxi cabs","text":"80% taxi cabs Simpletown green 20% yellow. hit--run accident happened night involving taxi. witness claimed taxi yellow. extensive testing, determined witness can correctly identify color taxi 75% time conditions like ones present accident. probability taxi yellow?","code":""},{"path":"bayes.html","id":"flipping-coins","chapter":"2 Bayesian inference","heading":"2.4.2 Flipping coins","text":"observe sequence coin flips want determine coin trick coin (always comes heads) normal coin. Let \\(P(\\text{heads}) = \\theta\\). Let \\(h_1\\) hypothesis \\(\\theta = 0.5\\) (fair coin). Let \\(h_2\\) hypothesis \\(\\theta = 1\\) (trick coin).problem, define something called prior odds, ratio prior probabilities assigned two hypotheses: \\(\\frac{P(h_1)}{P(h_2)}\\). coins aren‚Äôt trick coins, assume \\(\\frac{P(h_1)}{P(h_2)} = 999\\), indicating strong (999 1) prior probability favor fair coins. can now compute posterior odds, ratio posterior probabilities two hypotheses observing data \\(d\\): \\(\\frac{P(h_1|d)}{P(h_2|d)}\\).Compute posterior odds observing following sequences coin flips:HHTHTHHHHHHHHHHHHHHH","code":""},{"path":"bayes.html","id":"solutions","chapter":"2 Bayesian inference","heading":"2.5 Solutions","text":"","code":""},{"path":"bayes.html","id":"taxi-cabs-1","chapter":"2 Bayesian inference","heading":"2.5.1 Taxi cabs","text":"Let \\(h_1\\) hypothesis taxi yellow. Let \\(h_2\\) hypothesis taxi green. Let data \\(d\\) witness report taxi yellow. Given problem statement, \\(P(h_1) = 0.2\\) \\(P(h_2) = 0.8\\). witness accurate 75% time, \\(P(d|h1) = 0.75\\) (witness saw yellow taxi correctly identified ) \\(P(d|h2) = 0.25\\) (witness saw green taxi identified yellow). Now apply Bayes‚Äôs rule:\\[\\begin{align}\nP(h_1|d) &= \\frac{P(d|h_1) P(h_1)}{P(d)} \\\\\n&= \\frac{P(d|h_1) P(h_1)}{P(d|h_1) P(h_1) + P(d|h_2) P(h_2)} \\\\\n&= \\frac{(0.75) (0.2)}{(0.75)(0.2) + (0.25)(0.8)} \\approx 0.43\n\\end{align}\\]yellow cabs rare (low prior probability), actually probable cab green, even though witness 75% accurate.","code":""},{"path":"bayes.html","id":"flipping-coins-1","chapter":"2 Bayesian inference","heading":"2.5.2 Flipping coins","text":"","code":""},{"path":"bayes.html","id":"hhtht","chapter":"2 Bayesian inference","heading":"HHTHT","text":"\\[\n\\begin{align}\n\\frac{P(h_1|d)}{P(h_2|d)} &= \\frac{P(d|h_1)}{P(d|h_2)} \\frac{P(h_1)}{P(h_2)} \\\\\n&= \\frac{(1/2)^5}{0} \\times 999 = \\inf\n\\end{align}\n\\]\nsequence isn‚Äôt even possible \\(h_2\\) infinite evidence favor \\(h_1\\).","code":""},{"path":"bayes.html","id":"hhhhh","chapter":"2 Bayesian inference","heading":"HHHHH","text":"\\[\n\\begin{align}\n\\frac{P(h_1|d)}{P(h_2|d)} &= \\frac{P(d|h_1)}{P(d|h_2)} \\frac{P(h_1)}{P(h_2)} \\\\\n&= \\frac{(1/2)^5}{1^5} \\times 999 = 31.2\n\\end{align}\n\\]sequence favors \\(h_1\\) factor 31. Even five heads row can‚Äôt overcome strong prior favoring \\(h_1\\).","code":""},{"path":"bayes.html","id":"hhhhhhhhhh","chapter":"2 Bayesian inference","heading":"HHHHHHHHHH","text":"\\[\n\\begin{align}\n\\frac{P(h_1|d)}{P(h_2|d)} &= \\frac{P(d|h_1)}{P(d|h_2)} \\frac{P(h_1)}{P(h_2)} \\\\\n&= \\frac{(1/2)^{10}}{1^{10}} \\times 999 = 0.98\n\\end{align}\n\\]Now evidence favors \\(h_2\\) (trick coin) just barely.","code":""},{"path":"generalization.html","id":"generalization","chapter":"3 Generalization","heading":"3 Generalization","text":"last chapter, learned much cognition making inferences. common inference ‚Äôre faced involves generalizing examples things new cases.child hears brand new word figure objects apply word .eat one candy assorted box try guess others might taste like.remember friend liked crosswords weekend trip cabin one time, guess might like book puzzles gift (generalizing interests).can apply Bayesian inference kinds problems?","code":""},{"path":"generalization.html","id":"hormones","chapter":"3 Generalization","heading":"3.1 Healthy hormone levels üíâ","text":"example comes 2001 paper Josh Tenenbaum Thomas Griffiths1The basic problem: learn value healthy hormone level (say, 60) varies scale 1 100 (integers ). probability another value (say, 70) also healthy?","code":""},{"path":"generalization.html","id":"setting-up-a-model","chapter":"3 Generalization","heading":"3.1.1 Setting up a model","text":"","code":""},{"path":"generalization.html","id":"the-hypothesis-space","chapter":"3 Generalization","heading":"3.1.1.1 The hypothesis space","text":"start , ‚Äôll assume healthy values lie contiguous interval. Using term paper, interval consequential region \\(C\\).hypothesis space consists possible consequential regions. example, [0,100], [10,19], [44,45], valid hypotheses. full hypothesis space every valid interval 0 100.","code":""},{"path":"generalization.html","id":"prior","chapter":"3 Generalization","heading":"3.1.1.2 Prior","text":"much weight assign hypothesis? might reason favor shorter intervals longer ones, example. paper, use Erlang prior. Alternatively, simplicity calculation, assume uniform prior distribution, placing equal weight hypotheses, like previous chapter. tantamount making prior assumptions intervals probable.","code":""},{"path":"generalization.html","id":"likelihood","chapter":"3 Generalization","heading":"3.1.1.3 Likelihood","text":"Suppose learn healthy patient hormone level 60. likelihood observing value, assuming know hypothesis correct? , \\(P(x = 60 | h)\\). depends assume patient chosen.","code":""},{"path":"generalization.html","id":"weak-strong-sampling","chapter":"3 Generalization","heading":"3.1.1.3.1 Weak vs.¬†strong sampling","text":"weak sampling, assume observation sampled full range possibilities, just coincidence happened get one consequential region (healthy patient). ‚Äôs true, probability getting particular value doesn‚Äôt depend hypothesis true:\\[\nP(x|h) = \\frac{1}{L}\n\\]\\(L\\) length range possible values (100 case).strong sampling, assume observation specifically chosen example consequential region \\(C\\). words, someone chose healthy person tested hormone levels example . case, probability seeing particular value depends size region:\\[\nP(x|h) = \\begin{cases} \n  \\frac{1}{|h|} & \\text{} x \\h \\\\\n  0 & \\text{otherwise}\n  \\end{cases}\n\\]\n\\(|h|\\) size \\(h\\), .e., number values contained \\(h\\). multiple observations \\(X = \\{x_1, x_2, \\ldots, x_n \\}\\), \\(P(X|h) = (1/|h|)^n\\). assume sample independent like coin flip.result strong sampling assumption size principle: among hypotheses include observed examples, smallest receive higher posterior probability higher likelihoods.\nFigure 3.1: Sample hypotheses. thickness lines indicates likelihood, depicting size principle. Image Griffiths & Tenenbaum (2001).\n","code":""},{"path":"generalization.html","id":"posterior","chapter":"3 Generalization","heading":"3.1.1.4 Posterior","text":"can now simply apply Bayes‚Äôs rule compute probability hypothesis, given observation (set observations).\\[\nP(h|X) = \\frac{P(X|h) P(h)}{\\sum_{h_i} P(X|h_i) P(h_i)}\n\\]","code":""},{"path":"generalization.html","id":"generalizing","chapter":"3 Generalization","heading":"3.1.2 Generalizing","text":"aren‚Äôt quite finished. Remember really want know probability new value \\(y\\) also healthy hormone level. point done assigned probability interval consequential region.want essentially two-step process:hypothesis \\(h\\), check see \\(y\\) ., check probable \\(h\\) consequential region \\(C\\), given observations \\(X\\).basic idea sometimes known hypothesis averaging don‚Äôt actually care hypothesis right one, ‚Äôll just average hypotheses, weighted probable . Specifically, ‚Äôll compute:\\[\nP(y \\C|X) = \\sum_h P(y \\C | h) P(h | X)\n\\]\nsecond term right computed earlier using Bayes‚Äôs rule.first term? time, ‚Äôll assume weak sampling ‚Äôs reason assume new value \\(y\\) chosen healthy value .","code":""},{"path":"generalization.html","id":"homework-2-finish-the-details","chapter":"3 Generalization","heading":"3.1.3 Homework 2: Finish the details","text":"haven‚Äôt provided details model assignment finish implementation , run simulations, collect small amount real data compare model .","code":""},{"path":"generalization.html","id":"the-number-game","chapter":"3 Generalization","heading":"3.2 The number game üî¢","text":"domains, requiring concepts restricted contiguous intervals realistic. Numbers one example. Consider space possible number concepts make integers 1 100. addition concepts like ‚Äúnumbers 20 50,‚Äù many plausible concepts like ‚Äúmultiples 10,‚Äù ‚Äúeven numbers,‚Äù ‚Äúpowers 3.‚ÄùConsider following problem: given one examples \\(X\\) numbers fit rule want know probable new number \\(y\\) also fits rule.model discussed can naturally extended problem. likelihood, can make strong sampling assumption .prior things get little trickier. Intuitively concepts like ‚Äúeven numbers‚Äù seem probable even seeing examples concepts like ‚Äúmultiples 7.‚Äù now becomes psychological question: rules people find intuitively plausible? single way decide , run survey find : Give people long list rules ask judge intuitively natural seem. construct prior probability distribution using data.Alternatively, come definition ‚Äúcomplexity‚Äù hypotheses assume less complex hypotheses receive higher prior probability.chosen prior probability distribution \\(P(h)\\), can now proceed just .","code":""},{"path":"generalization.html","id":"inductive-generalizations-about-animal-properties","chapter":"3 Generalization","heading":"3.3 Inductive generalizations about animal properties üê¥","text":"Now let‚Äôs consider even complex generalization problem, based 2002 paper Neville Sanjana Josh Tenenbaum2. problem generalizing properties set example animals animals. paper uses following example:way read follows: premises state chimps squirrels blicketitis. conclusion horses blicketitis. inductive generalization question probable conclusion given premises? Intuitively, conclusion example seems plausible conclusion following example:interesting psychological question generalizations seem intuitively plausible others?","code":"Chimps have blicketitis\nSquirrels have blicketitis\n--------------------------\nHorses have blicketitisChimps have blicketitis\nGorillas have blicketitis\n--------------------------\nHorses have blicketitis"},{"path":"generalization.html","id":"hypothesis-space","chapter":"3 Generalization","heading":"3.3.1 Hypothesis space","text":"problem conceptually similar ones ‚Äôve already discussing. want infer animals blicketitis seeing examples animals blicketitis. first question answer analogue consequential region problem. Animals don‚Äôt naturally fall one-dimensional interval ‚Äôll need define different hypothesis space. One possibility hierarchy, naturally captures knowledge people animals.","code":""},{"path":"generalization.html","id":"clustering","chapter":"3 Generalization","heading":"3.3.1.1 Clustering","text":"paper first creates hierarchy eight animals using similarity data collected people. Specifically, asked people judge similar pairs eight animals calculated average similarity judgment animal.similarity judgments can used construct tree using simple clustering algorithm. algorithm works follows:Put animals cluster.one cluster hasn‚Äôt placed group, following:\nIdentify pair clusters greatest similarity .\nGroup clusters new cluster.\nIdentify pair clusters greatest similarity .Group clusters new cluster.several approaches computing similarity two clusters contain multiple animals. example, might use maximum similarity pair individual animals two clusters.results algorithm can represented tree, shown . node tree represents cluster. hypotheses consider combination 1, 2, 3 clusters determined using clustering algorithm.\nFigure 3.2: tree animal species. Image Sanjana & Tenenbaum (2002).\n","code":""},{"path":"generalization.html","id":"the-model","chapter":"3 Generalization","heading":"3.3.2 The model","text":"can now define model. First, let‚Äôs define \\(P(h)\\) \\(h\\) set clusters. authors make assumption analogous following:\\[\nP(h) \\propto \\frac{1}{\\phi^k}\n\\]\n\\(k\\) number clusters \\(h\\). \\(\\phi\\) parameter can choose. long \\(\\phi > 1\\), \\(P(h)\\) smaller hypotheses consisting clusters. effect assigning higher weight ‚Äúsimpler‚Äù hypotheses., make strong sampling assumption likelihood. time, \\(|h|\\) number animal species \\(h\\) \\(n\\) number examples premises.consequence assumptions something like Occam‚Äôs razor says simplest explanation preferred. model assign 0 likelihood hypotheses don‚Äôt include \\(X\\), thus narrowing hypothesis space just hypotheses possible. , favor hypotheses fewer clusters fewer animals. words, favor simplest hypotheses consistent examples ‚Äôve seen.","code":""},{"path":"generalization.html","id":"try-out-the-model-yourself","chapter":"3 Generalization","heading":"3.3.3 Try out the model yourself","text":"can try running version model making copy code . , let‚Äôs look model handles specific cases.Let‚Äôs start single example: horses can get blicketitis., see standard generalization curve. Now, let‚Äôs add animals.Now model increased probability animals. makes sense reason think blicketitis might affect lots different animals.Adding squirrel supported idea. add additional examples animals ‚Äôve already seen?Now probabilities animals drop, ‚Äôs starting look like maybe disease affects four animals ‚Äôve seen far.sum , ‚Äôve seen small number examples, like single horse, model generally prefer simpler hypotheses. ‚Äôve seen data, favor complex hypotheses (like group animals two separate evolutionary clusters) data support .one final example, let‚Äôs look specific impact multiple examples single animal., see model becomes increasingly confident property unique gorillas. makes intuitive sense ‚Äôs something people seem exhibit judgments. , point paper, ‚Äôs something non-probabilistic models can easily explain.","code":"animalGeneralization([\"horse\"])## array([1.        , 0.52984834, 0.30628906, 0.30628906, 0.19579428,\n##        0.19579428, 0.12893614, 0.12893614, 0.0842413 , 0.0842413 ])animalGeneralization([\"horse\", \"cow\", \"mouse\"])## array([1.        , 1.        , 0.57379051, 0.57379051, 0.47852518,\n##        0.47852518, 1.        , 0.60980466, 0.1707609 , 0.1707609 ])animalGeneralization([\"horse\", \"cow\", \"mouse\", \"squirrel\"])## array([1.        , 1.        , 0.64983602, 0.64983602, 0.58531333,\n##        0.58531333, 1.        , 1.        , 0.18405475, 0.18405475])animalGeneralization([\"horse\", \"cow\", \"mouse\", \"squirrel\", \"horse\", \"squirrel\"])## array([1.        , 1.        , 0.32551484, 0.32551484, 0.26938358,\n##        0.26938358, 1.        , 1.        , 0.06883892, 0.06883892])animalGeneralization([\"gorilla\"])## array([0.22011594, 0.22011594, 0.22011594, 0.22011594, 0.46663258,\n##        1.        , 0.13668495, 0.13668495, 0.08643809, 0.08643809])animalGeneralization([\"gorilla\", \"gorilla\"])## array([0.06236149, 0.06236149, 0.06236149, 0.06236149, 0.2473422 ,\n##        1.        , 0.03994508, 0.03994508, 0.02971748, 0.02971748])animalGeneralization([\"gorilla\", \"gorilla\", \"gorilla\"])## array([0.01730482, 0.01730482, 0.01730482, 0.01730482, 0.1259028 ,\n##        1.        , 0.01270155, 0.01270155, 0.01111944, 0.01111944])"},{"path":"generalization.html","id":"results","chapter":"3 Generalization","heading":"3.3.4 Results","text":"results paper show model predicts people‚Äôs judgments quite well, better alternative models rely Bayesian inference. results suggest assumptions model similar assumptions people make making inductive generalizations.\nFigure 3.3: Comparison model results human judgments. Image Sanjana & Tenenbaum (2002).\n","code":""},{"path":"categorization.html","id":"categorization","chapter":"4 Categorization","heading":"4 Categorization","text":"last chapter introduced problem inductive generalization. chapter focus specific case generalization particular interest psychologists, cognitive scientists, people use AI machine learning: classification categorization. Basically, assigning labels things.People constantly classify things world categories (chairs, cats, friends, enemies, edible things, ). helps us communicate function novel situations.machine learning, ‚Äôs often useful classify inputs different categories like whether social media post violates community standards , whether image contains human face , whether MRI contains tumor.Psychologists study people form categories provides window organize knowledge. basic problem one studied machine learning.‚Äôm going intrdouce fairly simple psychological model categorization introduced Robert Nosofsky called Generalized Context Model. assumes people make classification judgments using following general algorithm:Remember examples categories ‚Äôve seen .want classify new instance, compare previous examples different categories rate similar one.Assign category highest average similarity.model makes outlandish assumptions (like idea people remember every example seen ). , first approximation, decent job predicting people‚Äôs classification judgments lot situations. learning purposes, advantage pretty easy understand implement.","code":""},{"path":"categorization.html","id":"a-typical-category-learning-experiment","chapter":"4 Categorization","heading":"4.1 A typical category learning experiment üü¢üü®","text":"psychology, category learning experiments pretty similar structure. People see unfamiliar stimuli differ several dimensions can sorted different categories. task learn distinguishes one category another. experiments usually consist two phases: training phase, testing phase.Training phase: People see many examples category classify , often simply guessing first. get feedback gradually learn tell different categories apart.Testing phase: People see examples, usually brand new ones, classify . time don‚Äôt get feedback. point phase test people actually learned meaning categories.stimuli abstract shapes, cartoon insects, race cars different features, anything else. matters clearly distinguishable features.","code":""},{"path":"categorization.html","id":"representing-stimuli-in-a-model","chapter":"4 Categorization","heading":"4.2 Representing stimuli in a model üß©","text":"modeling purposes, types stimuli (discrete-valued features) can represented matrix, following properties:dimension matrix represents different feature stimuli.length dimension represents many different values feature can .single item can represented binary matrix 1s cells indicating feature values 0s everywhere else.example, Robert Nosofsky‚Äôs (1986) test GCM3, stimuli semicircles lines . two feature dimensions (1) circle size (2) line orientation. feature four possible values.Using binary matrix representation, one possible stimulus Nosofsky experiment:Nosofksy stimuli pretty simple: can exactly one value dimension, representation always single 1 matrix.wanted know many stimulus present model, pretty cumbersome maintain long list arrays like one. instead maintain single matrix stores counts stimuli. , values element represent number stimuli observed feature values.example, let‚Äôs consider ‚Äúdimensional‚Äù condition Nosofsky‚Äôs experiment, subjects saw stimuli ‚Äúdiagonals‚Äù matrix. Let‚Äôs assume saw one 100 times (actually, mind-numbing 1200 trials 2600 practice trials ü•¥). represent like :still isn‚Äôt ideal though. doesn‚Äôt represent category labels subjects got training. don‚Äôt know examples belonged categories.Thinking ahead GCM works, let‚Äôs instead represent training examples like list, can iterate :format, row matrix array three elements. first element category label: 1 2. second two elements indices stimulus matrix. Note: ‚Äôve deviated Python norms consistent numbering table original Nosofsky paper, shown . working model, remember indexing Python starts 0.\nFigure 4.1: Training examples dimensional condition. Image Nosofsky (1986).\n","code":"import numpy as np\n\nstimulus = np.array([[0, 0, 0, 0],\n                     [0, 0, 0, 0],\n                     [0, 0, 1, 0],\n                     [0, 0, 0, 0]], dtype=int)\nprint(stimulus)## [[0 0 0 0]\n##  [0 0 0 0]\n##  [0 0 1 0]\n##  [0 0 0 0]]training_set = np.array([[100, 0, 0, 100],\n                         [0, 100, 100, 0],\n                         [0, 100, 100, 0],\n                         [100, 0, 0, 100]], dtype=int)\nprint(training_set)## [[100   0   0 100]\n##  [  0 100 100   0]\n##  [  0 100 100   0]\n##  [100   0   0 100]]tDimensional = np.zeros((8,3), dtype=int)\ntDimensional[0] = [1,1,1]\ntDimensional[1] = [1,2,2]\ntDimensional[2] = [1,3,2]\ntDimensional[3] = [1,4,1]\ntDimensional[4] = [2,1,4]\ntDimensional[5] = [2,2,3]\ntDimensional[6] = [2,3,3]\ntDimensional[7] = [2,4,4]\n\nprint(tDimensional)## [[1 1 1]\n##  [1 2 2]\n##  [1 3 2]\n##  [1 4 1]\n##  [2 1 4]\n##  [2 2 3]\n##  [2 3 3]\n##  [2 4 4]]"},{"path":"categorization.html","id":"the-generalized-context-model-gcm","chapter":"4 Categorization","heading":"4.3 The Generalized Context Model (GCM)","text":", describe special case model applies tasks (like one paper) two categories. However, model can easily extended number categories. model defined two key equations.first equation defines probability classifying Stimulus \\(S_i\\) Category \\(C_1\\) (.e., Category 1):\\[\n\\begin{equation}\nP(C_1|S_i) = \\frac{b_1 \\sum_{j \\C_1} \\eta_{ij}}{b_1 \\sum_{j \\C_1} \\eta_{ij} + (1-b_1) \\sum_{k \\C_2} \\eta_{ik}}\n\\tag{4.1}\n\\end{equation}\n\\]sum \\(\\sum_{j \\C_1}\\) sum stimuli \\(S_j\\) belong Category 1. sum \\(\\sum_{j \\C_1}\\) sum stimuli \\(S_k\\) belong Category 2.Equation (4.1) parameter \\(b_1\\) defined response bias Category 1. \\(b_1\\) can range 0 1 captures possibility subject biased respond one category another. similar identical prior distribution. \\(b_1\\) small, model biased toward Category 2; \\(b_1\\) large, model biased toward Category 1. Note \\(b = 0.5\\), cancels equation.also include response bias Category 2, model assumes \\(\\sum_i b_i = 1\\). Therefore, two categories, \\(b_2 = 1 - b_1\\) see second term denominator Equation (4.1).\\(\\eta_{ij}\\) function defines similar \\(S_i\\) \\(S_j\\) . GCM assumes stimuli can represented points multi-dimensional space (case, two-dimensional space) similarity defined function distance two points:\\[\n\\eta_{ij} = e^{-c^2 \\left[w_1 (f_{i1} - f_{j1})^2 + (1-w_1)(f_{i2} - f_{j2})^2 \\right]}\n\\]\\(f_{i1}\\) \\(f_{i2}\\) refer feature values dimensions 1 2 \\(S_i\\). equation two parameters: \\(c\\) \\(w_1\\). \\(c\\) scaling parameter affects steeply exponential curve . allow us account different people might different ideas close two stimuli must called similar. \\(w_1\\) called attentional weight dimension 1. parameter captures much weight placed dimension 1 dimension 2. Just like \\(b_1\\), \\(w_1\\) can range 0 1, larger , weight placed dimension 1. Similarly, add \\(w_2\\), attention weights constrained sum 1.","code":""},{"path":"categorization.html","id":"generating-model-predictions","chapter":"4 Categorization","heading":"4.3.1 Generating model predictions","text":"order generate predictions model, need two things. First, need define training stimuli , feature values . allow us compute sums Equation (4.1).Next, need specify values 3 parameters \\(b_1\\), \\(c\\), \\(w_1\\). Nosofsky first collecting data people experiment. , every stimulus \\(S_i\\) can record proportion times people classified Category 1. words, can collect empirical estimates \\(P(C_1|S_i)\\) values \\(S_i\\).Nosofsky uses maximum likelihood procedure fitting model data. Conceptually, idea find set values three parameters produce model predictions close possible empirical data. (exactly linear regression works, data \\((\\bar{x},\\bar{y})\\) want find best-fitting values \\(\\) \\(b\\) function \\(y = ax + b\\) describes relationship \\(x\\) \\(y\\).)can define model fit mean squared error (MSE). MSE defined \\[\n\\frac{1}{n} \\sum_i (y_i - x_i)^2\n\\]\\(y\\) data \\(x\\) model predictions. lower MSE means better model fit. Finding best values parameters can achieved exhaustive search values. example, might consider possible values parameter increments 0.1. write program performs following basic algorithm:Set minimum MSE \\(\\inf\\).possible values \\(b_1\\), \\(c\\), \\(w_1\\):\nGenerate model predictions \\(P(R_1|S_i)\\) \\(S_i\\).\nCompute MSE empirical data model predictions generated previous step.\nMSE smaller minimum MSE, set minimum MSE current value store current values \\(b_1\\), \\(c\\), \\(w_1\\).\nGenerate model predictions \\(P(R_1|S_i)\\) \\(S_i\\).Compute MSE empirical data model predictions generated previous step.MSE smaller minimum MSE, set minimum MSE current value store current values \\(b_1\\), \\(c\\), \\(w_1\\).Return stored values \\(b_1\\), \\(c\\), \\(w_1\\).","code":""},{"path":"categorization.html","id":"homework-3","chapter":"4 Categorization","heading":"4.3.2 Homework 3","text":"next homework implement GCM described . homework, encode training test stimuli implement equations model-fitting algorithm . Additionally, get experience behavioral side cognitive science, create working version category learning experiment order collect data can use compare GCM‚Äôs predictions.","code":""},{"path":"hierarchical-generalization.html","id":"hierarchical-generalization","chapter":"5 Hierarchical generalization","heading":"5 Hierarchical generalization","text":"previous examples, always finite number hypotheses making inferences (number black balls, fair trick coin, yellow green taxi). Sometimes, want consider infinite set hypotheses. example, flipping coin, probability coin coming heads? answer question number interval [0,1].","code":""},{"path":"hierarchical-generalization.html","id":"the-beta-binomial-model","chapter":"5 Hierarchical generalization","heading":"5.1 The Beta-Binomial model ü™ô","text":"\nFigure 5.1: Photo ZSun Fu Unsplash.\ncan answer question model called Beta-Binomial model, named probability distributions uses. First, let‚Äôs set basic assumptions model.Let \\(P(\\text{heads}) = \\theta\\). don‚Äôt know \\(\\theta\\) . observing sequence coin flips \\(D\\), want estimate \\(\\theta\\). can accomplished directly applying Bayes‚Äôs rule:\\[\nP(\\theta|D) = \\frac{P(D|\\theta) P(\\theta)}{P(D)}\n\\]data \\(D\\) case corresponds number \\(k\\) heads \\(n\\) total flips. follows Binomial distribution, describes probability getting \\(k\\) successes \\(n\\) trials, probability success trial \\(\\theta\\). define heads ‚Äúsuccess.‚Äù\\[\n\\begin{align}\nP(D|\\theta) = P(k|\\theta,n) &= \\text{Bin}(k; n, \\theta) \\\\\n&= \\binom{n}{k} \\theta^{k} (1-\\theta)^{n-k}\n\\end{align}\n\\]notation \\(\\text{Bin}(\\cdot)\\) function indicates distribution \\(k\\) (number successes) distribution parameters \\(n\\) (total number trials) \\(\\theta\\) (probability success trial).can define prior, \\(P(\\theta)\\), however like. \\(\\theta\\) random variable can take value 0 1, just say \\(P(\\theta) = 0.5\\) like earlier examples. Instead, \\(P(\\theta)\\) must probability distribution assigns probabilities value 0 1. know nothing \\(\\theta\\), use Uniform(\\([0,1]\\)) non-informative prior assigns equal probability values \\(\\theta\\).Alternatively, convenient choice (reasons explained ) \\(P(\\theta)\\) Beta distribution:\\[\nP(\\theta) = \\text{Beta}(\\theta;\\alpha,\\beta)\n\\]Beta distribution two parameters: \\(\\alpha > 0\\) \\(\\beta > 0\\). Let‚Äôs create function allow us visualize Beta distribution.plot_beta takes two arguments: (\\(\\alpha\\)), b(\\(\\beta\\)) plots Beta distribution parameter values.Let‚Äôs see looks like different values.\\(\\alpha = \\beta = 1\\), Beta distribution identical Uniform(\\([0,1]\\)) distribution.\\(\\alpha\\) \\(\\beta\\) greater 1 equal, get distribution peak around 0.5. strong prior expectations coin unbiased, increase parameters even :\\(\\alpha\\) \\(\\beta\\) equal?allows us capture skewed priors, perhaps capturing belief coin specific bias.Now, \\(\\alpha\\) \\(\\beta\\) less 1?might capture belief coin strongly biased, aren‚Äôt sure direction.","code":"from scipy import stats\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_beta(a, b):\n  x = np.linspace(0,1,num=500)\n  px = stats.beta.pdf(x, a, b)\n  \n  fig, ax = plt.subplots()\n  ax.plot(x, px)\n  plt.show()plot_beta(1,1)plot_beta(3,3)plot_beta(50,50)plot_beta(4,2)plot_beta(0.5,0.5)"},{"path":"hierarchical-generalization.html","id":"conjugate-distributions","chapter":"5 Hierarchical generalization","heading":"5.1.1 Conjugate distributions","text":"Beta distribution conjugate distribution Binomial distribution. means likelihood Binomial distribution prior Beta distribution, posterior also Beta distribution. Specifically, making assumptions,\\[\nP(\\theta|D) = \\text{Beta}(\\theta; \\alpha + k, \\beta + n-k)\n\\]parameters posterior distribution (1) sum \\(\\alpha\\) prior number observed heads (2) sum \\(\\beta\\) prior number observed tails. means parameters \\(\\alpha\\) \\(\\beta\\) Beta prior natural interpretation ‚Äúvirtual flips.‚Äù example, larger \\(\\alpha\\) compared \\(\\beta\\), biased toward heads expect \\(\\theta\\) . Additionally, larger \\(\\alpha\\) \\(\\beta\\) , certain (less diffuse) prior .","code":""},{"path":"hierarchical-generalization.html","id":"parameter-estimation","chapter":"5 Hierarchical generalization","heading":"5.1.2 Parameter estimation","text":"used conjugate distribution, can use plot_beta function generate posterior probability distributions coin flips.Suppose start fairly strong belief coin fair, represented distribution:Now, suppose flip coin 20 times comes heads every time. think bias coin now? According model:can see, cause shift beliefs somewhat.wasn‚Äôt totally realistic, though. picked coin ground, prior beliefs biased probably look like :happens now flipped coin 20 times came heads every time?might mildly surprised, 20 flips wouldn‚Äôt enough budge estimate bias coin much.Finally, let‚Äôs imagine situation weak prior belief coin biased:Now flip coin 100 times comes heads 48 times. updated beliefs ?can see, posterior distribution shows think coin probably fair now. illustrates sufficient evidence can override prior beliefs.","code":"plot_beta(30,30)plot_beta(30+20,30)plot_beta(2000,2000)plot_beta(2000+20,2000)plot_beta(5,1)plot_beta(5+48,1+52)"},{"path":"hierarchical-generalization.html","id":"hypothesis-averaging","chapter":"5 Hierarchical generalization","heading":"5.1.3 Hypothesis averaging","text":"Chapter 3, solved generalization problem summing hypotheses, weighted posterior probabilities. , can something similar.Suppose want know probability next flip coming heads. words, want know \\(P(\\text{heads}|D)\\). can averaging possible values \\(\\theta\\):\\[\nP(\\text{heads}|D) = \\int_\\theta P(\\text{heads}|\\theta) \\cdot P(\\theta|D) d\\theta = \\int_\\theta \\theta \\cdot P(\\theta|D) d\\theta\n\\]","code":""},{"path":"hierarchical-generalization.html","id":"overhypotheses","chapter":"5 Hierarchical generalization","heading":"5.2 Overhypotheses üôÜ","text":"Now consider slightly different situation. flip 19 different coins row, one time, come heads. Now pick 20th coin bag previous 19 coins. think probability 20th coin coming heads? higher 0.5?answered yes, ‚Äôs probably formed overhypothesis bias coins. flipping coins, may concluded particular set coins likely usual biased. result, estimate probability 20th coin coming heads higher otherwise .","code":""},{"path":"hierarchical-generalization.html","id":"the-shape-bias","chapter":"5 Hierarchical generalization","heading":"5.2.1 The shape bias","text":"coins example pretty artificial, notion overhypotheses one find language learning. phenomenon known shape bias refers fact even young children likely generalize new word based shape rather properties like color texture.\nFigure 5.2: common task used test shape bias.\nmakes sense objects tend common shapes less likely common colors textures.","code":""},{"path":"hierarchical-generalization.html","id":"modeling-the-learning-of-overhypotheses-through-hierarchical-bayesian-learning","chapter":"5 Hierarchical generalization","heading":"5.2.2 Modeling the learning of overhypotheses through hierarchical Bayesian learning","text":"Charles Kemp, Andy Perfors, Josh Tenenbaum developed model kind learning. focused bags black white marbles rather flipping coins. imagine problem many bags marbles draw . drawing many bags, draw single marble new bag make prediction proportion black white marbles bag.details model outside scope book. basic idea model learns two levels simultaneously. higher level, model learns parameters \\(\\alpha\\) \\(\\beta\\) Beta distribution characterizes proportion black white marbles bag. saw , Beta distribution can peak around particular proportion, can peaked around 0 1, meaning bag likely nearly black white.lower level, model learns specific distribution marbles within bag. draw 20 marbles 5 black, may uncertainty overall proportion bag, best estimate around 5/20 1/4.model excels able draw inferences across bags. see many bags full black white marbles, draw single black marble new bag, likely confident rest marbles bag black.see many bags mixed proportions black white marbles, draw single black marble new bag, far less confident proportion black marbles bag. model doesn‚Äôt make inferences multiple levels struggle draw distinction.","code":""},{"path":"sampling-assumptions.html","id":"sampling-assumptions","chapter":"6 Sampling assumptions","heading":"6 Sampling assumptions","text":"far, ‚Äôve seen strong sampling logical assumption make. isn‚Äôt always.example, consider hormone problem Chapter 3. randomly testing people‚Äôs hormone levels every person just happened healthy, clearly strong sampling wouldn‚Äôt appropriate, sampling full population healthy unhealthy people.raises question: people sensitive process data generated?Spoiler alert: Yes.","code":""},{"path":"sampling-assumptions.html","id":"word-learning","chapter":"6 Sampling assumptions","heading":"6.1 Word learning üí¨","text":"Suppose see following collection objects table.\nFigure 6.1: Xu & Tenenbaum (2007).\nNow consider following situation:teacher picks three blue circles pile calls ‚Äúwugs.‚Äùwug? circles wugs? just blue circles?Consider different situation:teacher picks one blue circle pile calls ‚Äúwug.‚Äù teacher asks child choose two (child picks two blue circles). teacher confirms also wugs.wug? circles wugs? just blue circles?situations, observed data : three blue circles labeled wugs. ‚Äôs different process data generated. one case, knowledgeable teacher picked wugs; , teacher picked one example (probably knowledgeable) child picked others.use terms ‚Äôve seen , teacher using strong sampling. child using something closer weak sampling.\nFigure 6.2: hierarchy objects. Xu & Tenenbaum (2007).\nbased actual study Fei Xu Josh Tenenbaum, also developed model task. asked whether children adults generalize word ‚Äúwug‚Äù basic-level category (circles lines ) subordinate category (blue circles lines ). results:\nFigure 6.3: Experimental results Xu & Tenenbaum (2007).\nPeople clearly distinguished teacher-driven situation child-driven (learner-driven) situation, much likely generalize ‚Äúwugs‚Äù subordinate category teacher picked objects.","code":""},{"path":"sampling-assumptions.html","id":"pedagogical-sampling","chapter":"6 Sampling assumptions","heading":"6.2 Pedagogical sampling üßë‚Äçüè´","text":"teacher choosing wugs said using pedagogical sampling. teacher deliberately used knowledge wug concept select informative examples help child learn concept quickly possible.idea explored Patrick Shafto, Noah Goodman, Thomas Griffiths (well Elizabeth Bonawitz researchers related work). put formal terms:\\[\n\\begin{equation}\nP_{\\text{teacher}}(d|h) \\propto (P_{\\text{learner}}(h|d))^\\alpha\n\\tag{6.1}\n\\end{equation}\n\\], \\(\\alpha\\) parameter controls optimized teacher‚Äôs choices . \\(\\alpha \\rightarrow \\infty\\), teacher choose examples \\(d\\) maximize learner‚Äôs posterior probability.means can figure learner update beliefs concept directly applying Bayes rule:\\[\n\\begin{equation}\n  P_{\\text{learner}}(h|d) = \\frac{P_{\\text{teacher}}(d|h)P(h)}{\\sum_{h_i} P_{\\text{teacher}}(d|h_i)P(h_i)}\n\\tag{6.2}\n\\end{equation}\n\\]equation makes clear teacher learner inextricably linked: teacher chooses examples maximize learner‚Äôs understanding, learner updates beliefs based expectations teacher choosing examples. model recursive.researchers explain paper, two equations system equations can rearranged create following equation defining teacher choose examples:\\[\n\\begin{equation}\n  P_{\\text{teacher}}(d|h) \\propto \\left( \\frac{P_{\\text{teacher}}(d|h) P(h)}{\\sum_{h_i} P_{\\text{teacher}}(d|h_i) P(h_i)} \\right)^\\alpha\n\\tag{6.3}\n\\end{equation}\n\\]solve equation? paper, use iterative algorithm, works like :Initialize \\(P_{\\text{teacher}}(d|h)\\) using weak sampling.Iterate following steps \\(P_{\\text{teacher}}(d|h)\\) stabilizes (doesn‚Äôt change one iteration next):\npossible \\(h\\) \\(d\\), compute \\(P_{\\text{learner}}(h|d)\\) using Equation (6.2) \\(P_{\\text{teacher}}(d|h)\\) values previous iteration.\npossible \\(d\\) \\(h\\), update \\(P_{\\text{teacher}}(d|h)\\) using \\(P_{\\text{learner}}(h|d)\\) values previous step, \\(P_{\\text{teacher}}(d|h_i) = P_{\\text{learner}}(h_i|d) / \\sum_{d_j} P_{\\text{learner}}(h_i|d_j)\\) (Equation (6.1)).\npossible \\(h\\) \\(d\\), compute \\(P_{\\text{learner}}(h|d)\\) using Equation (6.2) \\(P_{\\text{teacher}}(d|h)\\) values previous iteration.possible \\(d\\) \\(h\\), update \\(P_{\\text{teacher}}(d|h)\\) using \\(P_{\\text{learner}}(h|d)\\) values previous step, \\(P_{\\text{teacher}}(d|h_i) = P_{\\text{learner}}(h_i|d) / \\sum_{d_j} P_{\\text{learner}}(h_i|d_j)\\) (Equation (6.1)).","code":""},{"path":"sampling-assumptions.html","id":"the-rectangle-game","chapter":"6 Sampling assumptions","heading":"6.2.1 The rectangle game","text":"researchers tested model experiment using simple task called rectangle game.game, concepts rectangular boundaries two-dimensional space. teacher knows boundary learner figure examples given teacher.\nFigure 6.4: rectangle game. Shafto et al.¬†(2014).\none version, teacher can provide positive examples (‚Äú‚Äôs example ‚Äôs inside boundary‚Äù). another version, teacher can also provide negative examples (‚Äú‚Äôs example ‚Äôs inside boundary‚Äù).experiment, asked people play role learner rectangle game. three conditions:Teaching-pedagogical learning: People first acted teachers, learners.Pedagogical learning: People acted just learners told examples saw chosen teacher.Non-pedagogical learning: People acted just learners told examples saw chosen teacher.showing people examples, asked draw rectangle best guess boundary .One key prediction model informative locations examples pedagogical sampling corners rectangles. test whether learners understood , measured proportion examples ended corners rectangles people drew. results shown .\nFigure 6.5: Results Shafto et al.‚Äôs (2014) Experiment 1.\ncan see, people thought knowledgeable teacher providing examples (teaching-pedagogical pedagogical learning conditions), much likely draw tight boundaries around examples, compared didn‚Äôt think teacher providing examples (non-pedagogical learning condition). suggests people indeed sensitive process data generated incorporated understanding inferences.next chapter, ‚Äôll work one application people‚Äôs sensitivity different sampling assumptions: understanding pragmatics speech.","code":""},{"path":"pragmatics.html","id":"pragmatics","chapter":"7 Language pragmatics","heading":"7 Language pragmatics","text":"talk , often rely shared assumptions context get points across quickly. Linguists call pragmatics. example, consider following exchange:: ‚Äôm almost gas.B: ‚Äôs gas station ahead.Person didn‚Äôt explicitly ask question, Person B understood wouldn‚Äôt said anything didn‚Äôt need something weren‚Äôt explaining something , B responded answer implied question (‚Äúcan get gas?‚Äù).chapter, ‚Äôll see aspects pragmatics can explained combining Bayesian inference two new ideas: information theory decision theory (expected utility).","code":""},{"path":"pragmatics.html","id":"surprisal","chapter":"7 Language pragmatics","heading":"7.1 Surprisal üòØ","text":"‚Äôll focus one concept information theory: surprisal. Surprisal captures unexpected observation . Intuitively, unexpectedness observation depend probable . Namely, probable , less unexpected .want function \\(f(p)\\) takes probability gives us unexpectedness. reasonable constraints \\(f\\):observation probability 0, infinitely unexpected: \\(f(0) = \\infty\\).observation probability 1, unexpected : \\(f(1) = 0\\).two independent observations probabilities \\(p_1\\) \\(p_2\\), probability occurring \\(p_1 \\cdot p_2\\), want total unexpectedness observing sum unexpectedness observation: \\(f(p_1 \\cdot p_2) = f(p_1) + f(p_2)\\).function satisfies constraints :\\[\nf(p) = -log(p)\n\\]\nLet‚Äôs verify.\\(-log(p) \\rightarrow \\infty\\) \\(p \\rightarrow 0\\) ‚úÖ\\(-log(1) = 0\\) ‚úÖ\\(-log(p_1 \\cdot p_2) = -(log(p_1) + log(p_2)) = (-log(p_1))+(-log(p_2))\\) ‚úÖ‚Äôs plot function:surprisal language?‚Äôre communicating someone, surprisal can help decide information give person. example, imagine observe number 1 10 want share someone else. either say:saw even number.say:saw two.say? second one, duh. ?formal terms, ‚Äôre trying send signal number observed surprising person receiving signal. , ‚Äôre trying minimize surprisal \\(P(2|\\text{signal})\\).Exercise: Compute surprisal two signals.","code":"import numpy as np\nimport matplotlib.pyplot as plt\n\np = np.linspace(0.001,1,num=500)\nfp = -np.log(p)\n  \nfig, ax = plt.subplots()\nax.plot(p, fp)\nax.set_xlabel(\"p\")\nax.set_ylabel(\"surprisal\")\nplt.show()"},{"path":"pragmatics.html","id":"the-rational-speech-act-model","chapter":"7 Language pragmatics","heading":"7.2 The Rational Speech Act model üç™","text":"Let‚Äôs apply concept surprisal following example. initially two cookies cookie jar, may eaten , ‚Äôre telling friend many . can say:‚Äúcookies.‚Äù (ate one two.)‚Äúcookies.‚Äù (ate two.)‚Äúcookies.‚Äù (ate none.)Nothing ‚Äì stay silent. (eaten none number cookies.)Let‚Äôs define situation using binary matrix. row signal (say) column state (many cookies eaten).‚Äôll define function prints matrix heat map.Now let‚Äôs calculate literal listener interpret statements. row matrix (one signal) define distribution states. , state \\(h\\) signal \\(d\\), want compute:\\[\nP_{\\text{listener}}(h|d) \\propto P(d|h) P(h)\n\\]literal listener assumes , given state, signal consistent state chosen random speaker. already captured matrix : Signals consistent state value 1 cells signals inconsistent 0. also assume listener uniform prior states. Therefore, can calculate literal listener‚Äôs probabilities just normalizing rows matrix.‚Äôll , let‚Äôs write function .Now let‚Äôs calculate pragmatic speaker. pragmatic speaker chooses signal given state. Therefore, time, columns matrix probability distributions.Unlike literal listener, pragmatic speaker trying convey information, choose signal greatest expected utility literal listener. Expected utility case defined expected increase understanding true state world. Remember, already way quantifying : surprisal. ‚Äôll define utility negative surprisal state given signal point view literal listener: \\(-(-log(P_{\\text{listener}}(h|d)))\\).code, used something known ‚Äúsoftmax‚Äù rather true maximizing function. exponential function captures idea speaker choose probabilistically, generally choosing options utility. fairly common assumption psychology economics often don‚Äôt complete information making decisions (modeling people‚Äôs decisions), assuming pure maximizing function isn‚Äôt always best choice.However, also included parameter alpha controls close maximizing speaker . larger alpha , closer speaker get maximizing utility.Now can calculate probabilities pragmatic listener. similar literal listener pragmatic listener receives signal computes probability state given signal. difference pragmatic listener doesn‚Äôt assume signals chosen random, signals chosen pragmatic speaker .can compute pragmatic listener normalizing matrix along rows.pragmatic listener correctly infers scalar implicature, concept pragmatics linguistics. Even though ‚Äúcookies‚Äù consistent eating cookies, pragmatic listener infer person says ate eat (otherwise said ).","code":"state_matrix = np.array([[0,1,1],\n                         [0,0,1],\n                         [1,0,0],\n                         [1,1,1]])\n                         \nsignals = [\"some\", \"all\", \"no\", \"silent\"]\nstates = [\"0\",\"1\",\"2\"]def print_state_matrix(m, x_labels=None, y_labels=None):\n  fig, ax = plt.subplots()\n  im = ax.imshow(m)\n  \n  if (x_labels and y_labels):\n    ax.set_xticks(np.arange(len(x_labels)), labels=x_labels)\n    ax.set_yticks(np.arange(len(y_labels)), labels=y_labels)\n  \n    for i in range(len(y_labels)):\n      for j in range(len(x_labels)):\n        text = ax.text(j, i, round(m[i, j],2),\n          ha=\"center\", va=\"center\", color=\"w\")\n  \n  fig.tight_layout()\n  plt.show()\n\n# Print out the matrix\nprint_state_matrix(state_matrix, states, signals)def normalize_rows(m):\n  normalized_m = np.zeros((m.shape))\n  row_num = 0\n  for row in m:\n    normalized_m[row_num,:] = row / sum(row)\n    row_num += 1\n    \n  return(normalized_m)literal_listener = normalize_rows(state_matrix)\nprint_state_matrix(literal_listener, states, signals)# First we apply a softmax decision function\nalpha = 1 # paramater controls how close to maximizing speaker is\npragmatic_speaker = np.exp(np.log(literal_listener)*alpha)## <string>:1: RuntimeWarning: divide by zero encountered in logfor col in range(pragmatic_speaker.shape[1]):\n  pragmatic_speaker[:,col] = (pragmatic_speaker[:,col] /\n    sum(pragmatic_speaker[:,col]))\n\nprint_state_matrix(pragmatic_speaker, states, signals)pragmatic_listener = normalize_rows(pragmatic_speaker)\nprint_state_matrix(pragmatic_listener, states, signals)"},{"path":"pragmatics.html","id":"does-this-model-match-human-behavior","chapter":"7 Language pragmatics","heading":"7.3 Does this model match human behavior? üîµ","text":"word, yes. One first papers look Michael Frank Noah Goodman. applied model simple task picking shape small set tested model‚Äôs predictions behavioral experiment.","code":""},{"path":"pragmatics.html","id":"the-task","chapter":"7 Language pragmatics","heading":"7.3.1 The task","text":"speaker: ‚ÄúImagine talking someone want refer middle object. word use: blue circle?‚Äù üü¶ üîµ üü©listener: ‚ÄúImagine someone talking uses word blue refer one objects. object talking ?‚Äù üü¶ üîµ üü©task experiment, researchers varied actual set objects systematic way.","code":""},{"path":"pragmatics.html","id":"the-model-1","chapter":"7 Language pragmatics","heading":"7.3.2 The model","text":"model virtually identical cookie jar model just worked . assume speakers try choose words maximize listener‚Äôs utility, measured surprisal.literal listener case assume object consistent speaker‚Äôs chosen word one referring .assumptions, researchers derive probability speaker choosing word (details paper) :\\[\nP(w|r_s,C) = \\frac{|w|^{-1}}{\\sum_{w^\\prime \\W} {|w^\\prime|}^{-1}}\n\\]\\(w\\): speaker‚Äôs chosen word.\\(r_s\\): object speaker meant refer .\\(C\\): set objects.\\(|w|\\): number objects \\(w\\) apply .\\(W\\): set words apply object speaker meant refer .Regarding \\(W\\), imagine speaker wanted refer blue circle. case, \\(W = \\{ \\text{blue}, \\text{circle} \\}\\), either word apply object.According model, speakers tend choose words uniquely identify object set. example, set , blue applies two objects (blue square blue circle), circle applies one, circle get higher probability.","code":""},{"path":"pragmatics.html","id":"homework-4-implement-the-model","chapter":"7 Language pragmatics","heading":"7.3.3 Homework 4: Implement the model","text":"haven‚Äôt provided details model assignment finish implementation , run simulations, collect small amount real data compare model .","code":""},{"path":"social-cognition.html","id":"social-cognition","chapter":"8 Social cognition","heading":"8 Social cognition","text":"‚Äôs nothing yet.","code":""}]
