[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"first version digital book Computational Psychology course ‚Äì cognitive modeling course. includes homework assignments written Python.","code":""},{"path":"index.html","id":"thanks","chapter":"Welcome","heading":"Thanks","text":"book borrows inspiration content (especially Chapters 2 7) Fausto Carcassi‚Äôs Introduction Cognitive Modelling R book tremendously grateful sharing book publicly. Hopefully resource equally helpful others.number modeling examples homework assignments adapted code written Danielle Navarro previous iterations Computational Cognitive Science course Andy Perfors. grateful making materials public (well documented).feedback welcome encouraged.","code":""},{"path":"index.html","id":"license","chapter":"Welcome","heading":"License","text":"Anyone free reuse adapt book non-commercial purposes, attribution. use book way, please tell . ‚Äôd love know!","code":""},{"path":"intro.html","id":"intro","chapter":"1 Why computational modeling?","heading":"1 Why computational modeling?","text":"field psychology full theories. book focused specific type theory: type can formalized computational model.advantage computational (mathematical) theory one can‚Äôt expressed computational terms? biggest advantage precision. example, suppose say attention like spotlight üî¶: can attend things currently within light, can control light shining, things outside light outside awareness. kind theory ‚Äì analogy-based one ‚Äì ‚Äôs good start making general qualitative predictions attention works.‚Äôll quickly run problems want make precise quantitative predictions attention. big spotlight? size expand contract? quickly move around? attention completely absent outside spotlight ramp get near edge light? words, wanted build computer model theory, simple analogy doesn‚Äôt cut .Computational models, nothing else, force us explicit assumptions.","code":""},{"path":"intro.html","id":"representations","chapter":"1 Why computational modeling?","heading":"1.1 Representations üî∏","text":"don‚Äôt perceive world truly . give one example, visible spectrum eyes can detect just fraction full electromagnetic spectrum. words, ‚Äôre seeing incomplete picture surrounding world.\nFigure 1.1: visible light spectrum. Source: Philip Ronan/Wikipedia.\nSimilarly, constantly making assumptions things see hear using assumptions fill gaps.heads kind model world around us ‚Äì cognitive scientists call mental representation. representations help us reach rapid conclusions things involving language, causes effects, concepts, mental states, many aspects cognition.key questions cognitive scientists use computational models :mental representations rely ?minds use representations learn get new information?kind information get expectations kind information ‚Äôre getting affect use ?book elaborate, examples, questions.","code":""},{"path":"intro.html","id":"homework-1-build-your-first-computational-model","chapter":"1 Why computational modeling?","heading":"1.2 Homework 1: Build your first computational model üíª","text":"get initial experience computational modeling, ‚Äôll build experiment simple model classical conditioning developed Robert Rescorla Allan Wagner ‚Äì now called Rescorla-Wagner model.homework assignments book done Google Colab. Click button top section view Homework 1.‚Äôre unfamiliar Colab (Jupyter Notebooks), watch brief introduction video.Note: ‚Äôll make copy notebook saved Drive order edit .","code":""},{"path":"bayes.html","id":"bayes","chapter":"2 Bayesian inference","heading":"2 Bayesian inference","text":"chapter reviews basic probability Bayesian inference. might asking : psychology? answer becomes clear recognize ‚Äôre making sense world drawing inferences. see ambiguous image, rabbit duck? someone mumbles something, say ‚Äúhello‚Äù ‚Äúgo hell?‚Äù take pill headache goes away, pill eliminate headache headache go away ?\nFigure 2.1: duck-rabbit illusion.\nexamples, one hypothesis observed. Probability Bayesian inference provide tools optimally determining probable different hypotheses . One claims book people making inferences situations like , inferences often well predicted optimal inferences dictated probability theory.","code":""},{"path":"bayes.html","id":"basic-probability","chapter":"2 Bayesian inference","heading":"2.1 Basic probability üé≤","text":"Conditional probability: \\(P(b|) = \\frac{P(,b)}{P()}\\)Chain rule: \\(P(,b) = P(b|)P()\\)Marginalization: \\(P(d) = \\sum_h P(d,h) = \\sum_h P(d|h) P(h)\\)Bayes‚Äôs rule: \\(P(h|d) = \\frac{P(d|h) P(h)}{P(d)} = \\frac{P(d|h) P(h)}{\\sum_h P(d|h) P(h)}\\). \\(P(d|h)\\) referred likelihood, \\(P(h)\\) prior, \\(P(h|d)\\) posterior.","code":""},{"path":"bayes.html","id":"a-motivating-example-sampling-from-a-bag","chapter":"2 Bayesian inference","heading":"2.2 A motivating example: Sampling from a bag üëù","text":"Suppose bag full black red balls. can‚Äôt see inside bag don‚Äôt know many black red balls inside, know nine total balls bag.want know many black balls red balls . finite number hypotheses: {0 black balls, 1 black ball, 2 black balls, ‚Ä¶, 9 black balls}. Let‚Äôs call hypotheses \\(B_0\\), \\(B_1\\), etc., respectively.don‚Äôt know hypothesis true, might idea hypotheses likely others. Therefore, natural represent uncertainty probability distribution possible unknown states world ‚Äì case, 10 hypotheses. hypothesis gets assigned probability, probabilities sum 1.simplicity, let‚Äôs assume don‚Äôt idea hypotheses likely. words, give every hypothesis probability: 1/10 = 0.1. also called uniform distribution hypotheses. distribution prior.Now suppose put hand bag pull ball random. possible observations : {black, red}, Let‚Äôs call \\(B\\) \\(R\\), respectively. probability observing color depends hypothesis true, .e., many balls color bag. instance, \\(B_0\\) true (0 black balls bag), probability observing red ball 1 (\\(P(R|B_0)=1\\)), probability observing black ball 0 (\\(P(B|B_0)=0\\)). expressions tell us probable observations , given specific hypothesis, likelihoods.","code":""},{"path":"bayes.html","id":"sampling-from-the-generative-model","chapter":"2 Bayesian inference","heading":"2.2.1 Sampling from the generative model","text":"Now distribution hypotheses (prior), \\(P(h)\\), distribution observations given hypothesis (likelihood), \\(P(d|h)\\). two things allow us create generative model, model sampling new data.sample generative model? Note hypothesis \\(h\\) true depend data, data \\(d\\) depends hypothesis true. Therefore, can sample generative model using following two-step process:Sample hypothesis prior.Sample data given hypothesis, using likelihood.Let‚Äôs first create vector probability hypothesis:Now ‚Äôll first step: create vector 10000 hypotheses sampled prior:, number corresponds one hypothesis: 0 corresponds \\(B_0\\), 1 \\(B_1\\), . sample represents one possible way (hypothesis) world . Since prior uniform (hypothesis probability), hypothesis appears equally often. can plot samples verify:Now next step. sample prior_samples, want sample observation. , let‚Äôs pause second think probability pulling black ball given hypothesis \\(B_3\\) true, example. means 3 black balls 6 red balls bag. probability pulling black ball bag random 3/9.Generalizing idea, can get probability pulling black ball bag dividing elements prior_samples 9:Now, complete generative model, just need sample one value element p_black. sample represents draw bag.ball_samples 1 black 0 red. , let‚Äôs plot samples.can think plot representing overall beliefs number red black balls bag, averaged possible hypotheses.surprisingly, got equal numbers red black balls. makes sense: didn‚Äôt prior expectations whether red black balls likely bag.beliefs change pull ball bag? , respond evidence?","code":"import numpy as np\nimport random\n\nrandom.seed(2022) # set random seed to get same results every time\n\nh_priors = np.repeat(0.1,10)\nprint(h_priors)## [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]prior_samples = np.array(random.choices(np.arange(0,10,1),\n                   weights = h_priors, \n                   k = 10000))\n                   \nprint(prior_samples[0:9]) # printing out just a few## [5 4 3 0 7 9 4 6 8]import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nn, bins, patches = ax.hist(prior_samples, bins=10)\nax.set_xlabel('Prior sample')\nax.set_ylabel('Number of samples')p_black = prior_samples / 9\nprint(p_black[0:4]) # print out just a few## [0.55555556 0.44444444 0.33333333 0.        ]ball_samples = np.random.binomial(n = np.repeat(1,len(p_black)),\n                                  p = p_black)\nprint(ball_samples[0:9])## [0 1 0 0 1 1 0 1 0]fig, ax = plt.subplots()\nn, bins, patches = ax.hist(ball_samples, bins=2)\nax.set_xticks([0,1])\nax.set_ylabel('Number of samples')"},{"path":"bayes.html","id":"bayesian-updating-learning-from-evidence","chapter":"2 Bayesian inference","heading":"2.3 Bayesian updating: Learning from evidence ü§î","text":"Let‚Äôs apply Bayes‚Äôs rule see optimally incorporate new data beliefs.","code":""},{"path":"bayes.html","id":"applying-bayess-rule-to-the-bag-case","chapter":"2 Bayesian inference","heading":"2.3.1 Applying Bayes‚Äôs rule to the bag case","text":"Suppose uniform prior distribution 10 hypotheses balls bag. Now pick ball ‚Äôs black. Given observation \\(B\\), change probabilities give hypothesis?Intuitively, now give little bit probability hypotheses black balls red balls, hypotheses make observations likely. Moreover, can safely exclude hypothesis \\(B_0\\), observation impossible \\(B_0\\) true. Let‚Äôs calculate Bayes‚Äôs rule.prior vector h_priors defined . Given observed \\(B\\), likelihood tell us, hypothesis, probability \\(B\\) given hypothesis. example, \\(B_9\\), likelihood \\(P(B|B_9) = 1\\). \\(B_8\\), \\(P(B|B_8) = 8/9\\), 8 9 balls black.Generalizing idea, \\(P(B|B_n) = n/9\\). can therefore compute likelihoods hypotheses vector:Now suppose want find probability hypothesis \\(B_5\\) observing one draw \\(B\\). Let‚Äôs apply Bayes‚Äôs rule:\\[P(B_5 | B) = \\frac{P(B|B_5) P(B_5)}{\\sum_h{p(B|h) P(h)}}\\]Let‚Äôs compute parts need calculate \\(P(B_5 | B)\\).Let‚Äôs update probabilities hypotheses compact way.expected, Bayes‚Äôs rule says increase probability assign hypotheses black balls red balls. Additionally, let‚Äôs double-check posterior probabilities sum 1 (requirement valid probability distribution).Finally, let‚Äôs plot posterior probabilities.","code":"likelihoods = np.arange(0,10,1) / 9\n\nprint(likelihoods)## [0.         0.11111111 0.22222222 0.33333333 0.44444444 0.55555556\n##  0.66666667 0.77777778 0.88888889 1.        ]# Prior\np_B5 = h_priors[3]\n\n# Likelihood\nlikelihood_B5 = likelihoods[5]\n\n# Data\np_B = sum(likelihoods*h_priors)\n\n# Posterior\np_B5_given_B = p_B5 * likelihood_B5 / p_B\n\n# Print out results\nprint(\"P(B5) = \" + str(p_B5)) # Prior## P(B5) = 0.1print(\"P(B|B5) = \" + str(likelihood_B5)) # Likelihood## P(B|B5) = 0.5555555555555556print(\"P(B) = \" + str(p_B)) # Data## P(B) = 0.5print(\"P(B5|B) = \" + str(p_B5_given_B)) # Posterior## P(B5|B) = 0.11111111111111112posteriors = (likelihoods * h_priors) / sum(likelihoods * h_priors)\n\nfor i in range(len(posteriors)):\n  print(\"P(B\" + str(i) + \"|B) = \" + str(posteriors[i]))## P(B0|B) = 0.0\n## P(B1|B) = 0.022222222222222223\n## P(B2|B) = 0.044444444444444446\n## P(B3|B) = 0.06666666666666667\n## P(B4|B) = 0.08888888888888889\n## P(B5|B) = 0.11111111111111112\n## P(B6|B) = 0.13333333333333333\n## P(B7|B) = 0.15555555555555556\n## P(B8|B) = 0.17777777777777778\n## P(B9|B) = 0.2sum(posteriors)## 1.0fig, ax = plt.subplots()\nhypotheses = ('B0', 'B1', 'B2', 'B3', 'B4',\n              'B5', 'B6', 'B7', 'B8', 'B9')\ny_pos = np.arange(len(hypotheses))\n\nax.barh(y_pos, posteriors, align='center')## <BarContainer object of 10 artists>ax.set_yticks(y_pos)\nax.set_yticklabels(hypotheses)\nax.set_xlabel('Probability')\nax.set_ylabel('Hypothesis')"},{"path":"bayes.html","id":"normalization","chapter":"2 Bayesian inference","heading":"2.3.2 How to avoid calculating P(d)","text":"practice, generally need calculate \\(P(d)\\) (denominator Bayes‚Äôs rule) explicitly. ‚Äôll give general idea section.First, create vector prior probabilities, many components hypotheses. ‚Äôll just reuse h_priors. Note probabilities sum 1, ‚Äôs probability distribution.Next, create likelihood array. calculations , vector likelihoods specific observation. However, like something encodes likelihood function possible observation given possible hypothesis, rather just specific observation.example, two possible observations: \\(B\\) \\(R\\). can encode likelihood \\(m \\times n\\) array \\(m\\) number hypotheses \\(n\\) number possible observations. case: \\(10 \\times 2\\).Now multiply prior likelihoods together (numerator Bayes‚Äôs rule) element-wise (first element gets multiplied first element, second element second element, etc.):Finally, want distribution column, .e., distribution hypotheses given observation. Therefore, sum column divide element sum column:gives us posterior without us explicitly calculate evidence observation!general idea . denominator Bayes‚Äôs rule, fixed observation, constant, can usually get away computing \\(P(d|h) P(h)\\) every possible hypothesis \\(h\\) ‚Äúnormalize‚Äù resulting values sum 1 (remember order valid probability distribution).","code":"h_priors## array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])likelihood_array = np.array((np.arange(0,10,1) / 9,\n                             1-(np.arange(0,10,1) / 9))).T\nprint(likelihood_array)## [[0.         1.        ]\n##  [0.11111111 0.88888889]\n##  [0.22222222 0.77777778]\n##  [0.33333333 0.66666667]\n##  [0.44444444 0.55555556]\n##  [0.55555556 0.44444444]\n##  [0.66666667 0.33333333]\n##  [0.77777778 0.22222222]\n##  [0.88888889 0.11111111]\n##  [1.         0.        ]]prior_array = np.array((h_priors, h_priors)).T\nbayes_numerator = likelihood_array * prior_array\n\nprint(bayes_numerator)## [[0.         0.1       ]\n##  [0.01111111 0.08888889]\n##  [0.02222222 0.07777778]\n##  [0.03333333 0.06666667]\n##  [0.04444444 0.05555556]\n##  [0.05555556 0.04444444]\n##  [0.06666667 0.03333333]\n##  [0.07777778 0.02222222]\n##  [0.08888889 0.01111111]\n##  [0.1        0.        ]]posteriors = bayes_numerator / np.sum(bayes_numerator, axis = 0)\nprint(posteriors)## [[0.         0.2       ]\n##  [0.02222222 0.17777778]\n##  [0.04444444 0.15555556]\n##  [0.06666667 0.13333333]\n##  [0.08888889 0.11111111]\n##  [0.11111111 0.08888889]\n##  [0.13333333 0.06666667]\n##  [0.15555556 0.04444444]\n##  [0.17777778 0.02222222]\n##  [0.2        0.        ]]"},{"path":"bayes.html","id":"bayes-exercises","chapter":"2 Bayesian inference","heading":"2.4 Exercises üìù","text":"","code":""},{"path":"bayes.html","id":"taxi-cabs","chapter":"2 Bayesian inference","heading":"2.4.1 Taxi cabs","text":"80% taxi cabs Simpletown green 20% yellow. hit--run accident happened night involving taxi. witness claimed taxi yellow. extensive testing, determined witness can correctly identify color taxi 75% time conditions like ones present accident. probability taxi yellow?","code":""},{"path":"bayes.html","id":"flipping-coins","chapter":"2 Bayesian inference","heading":"2.4.2 Flipping coins","text":"observe sequence coin flips want determine coin trick coin (always comes heads) normal coin. Let \\(P(\\text{heads}) = \\theta\\). Let \\(h_1\\) hypothesis \\(\\theta = 0.5\\) (fair coin). Let \\(h_2\\) hypothesis \\(\\theta = 1\\) (trick coin).problem, define something called prior odds, ratio prior probabilities assigned two hypotheses: \\(\\frac{P(h_1)}{P(h_2)}\\). coins aren‚Äôt trick coins, assume \\(\\frac{P(h_1)}{P(h_2)} = 999\\), indicating strong (999 1) prior probability favor fair coins. can now compute posterior odds, ratio posterior probabilities two hypotheses observing data \\(d\\): \\(\\frac{P(h_1|d)}{P(h_2|d)}\\).Compute posterior odds observing following sequences coin flips:HHTHTHHHHHHHHHHHHHHH","code":""},{"path":"bayes.html","id":"solutions","chapter":"2 Bayesian inference","heading":"2.5 Solutions","text":"","code":""},{"path":"bayes.html","id":"taxi-cabs-1","chapter":"2 Bayesian inference","heading":"2.5.1 Taxi cabs","text":"Let \\(h_1\\) hypothesis taxi yellow. Let \\(h_2\\) hypothesis taxi green. Let data \\(d\\) witness report taxi yellow. Given problem statement, \\(P(h_1) = 0.2\\) \\(P(h_2) = 0.8\\). witness accurate 75% time, \\(P(d|h1) = 0.75\\) (witness saw yellow taxi correctly identified ) \\(P(d|h2) = 0.25\\) (witness saw green taxi identified yellow). Now apply Bayes‚Äôs rule:\\[\\begin{align}\nP(h_1|d) &= \\frac{P(d|h_1) P(h_1)}{P(d)} \\\\\n&= \\frac{P(d|h_1) P(h_1)}{P(d|h_1) P(h_1) + P(d|h_2) P(h_2)} \\\\\n&= \\frac{(0.75) (0.2)}{(0.75)(0.2) + (0.25)(0.8)} \\approx 0.43\n\\end{align}\\]yellow cabs rare (low prior probability), actually probable cab green, even though witness 75% accurate.","code":""},{"path":"bayes.html","id":"flipping-coins-1","chapter":"2 Bayesian inference","heading":"2.5.2 Flipping coins","text":"","code":""},{"path":"bayes.html","id":"hhtht","chapter":"2 Bayesian inference","heading":"HHTHT","text":"\\[\n\\begin{align}\n\\frac{P(h_1|d)}{P(h_2|d)} &= \\frac{P(d|h_1)}{P(d|h_2)} \\frac{P(h_1)}{P(h_2)} \\\\\n&= \\frac{(1/2)^5}{0} \\times 999 = \\inf\n\\end{align}\n\\]\nsequence isn‚Äôt even possible \\(h_2\\) infinite evidence favor \\(h_1\\).","code":""},{"path":"bayes.html","id":"hhhhh","chapter":"2 Bayesian inference","heading":"HHHHH","text":"\\[\n\\begin{align}\n\\frac{P(h_1|d)}{P(h_2|d)} &= \\frac{P(d|h_1)}{P(d|h_2)} \\frac{P(h_1)}{P(h_2)} \\\\\n&= \\frac{(1/2)^5}{1^5} \\times 999 = 31.2\n\\end{align}\n\\]sequence favors \\(h_1\\) factor 31. Even five heads row can‚Äôt overcome strong prior favoring \\(h_1\\).","code":""},{"path":"bayes.html","id":"hhhhhhhhhh","chapter":"2 Bayesian inference","heading":"HHHHHHHHHH","text":"\\[\n\\begin{align}\n\\frac{P(h_1|d)}{P(h_2|d)} &= \\frac{P(d|h_1)}{P(d|h_2)} \\frac{P(h_1)}{P(h_2)} \\\\\n&= \\frac{(1/2)^{10}}{1^{10}} \\times 999 = 0.98\n\\end{align}\n\\]Now evidence favors \\(h_2\\) (trick coin) just barely.","code":""},{"path":"generalization.html","id":"generalization","chapter":"3 Generalization","heading":"3 Generalization","text":"last chapter, learned much cognition making inferences. common inference ‚Äôre faced involves generalizing examples things new cases.child hears brand new word figure objects apply word .eat one candy assorted box try guess others might taste like.remember friend liked crosswords weekend trip cabin one time, guess might like book puzzles gift (generalizing interests).can apply Bayesian inference kinds problems?","code":""},{"path":"generalization.html","id":"hormones","chapter":"3 Generalization","heading":"3.1 Healthy hormone levels üíâ","text":"example comes 2001 paper Josh Tenenbaum Thomas Griffiths1The basic problem: learn value healthy hormone level (say, 60) varies scale 1 100 (integers ). probability another value (say, 70) also healthy?","code":""},{"path":"generalization.html","id":"setting-up-a-model","chapter":"3 Generalization","heading":"3.1.1 Setting up a model","text":"","code":""},{"path":"generalization.html","id":"the-hypothesis-space","chapter":"3 Generalization","heading":"3.1.1.1 The hypothesis space","text":"start , ‚Äôll assume healthy values lie contiguous interval. Using term paper, interval consequential region \\(C\\).hypothesis space consists possible consequential regions. example, [0,100], [10,19], [44,45], valid hypotheses. full hypothesis space every valid interval 0 100.","code":""},{"path":"generalization.html","id":"prior","chapter":"3 Generalization","heading":"3.1.1.2 Prior","text":"much weight assign hypothesis? might reason favor shorter intervals longer ones, example. paper, use Erlang prior. Alternatively, simplicity calculation, assume uniform prior distribution, placing equal weight hypotheses, like previous chapter. tantamount making prior assumptions intervals probable.","code":""},{"path":"generalization.html","id":"likelihood","chapter":"3 Generalization","heading":"3.1.1.3 Likelihood","text":"Suppose learn healthy patient hormone level 60. likelihood observing value, assuming know hypothesis correct? , \\(P(x = 60 | h)\\). depends assume patient chosen.","code":""},{"path":"generalization.html","id":"weak-strong-sampling","chapter":"3 Generalization","heading":"3.1.1.3.1 Weak vs.¬†strong sampling","text":"weak sampling, assume observation sampled full range possibilities, just coincidence happened get one consequential region (healthy patient). ‚Äôs true, probability getting particular value doesn‚Äôt depend hypothesis true:\\[\nP(x|h) = \\frac{1}{L}\n\\]\\(L\\) length range possible values (100 case).strong sampling, assume observation specifically chosen example consequential region \\(C\\). words, someone chose healthy person tested hormone levels example . case, probability seeing particular value depends size region:\\[\nP(x|h) = \\begin{cases} \n  \\frac{1}{|h|} & \\text{} x \\h \\\\\n  0 & \\text{otherwise}\n  \\end{cases}\n\\]\n\\(|h|\\) size \\(h\\), .e., number values contained \\(h\\). multiple observations \\(X = \\{x_1, x_2, \\ldots, x_n \\}\\), \\(P(X|h) = (1/|h|)^n\\). assume sample independent like coin flip.result strong sampling assumption size principle: among hypotheses include observed examples, smallest receive higher posterior probability higher likelihoods.\nFigure 3.1: Sample hypotheses. thickness lines indicates likelihood, depicting size principle. Image Griffiths & Tenenbaum (2001).\n","code":""},{"path":"generalization.html","id":"posterior","chapter":"3 Generalization","heading":"3.1.1.4 Posterior","text":"can now simply apply Bayes‚Äôs rule compute probability hypothesis, given observation (set observations).\\[\nP(h|X) = \\frac{P(X|h) P(h)}{\\sum_{h_i} P(X|h_i) P(h_i)}\n\\]","code":""},{"path":"generalization.html","id":"generalizing","chapter":"3 Generalization","heading":"3.1.2 Generalizing","text":"aren‚Äôt quite finished. Remember really want know probability new value \\(y\\) also healthy hormone level. point done assigned probability interval consequential region.want essentially two-step process:hypothesis \\(h\\), check see \\(y\\) ., check probable \\(h\\) consequential region \\(C\\), given observations \\(X\\).basic idea sometimes known hypothesis averaging don‚Äôt actually care hypothesis right one, ‚Äôll just average hypotheses, weighted probable . Specifically, ‚Äôll compute:\\[\nP(y \\C|X) = \\sum_h P(y \\C | h) P(h | X)\n\\]\nsecond term right computed earlier using Bayes‚Äôs rule.first term? time, ‚Äôll assume weak sampling ‚Äôs reason assume new value \\(y\\) chosen healthy value .","code":""},{"path":"generalization.html","id":"homework-2-finish-the-details","chapter":"3 Generalization","heading":"3.1.3 Homework 2: Finish the details","text":"haven‚Äôt provided details model assignment finish implementation , run simulations, collect small amount real data compare model .","code":""},{"path":"generalization.html","id":"the-number-game","chapter":"3 Generalization","heading":"3.2 The number game üî¢","text":"domains, requiring concepts restricted contiguous intervals realistic. Numbers one example. Consider space possible number concepts make integers 1 100. addition concepts like ‚Äúnumbers 20 50,‚Äù many plausible concepts like ‚Äúmultiples 10,‚Äù ‚Äúeven numbers,‚Äù ‚Äúpowers 3.‚ÄùConsider following problem: given one examples \\(X\\) numbers fit rule want know probable new number \\(y\\) also fits rule.model discussed can naturally extended problem. likelihood, can make strong sampling assumption .prior things get little trickier. Intuitively concepts like ‚Äúeven numbers‚Äù seem probable even seeing examples concepts like ‚Äúmultiples 7.‚Äù now becomes psychological question: rules people find intuitively plausible? single way decide , run survey find : Give people long list rules ask judge intuitively natural seem. construct prior probability distribution using data.Alternatively, come definition ‚Äúcomplexity‚Äù hypotheses assume less complex hypotheses receive higher prior probability.chosen prior probability distribution \\(P(h)\\), can now proceed just .","code":""},{"path":"generalization.html","id":"inductive-generalizations-about-animal-properties","chapter":"3 Generalization","heading":"3.3 Inductive generalizations about animal properties üê¥","text":"Now let‚Äôs consider even complex generalization problem, based 2002 paper Neville Sanjana Josh Tenenbaum2. problem generalizing properties set example animals animals. paper uses following example:way read follows: premises state chimps squirrels blicketitis. conclusion horses blicketitis. inductive generalization question probable conclusion given premises? Intuitively, conclusion example seems plausible conclusion following example:interesting psychological question generalizations seem intuitively plausible others?","code":"Chimps have blicketitis\nSquirrels have blicketitis\n--------------------------\nHorses have blicketitisChimps have blicketitis\nGorillas have blicketitis\n--------------------------\nHorses have blicketitis"},{"path":"generalization.html","id":"hypothesis-space","chapter":"3 Generalization","heading":"3.3.1 Hypothesis space","text":"problem conceptually similar ones ‚Äôve already discussing. want infer animals blicketitis seeing examples animals blicketitis. first question answer analogue consequential region problem. Animals don‚Äôt naturally fall one-dimensional interval ‚Äôll need define different hypothesis space. One possibility hierarchy, naturally captures knowledge people animals.","code":""},{"path":"generalization.html","id":"clustering","chapter":"3 Generalization","heading":"3.3.1.1 Clustering","text":"paper first creates hierarchy eight animals using similarity data collected people. Specifically, asked people judge similar pairs eight animals calculated average similarity judgment animal.similarity judgments can used construct tree using simple clustering algorithm. algorithm works follows:Put animals cluster.one cluster hasn‚Äôt placed group, following:\nIdentify pair clusters greatest similarity .\nGroup clusters new cluster.\nIdentify pair clusters greatest similarity .Group clusters new cluster.several approaches computing similarity two clusters contain multiple animals. example, might use maximum similarity pair individual animals two clusters.results algorithm can represented tree, shown . node tree represents cluster. hypotheses consider combination 1, 2, 3 clusters determined using clustering algorithm.\nFigure 3.2: tree animal species. Image Sanjana & Tenenbaum (2002).\n","code":""},{"path":"generalization.html","id":"the-model","chapter":"3 Generalization","heading":"3.3.2 The model","text":"can now define model. First, let‚Äôs define \\(P(h)\\) \\(h\\) set clusters. authors make assumption analogous following:\\[\nP(h) \\propto \\frac{1}{\\phi^k}\n\\]\n\\(k\\) number clusters \\(h\\). \\(\\phi\\) parameter can choose. long \\(\\phi > 1\\), \\(P(h)\\) smaller hypotheses consisting clusters. effect assigning higher weight ‚Äúsimpler‚Äù hypotheses., make strong sampling assumption likelihood. time, \\(|h|\\) number animal species \\(h\\) \\(n\\) number examples premises.consequence assumptions something like Occam‚Äôs razor says simplest explanation preferred. model assign 0 likelihood hypotheses don‚Äôt include \\(X\\), thus narrowing hypothesis space just hypotheses possible. , favor hypotheses fewer clusters fewer animals. words, favor simplest hypotheses consistent examples ‚Äôve seen.","code":""},{"path":"generalization.html","id":"try-out-the-model-yourself","chapter":"3 Generalization","heading":"3.3.3 Try out the model yourself","text":"can try running version model making copy code . , let‚Äôs look model handles specific cases.Let‚Äôs start single example: horses can get blicketitis., see standard generalization curve. Now, let‚Äôs add animals.Now model increased probability animals. makes sense reason think blicketitis might affect lots different animals.Adding squirrel supported idea. add additional examples animals ‚Äôve already seen?Now probabilities animals drop, ‚Äôs starting look like maybe disease affects four animals ‚Äôve seen far.sum , ‚Äôve seen small number examples, like single horse, model generally prefer simpler hypotheses. ‚Äôve seen data, favor complex hypotheses (like group animals two separate evolutionary clusters) data support .one final example, let‚Äôs look specific impact multiple examples single animal., see model becomes increasingly confident property unique gorillas. makes intuitive sense ‚Äôs something people seem exhibit judgments. , point paper, ‚Äôs something non-probabilistic models can easily explain.","code":"animalGeneralization([\"horse\"])## array([1.        , 0.52984834, 0.30628906, 0.30628906, 0.19579428,\n##        0.19579428, 0.12893614, 0.12893614, 0.0842413 , 0.0842413 ])animalGeneralization([\"horse\", \"cow\", \"mouse\"])## array([1.        , 1.        , 0.57379051, 0.57379051, 0.47852518,\n##        0.47852518, 1.        , 0.60980466, 0.1707609 , 0.1707609 ])animalGeneralization([\"horse\", \"cow\", \"mouse\", \"squirrel\"])## array([1.        , 1.        , 0.64983602, 0.64983602, 0.58531333,\n##        0.58531333, 1.        , 1.        , 0.18405475, 0.18405475])animalGeneralization([\"horse\", \"cow\", \"mouse\", \"squirrel\", \"horse\", \"squirrel\"])## array([1.        , 1.        , 0.32551484, 0.32551484, 0.26938358,\n##        0.26938358, 1.        , 1.        , 0.06883892, 0.06883892])animalGeneralization([\"gorilla\"])## array([0.22011594, 0.22011594, 0.22011594, 0.22011594, 0.46663258,\n##        1.        , 0.13668495, 0.13668495, 0.08643809, 0.08643809])animalGeneralization([\"gorilla\", \"gorilla\"])## array([0.06236149, 0.06236149, 0.06236149, 0.06236149, 0.2473422 ,\n##        1.        , 0.03994508, 0.03994508, 0.02971748, 0.02971748])animalGeneralization([\"gorilla\", \"gorilla\", \"gorilla\"])## array([0.01730482, 0.01730482, 0.01730482, 0.01730482, 0.1259028 ,\n##        1.        , 0.01270155, 0.01270155, 0.01111944, 0.01111944])"},{"path":"generalization.html","id":"results","chapter":"3 Generalization","heading":"3.3.4 Results","text":"results paper show model predicts people‚Äôs judgments quite well, better alternative models rely Bayesian inference. results suggest assumptions model similar assumptions people make making inductive generalizations.\nFigure 3.3: Comparison model results human judgments. Image Sanjana & Tenenbaum (2002).\n","code":""},{"path":"categorization.html","id":"categorization","chapter":"4 Categorization","heading":"4 Categorization","text":"last chapter introduced problem inductive generalization. chapter focus specific case generalization particular interest psychologists, cognitive scientists, people use AI machine learning: classification categorization. Basically, assigning labels things.People constantly classify things world categories (chairs, cats, friends, enemies, edible things, ). helps us communicate function novel situations.machine learning, ‚Äôs often useful classify inputs different categories like whether social media post violates community standards , whether image contains human face , whether MRI contains tumor.Psychologists study people form categories provides window organize knowledge. basic problem one studied machine learning.‚Äôm going intrdouce fairly simple psychological model categorization introduced Robert Nosofsky called Generalized Context Model. assumes people make classification judgments using following general algorithm:Remember examples categories ‚Äôve seen .want classify new instance, compare previous examples different categories rate similar one.Assign category highest average similarity.model makes outlandish assumptions (like idea people remember every example seen ). , first approximation, decent job predicting people‚Äôs classification judgments lot situations. learning purposes, advantage pretty easy understand implement.","code":""},{"path":"categorization.html","id":"a-typical-category-learning-experiment","chapter":"4 Categorization","heading":"4.1 A typical category learning experiment üü¢üü®","text":"psychology, category learning experiments pretty similar structure. People see unfamiliar stimuli differ several dimensions can sorted different categories. task learn distinguishes one category another. experiments usually consist two phases: training phase, testing phase.Training phase: People see many examples category classify , often simply guessing first. get feedback gradually learn tell different categories apart.Testing phase: People see examples, usually brand new ones, classify . time don‚Äôt get feedback. point phase test people actually learned meaning categories.stimuli abstract shapes, cartoon insects, race cars different features, anything else. matters clearly distinguishable features.","code":""},{"path":"categorization.html","id":"representing-stimuli-in-a-model","chapter":"4 Categorization","heading":"4.2 Representing stimuli in a model üß©","text":"modeling purposes, types stimuli (discrete-valued features) can represented matrix, following properties:dimension matrix represents different feature stimuli.length dimension represents many different values feature can .single item can represented binary matrix 1s cells indicating feature values 0s everywhere else.example, Robert Nosofsky‚Äôs (1986) test GCM3, stimuli semicircles lines . two feature dimensions (1) circle size (2) line orientation. feature four possible values.Using binary matrix representation, one possible stimulus Nosofsky experiment:Nosofksy stimuli pretty simple: can exactly one value dimension, representation always single 1 matrix.wanted know many stimulus present model, pretty cumbersome maintain long list arrays like one. instead maintain single matrix stores counts stimuli. , values element represent number stimuli observed feature values.example, let‚Äôs consider ‚Äúdimensional‚Äù condition Nosofsky‚Äôs experiment, subjects saw stimuli ‚Äúdiagonals‚Äù matrix. Let‚Äôs assume saw one 100 times (actually, mind-numbing 1200 trials 2600 practice trials ü•¥). represent like :still isn‚Äôt ideal though. doesn‚Äôt represent category labels subjects got training. don‚Äôt know examples belonged categories.Thinking ahead GCM works, let‚Äôs instead represent training examples like list, can iterate :format, row matrix array three elements. first element category label: 1 2. second two elements indices stimulus matrix. Note: ‚Äôve deviated Python norms consistent numbering table original Nosofsky paper, shown . working model, remember indexing Python starts 0.\nFigure 4.1: Training examples dimensional condition. Image Nosofsky (1986).\n","code":"import numpy as np\n\nstimulus = np.array([[0, 0, 0, 0],\n                     [0, 0, 0, 0],\n                     [0, 0, 1, 0],\n                     [0, 0, 0, 0]], dtype=int)\nprint(stimulus)## [[0 0 0 0]\n##  [0 0 0 0]\n##  [0 0 1 0]\n##  [0 0 0 0]]training_set = np.array([[100, 0, 0, 100],\n                         [0, 100, 100, 0],\n                         [0, 100, 100, 0],\n                         [100, 0, 0, 100]], dtype=int)\nprint(training_set)## [[100   0   0 100]\n##  [  0 100 100   0]\n##  [  0 100 100   0]\n##  [100   0   0 100]]tDimensional = np.zeros((8,3), dtype=int)\ntDimensional[0] = [1,1,1]\ntDimensional[1] = [1,2,2]\ntDimensional[2] = [1,3,2]\ntDimensional[3] = [1,4,1]\ntDimensional[4] = [2,1,4]\ntDimensional[5] = [2,2,3]\ntDimensional[6] = [2,3,3]\ntDimensional[7] = [2,4,4]\n\nprint(tDimensional)## [[1 1 1]\n##  [1 2 2]\n##  [1 3 2]\n##  [1 4 1]\n##  [2 1 4]\n##  [2 2 3]\n##  [2 3 3]\n##  [2 4 4]]"},{"path":"categorization.html","id":"the-generalized-context-model-gcm","chapter":"4 Categorization","heading":"4.3 The Generalized Context Model (GCM)","text":", describe special case model applies tasks (like one paper) two categories. However, model can easily extended number categories. model defined two key equations.first equation defines probability classifying Stimulus \\(S_i\\) Category \\(C_1\\) (.e., Category 1):\\[\n\\begin{equation}\nP(C_1|S_i) = \\frac{b_1 \\sum_{j \\C_1} \\eta_{ij}}{b_1 \\sum_{j \\C_1} \\eta_{ij} + (1-b_1) \\sum_{k \\C_2} \\eta_{ik}}\n\\tag{4.1}\n\\end{equation}\n\\]sum \\(\\sum_{j \\C_1}\\) sum stimuli \\(S_j\\) belong Category 1. sum \\(\\sum_{j \\C_1}\\) sum stimuli \\(S_k\\) belong Category 2.Equation (4.1) parameter \\(b_1\\) defined response bias Category 1. \\(b_1\\) can range 0 1 captures possibility subject biased respond one category another. similar identical prior distribution. \\(b_1\\) small, model biased toward Category 2; \\(b_1\\) large, model biased toward Category 1. Note \\(b = 0.5\\), cancels equation.also include response bias Category 2, model assumes \\(\\sum_i b_i = 1\\). Therefore, two categories, \\(b_2 = 1 - b_1\\) see second term denominator Equation (4.1).\\(\\eta_{ij}\\) function defines similar \\(S_i\\) \\(S_j\\) . GCM assumes stimuli can represented points multi-dimensional space (case, two-dimensional space) similarity defined function distance two points:\\[\n\\eta_{ij} = e^{-c^2 \\left[w_1 (f_{i1} - f_{j1})^2 + (1-w_1)(f_{i2} - f_{j2})^2 \\right]}\n\\]\\(f_{i1}\\) \\(f_{i2}\\) refer feature values dimensions 1 2 \\(S_i\\). equation two parameters: \\(c\\) \\(w_1\\). \\(c\\) scaling parameter affects steeply exponential curve . allow us account different people might different ideas close two stimuli must called similar. \\(w_1\\) called attentional weight dimension 1. parameter captures much weight placed dimension 1 dimension 2. Just like \\(b_1\\), \\(w_1\\) can range 0 1, larger , weight placed dimension 1. Similarly, add \\(w_2\\), attention weights constrained sum 1.","code":""},{"path":"categorization.html","id":"generating-model-predictions","chapter":"4 Categorization","heading":"4.3.1 Generating model predictions","text":"order generate predictions model, need two things. First, need define training stimuli , feature values . allow us compute sums Equation (4.1).Next, need specify values 3 parameters \\(b_1\\), \\(c\\), \\(w_1\\). Nosofsky first collecting data people experiment. , every stimulus \\(S_i\\) can record proportion times people classified Category 1. words, can collect empirical estimates \\(P(C_1|S_i)\\) values \\(S_i\\).Nosofsky uses maximum likelihood procedure fitting model data. Conceptually, idea find set values three parameters produce model predictions close possible empirical data. (exactly linear regression works, data \\((\\bar{x},\\bar{y})\\) want find best-fitting values \\(\\) \\(b\\) function \\(y = ax + b\\) describes relationship \\(x\\) \\(y\\).)can define model fit mean squared error (MSE). MSE defined \\[\n\\frac{1}{n} \\sum_i (y_i - x_i)^2\n\\]\\(y\\) data \\(x\\) model predictions. lower MSE means better model fit. Finding best values parameters can achieved exhaustive search values. example, might consider possible values parameter increments 0.1. write program performs following basic algorithm:Set minimum MSE \\(\\inf\\).possible values \\(b_1\\), \\(c\\), \\(w_1\\):\nGenerate model predictions \\(P(R_1|S_i)\\) \\(S_i\\).\nCompute MSE empirical data model predictions generated previous step.\nMSE smaller minimum MSE, set minimum MSE current value store current values \\(b_1\\), \\(c\\), \\(w_1\\).\nGenerate model predictions \\(P(R_1|S_i)\\) \\(S_i\\).Compute MSE empirical data model predictions generated previous step.MSE smaller minimum MSE, set minimum MSE current value store current values \\(b_1\\), \\(c\\), \\(w_1\\).Return stored values \\(b_1\\), \\(c\\), \\(w_1\\).","code":""},{"path":"categorization.html","id":"homework-3","chapter":"4 Categorization","heading":"4.3.2 Homework 3","text":"next homework implement GCM described . homework, encode training test stimuli implement equations model-fitting algorithm . Additionally, get experience behavioral side cognitive science, create working version category learning experiment order collect data can use compare GCM‚Äôs predictions.","code":""},{"path":"hierarchical-generalization.html","id":"hierarchical-generalization","chapter":"5 Hierarchical generalization","heading":"5 Hierarchical generalization","text":"previous examples, always finite number hypotheses making inferences (number black balls, fair trick coin, yellow green taxi). Sometimes, want consider infinite set hypotheses. example, flipping coin, probability coin coming heads? answer question number interval [0,1].","code":""},{"path":"hierarchical-generalization.html","id":"beta-binomial","chapter":"5 Hierarchical generalization","heading":"5.1 The Beta-Binomial model ü™ô","text":"\nFigure 5.1: Photo ZSun Fu Unsplash.\ncan answer question model called Beta-Binomial model, named probability distributions uses. First, let‚Äôs set basic assumptions model.Let \\(P(\\text{heads}) = \\theta\\). don‚Äôt know \\(\\theta\\) . observing sequence coin flips \\(D\\), want estimate \\(\\theta\\). can accomplished directly applying Bayes‚Äôs rule:\\[\nP(\\theta|D) = \\frac{P(D|\\theta) P(\\theta)}{P(D)}\n\\]data \\(D\\) case corresponds number \\(k\\) heads \\(n\\) total flips. follows Binomial distribution, describes probability getting \\(k\\) successes \\(n\\) trials, probability success trial \\(\\theta\\). define heads ‚Äúsuccess.‚Äù\\[\n\\begin{align}\nP(D|\\theta) = P(k|\\theta,n) &= \\text{Bin}(k; n, \\theta) \\\\\n&= \\binom{n}{k} \\theta^{k} (1-\\theta)^{n-k}\n\\end{align}\n\\]notation \\(\\text{Bin}(\\cdot)\\) function indicates distribution \\(k\\) (number successes) distribution parameters \\(n\\) (total number trials) \\(\\theta\\) (probability success trial).can define prior, \\(P(\\theta)\\), however like. \\(\\theta\\) random variable can take value 0 1, just say \\(P(\\theta) = 0.5\\) like earlier examples. Instead, \\(P(\\theta)\\) must probability distribution assigns probabilities value 0 1. know nothing \\(\\theta\\), use Uniform(\\([0,1]\\)) non-informative prior assigns equal probability values \\(\\theta\\).Alternatively, convenient choice (reasons explained ) \\(P(\\theta)\\) Beta distribution:\\[\nP(\\theta) = \\text{Beta}(\\theta;\\alpha,\\beta)\n\\]Beta distribution two parameters: \\(\\alpha > 0\\) \\(\\beta > 0\\). Let‚Äôs create function allow us visualize Beta distribution.plot_beta takes two arguments: (\\(\\alpha\\)), b(\\(\\beta\\)) plots Beta distribution parameter values.Let‚Äôs see looks like different values.\\(\\alpha = \\beta = 1\\), Beta distribution identical Uniform(\\([0,1]\\)) distribution.\\(\\alpha\\) \\(\\beta\\) greater 1 equal, get distribution peak around 0.5. strong prior expectations coin unbiased, increase parameters even :\\(\\alpha\\) \\(\\beta\\) equal?allows us capture skewed priors, perhaps capturing belief coin specific bias.Now, \\(\\alpha\\) \\(\\beta\\) less 1?might capture belief coin strongly biased, aren‚Äôt sure direction.","code":"from scipy import stats\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_beta(a, b):\n  x = np.linspace(0,1,num=500)\n  px = stats.beta.pdf(x, a, b)\n  \n  fig, ax = plt.subplots()\n  ax.plot(x, px)\n  plt.show()plot_beta(1,1)plot_beta(3,3)plot_beta(50,50)plot_beta(4,2)plot_beta(0.5,0.5)"},{"path":"hierarchical-generalization.html","id":"conjugate-distributions","chapter":"5 Hierarchical generalization","heading":"5.1.1 Conjugate distributions","text":"Beta distribution conjugate distribution Binomial distribution. means likelihood Binomial distribution prior Beta distribution, posterior also Beta distribution. Specifically, making assumptions,\\[\nP(\\theta|D) = \\text{Beta}(\\theta; \\alpha + k, \\beta + n-k)\n\\]parameters posterior distribution (1) sum \\(\\alpha\\) prior number observed heads (2) sum \\(\\beta\\) prior number observed tails. means parameters \\(\\alpha\\) \\(\\beta\\) Beta prior natural interpretation ‚Äúvirtual flips.‚Äù example, larger \\(\\alpha\\) compared \\(\\beta\\), biased toward heads expect \\(\\theta\\) . Additionally, larger \\(\\alpha\\) \\(\\beta\\) , certain (less diffuse) prior .","code":""},{"path":"hierarchical-generalization.html","id":"parameter-estimation","chapter":"5 Hierarchical generalization","heading":"5.1.2 Parameter estimation","text":"used conjugate distribution, can use plot_beta function generate posterior probability distributions coin flips.Suppose start fairly strong belief coin fair, represented distribution:Now, suppose flip coin 20 times comes heads every time. think bias coin now? According model:can see, cause shift beliefs somewhat.wasn‚Äôt totally realistic, though. picked coin ground, prior beliefs biased probably look like :happens now flipped coin 20 times came heads every time?might mildly surprised, 20 flips wouldn‚Äôt enough budge estimate bias coin much.Finally, let‚Äôs imagine situation weak prior belief coin biased:Now flip coin 100 times comes heads 48 times. updated beliefs ?can see, posterior distribution shows think coin probably fair now. illustrates sufficient evidence can override prior beliefs.","code":"plot_beta(30,30)plot_beta(30+20,30)plot_beta(2000,2000)plot_beta(2000+20,2000)plot_beta(5,1)plot_beta(5+48,1+52)"},{"path":"hierarchical-generalization.html","id":"hypothesis-averaging","chapter":"5 Hierarchical generalization","heading":"5.1.3 Hypothesis averaging","text":"Chapter 3, solved generalization problem summing hypotheses, weighted posterior probabilities. , can something similar.Suppose want know probability next flip coming heads. words, want know \\(P(\\text{heads}|D)\\). can averaging possible values \\(\\theta\\):\\[\nP(\\text{heads}|D) = \\int_\\theta P(\\text{heads}|\\theta) \\cdot P(\\theta|D) d\\theta = \\int_\\theta \\theta \\cdot P(\\theta|D) d\\theta\n\\]","code":""},{"path":"hierarchical-generalization.html","id":"overhypotheses","chapter":"5 Hierarchical generalization","heading":"5.2 Overhypotheses üôÜ","text":"Now consider slightly different situation. flip 19 different coins row, one time, come heads. Now pick 20th coin bag previous 19 coins. think probability 20th coin coming heads? higher 0.5?answered yes, ‚Äôs probably formed overhypothesis bias coins. flipping coins, may concluded particular set coins likely usual biased. result, estimate probability 20th coin coming heads higher otherwise .","code":""},{"path":"hierarchical-generalization.html","id":"the-shape-bias","chapter":"5 Hierarchical generalization","heading":"5.2.1 The shape bias","text":"coins example pretty artificial, notion overhypotheses one find language learning. phenomenon known shape bias refers fact even young children likely generalize new word based shape rather properties like color texture.\nFigure 5.2: common task used test shape bias.\nmakes sense objects tend common shapes less likely common colors textures.","code":""},{"path":"hierarchical-generalization.html","id":"modeling-the-learning-of-overhypotheses-through-hierarchical-bayesian-learning","chapter":"5 Hierarchical generalization","heading":"5.2.2 Modeling the learning of overhypotheses through hierarchical Bayesian learning","text":"Charles Kemp, Andy Perfors, Josh Tenenbaum developed model kind learning. focused bags black white marbles rather flipping coins. imagine problem many bags marbles draw . drawing many bags, draw single marble new bag make prediction proportion black white marbles bag.details model outside scope book. basic idea model learns two levels simultaneously. higher level, model learns parameters \\(\\alpha\\) \\(\\beta\\) Beta distribution characterizes proportion black white marbles bag. saw , Beta distribution can peak around particular proportion, can peaked around 0 1, meaning bag likely nearly black white.lower level, model learns specific distribution marbles within bag. draw 20 marbles 5 black, may uncertainty overall proportion bag, best estimate around 5/20 1/4.model excels able draw inferences across bags. see many bags full black white marbles, draw single black marble new bag, likely confident rest marbles bag black.see many bags mixed proportions black white marbles, draw single black marble new bag, far less confident proportion black marbles bag. model doesn‚Äôt make inferences multiple levels struggle draw distinction.","code":""},{"path":"sampling-assumptions.html","id":"sampling-assumptions","chapter":"6 Sampling assumptions","heading":"6 Sampling assumptions","text":"far, ‚Äôve seen strong sampling logical assumption make. isn‚Äôt always.example, consider hormone problem Chapter 3. randomly testing people‚Äôs hormone levels every person just happened healthy, clearly strong sampling wouldn‚Äôt appropriate, sampling full population healthy unhealthy people.raises question: people sensitive process data generated?Spoiler alert: Yes.","code":""},{"path":"sampling-assumptions.html","id":"word-learning","chapter":"6 Sampling assumptions","heading":"6.1 Word learning üí¨","text":"Suppose see following collection objects table.\nFigure 6.1: Xu & Tenenbaum (2007).\nNow consider following situation:teacher picks three blue circles pile calls ‚Äúwugs.‚Äùwug? circles wugs? just blue circles?Consider different situation:teacher picks one blue circle pile calls ‚Äúwug.‚Äù teacher asks child choose two (child picks two blue circles). teacher confirms also wugs.wug? circles wugs? just blue circles?situations, observed data : three blue circles labeled wugs. ‚Äôs different process data generated. one case, knowledgeable teacher picked wugs; , teacher picked one example (probably knowledgeable) child picked others.use terms ‚Äôve seen , teacher using strong sampling. child using something closer weak sampling.\nFigure 6.2: hierarchy objects. Xu & Tenenbaum (2007).\nbased actual study Fei Xu Josh Tenenbaum, also developed model task. asked whether children adults generalize word ‚Äúwug‚Äù basic-level category (circles lines ) subordinate category (blue circles lines ). results:\nFigure 6.3: Experimental results Xu & Tenenbaum (2007).\nPeople clearly distinguished teacher-driven situation child-driven (learner-driven) situation, much likely generalize ‚Äúwugs‚Äù subordinate category teacher picked objects.","code":""},{"path":"sampling-assumptions.html","id":"pedagogical-sampling","chapter":"6 Sampling assumptions","heading":"6.2 Pedagogical sampling üßë‚Äçüè´","text":"teacher choosing wugs said using pedagogical sampling. teacher deliberately used knowledge wug concept select informative examples help child learn concept quickly possible.idea explored Patrick Shafto, Noah Goodman, Thomas Griffiths (well Elizabeth Bonawitz researchers related work). put formal terms:\\[\n\\begin{equation}\nP_{\\text{teacher}}(d|h) \\propto (P_{\\text{learner}}(h|d))^\\alpha\n\\tag{6.1}\n\\end{equation}\n\\], \\(\\alpha\\) parameter controls optimized teacher‚Äôs choices . \\(\\alpha \\rightarrow \\infty\\), teacher choose examples \\(d\\) maximize learner‚Äôs posterior probability.means can figure learner update beliefs concept directly applying Bayes rule:\\[\n\\begin{equation}\n  P_{\\text{learner}}(h|d) = \\frac{P_{\\text{teacher}}(d|h)P(h)}{\\sum_{h_i} P_{\\text{teacher}}(d|h_i)P(h_i)}\n\\tag{6.2}\n\\end{equation}\n\\]equation makes clear teacher learner inextricably linked: teacher chooses examples maximize learner‚Äôs understanding, learner updates beliefs based expectations teacher choosing examples. model recursive.researchers explain paper, two equations system equations can rearranged create following equation defining teacher choose examples:\\[\n\\begin{equation}\n  P_{\\text{teacher}}(d|h) \\propto \\left( \\frac{P_{\\text{teacher}}(d|h) P(h)}{\\sum_{h_i} P_{\\text{teacher}}(d|h_i) P(h_i)} \\right)^\\alpha\n\\tag{6.3}\n\\end{equation}\n\\]solve equation? paper, use iterative algorithm, works like :Initialize \\(P_{\\text{teacher}}(d|h)\\) using weak sampling.Iterate following steps \\(P_{\\text{teacher}}(d|h)\\) stabilizes (doesn‚Äôt change one iteration next):\npossible \\(h\\) \\(d\\), compute \\(P_{\\text{learner}}(h|d)\\) using Equation (6.2) \\(P_{\\text{teacher}}(d|h)\\) values previous iteration.\npossible \\(d\\) \\(h\\), update \\(P_{\\text{teacher}}(d|h)\\) using \\(P_{\\text{learner}}(h|d)\\) values previous step, \\(P_{\\text{teacher}}(d|h_i) = P_{\\text{learner}}(h_i|d) / \\sum_{d_j} P_{\\text{learner}}(h_i|d_j)\\) (Equation (6.1)).\npossible \\(h\\) \\(d\\), compute \\(P_{\\text{learner}}(h|d)\\) using Equation (6.2) \\(P_{\\text{teacher}}(d|h)\\) values previous iteration.possible \\(d\\) \\(h\\), update \\(P_{\\text{teacher}}(d|h)\\) using \\(P_{\\text{learner}}(h|d)\\) values previous step, \\(P_{\\text{teacher}}(d|h_i) = P_{\\text{learner}}(h_i|d) / \\sum_{d_j} P_{\\text{learner}}(h_i|d_j)\\) (Equation (6.1)).","code":""},{"path":"sampling-assumptions.html","id":"the-rectangle-game","chapter":"6 Sampling assumptions","heading":"6.2.1 The rectangle game","text":"researchers tested model experiment using simple task called rectangle game.game, concepts rectangular boundaries two-dimensional space. teacher knows boundary learner figure examples given teacher.\nFigure 6.4: rectangle game. Shafto et al.¬†(2014).\none version, teacher can provide positive examples (‚Äú‚Äôs example ‚Äôs inside boundary‚Äù). another version, teacher can also provide negative examples (‚Äú‚Äôs example ‚Äôs inside boundary‚Äù).experiment, asked people play role learner rectangle game. three conditions:Teaching-pedagogical learning: People first acted teachers, learners.Pedagogical learning: People acted just learners told examples saw chosen teacher.Non-pedagogical learning: People acted just learners told examples saw chosen teacher.showing people examples, asked draw rectangle best guess boundary .One key prediction model informative locations examples pedagogical sampling corners rectangles. test whether learners understood , measured proportion examples ended corners rectangles people drew. results shown .\nFigure 6.5: Results Shafto et al.‚Äôs (2014) Experiment 1.\ncan see, people thought knowledgeable teacher providing examples (teaching-pedagogical pedagogical learning conditions), much likely draw tight boundaries around examples, compared didn‚Äôt think teacher providing examples (non-pedagogical learning condition). suggests people indeed sensitive process data generated incorporated understanding inferences.next chapter, ‚Äôll work one application people‚Äôs sensitivity different sampling assumptions: understanding pragmatics speech.","code":""},{"path":"pragmatics.html","id":"pragmatics","chapter":"7 Language pragmatics","heading":"7 Language pragmatics","text":"talk , often rely shared assumptions context get points across quickly. Linguists call pragmatics. example, consider following exchange:: ‚Äôm almost gas.B: ‚Äôs gas station ahead.Person didn‚Äôt explicitly ask question, Person B understood wouldn‚Äôt said anything didn‚Äôt need something weren‚Äôt explaining something , B responded answer implied question (‚Äúcan get gas?‚Äù).chapter, ‚Äôll see aspects pragmatics can explained combining Bayesian inference two new ideas: information theory decision theory (expected utility).","code":""},{"path":"pragmatics.html","id":"surprisal","chapter":"7 Language pragmatics","heading":"7.1 Surprisal üòØ","text":"‚Äôll focus one concept information theory: surprisal. Surprisal captures unexpected observation . Intuitively, unexpectedness observation depend probable . Namely, probable , less unexpected .want function \\(f(p)\\) takes probability gives us unexpectedness. reasonable constraints \\(f\\):observation probability 0, infinitely unexpected: \\(f(0) = \\infty\\).observation probability 1, unexpected : \\(f(1) = 0\\).two independent observations probabilities \\(p_1\\) \\(p_2\\), probability occurring \\(p_1 \\cdot p_2\\), want total unexpectedness observing sum unexpectedness observation: \\(f(p_1 \\cdot p_2) = f(p_1) + f(p_2)\\).function satisfies constraints :\\[\nf(p) = -log(p)\n\\]\nLet‚Äôs verify.\\(-log(p) \\rightarrow \\infty\\) \\(p \\rightarrow 0\\) ‚úÖ\\(-log(1) = 0\\) ‚úÖ\\(-log(p_1 \\cdot p_2) = -(log(p_1) + log(p_2)) = (-log(p_1))+(-log(p_2))\\) ‚úÖ‚Äôs plot function:surprisal language?‚Äôre communicating someone, surprisal can help decide information give person. example, imagine observe number 1 10 want share someone else. either say:saw even number.say:saw two.say? second one, duh. ?formal terms, ‚Äôre trying send signal number observed surprising person receiving signal. , ‚Äôre trying minimize surprisal \\(P(2|\\text{signal})\\).Exercise: Compute surprisal two signals.","code":"import numpy as np\nimport matplotlib.pyplot as plt\n\np = np.linspace(0.001,1,num=500)\nfp = -np.log(p)\n  \nfig, ax = plt.subplots()\nax.plot(p, fp)\nax.set_xlabel(\"p\")\nax.set_ylabel(\"surprisal\")\nplt.show()"},{"path":"pragmatics.html","id":"rsa-model","chapter":"7 Language pragmatics","heading":"7.2 The Rational Speech Act model üç™","text":"Let‚Äôs apply concept surprisal following example. initially two cookies cookie jar, may eaten , ‚Äôre telling friend many . can say:‚Äúcookies.‚Äù (ate one two.)‚Äúcookies.‚Äù (ate two.)‚Äúcookies.‚Äù (ate none.)Nothing ‚Äì stay silent. (eaten none number cookies.)Let‚Äôs define situation using binary matrix. row signal (say) column state (many cookies eaten).‚Äôll define function prints matrix heat map.Now let‚Äôs calculate literal listener interpret statements. row matrix (one signal) define distribution states. , state \\(h\\) signal \\(d\\), want compute:\\[\nP_{\\text{listener}}(h|d) \\propto P(d|h) P(h)\n\\]literal listener assumes , given state, signal consistent state chosen random speaker. already captured matrix : Signals consistent state value 1 cells signals inconsistent 0. also assume listener uniform prior states. Therefore, can calculate literal listener‚Äôs probabilities just normalizing rows matrix.‚Äôll , let‚Äôs write function .Now let‚Äôs calculate pragmatic speaker. pragmatic speaker chooses signal given state. Therefore, time, columns matrix probability distributions.Unlike literal listener, pragmatic speaker trying convey information, choose signal greatest expected utility literal listener. Expected utility case defined expected increase understanding true state world. Remember, already way quantifying : surprisal. ‚Äôll define utility negative surprisal state given signal point view literal listener: \\(-(-log(P_{\\text{listener}}(h|d)))\\).code, used something known ‚Äúsoftmax‚Äù rather true maximizing function. exponential function captures idea speaker choose probabilistically, generally choosing options utility. fairly common assumption psychology economics often don‚Äôt complete information making decisions (modeling people‚Äôs decisions), assuming pure maximizing function isn‚Äôt always best choice.However, also included parameter alpha controls close maximizing speaker . larger alpha , closer speaker get maximizing utility.Now can calculate probabilities pragmatic listener. similar literal listener pragmatic listener receives signal computes probability state given signal. difference pragmatic listener doesn‚Äôt assume signals chosen random, signals chosen pragmatic speaker .can compute pragmatic listener normalizing matrix along rows.pragmatic listener correctly infers scalar implicature, concept pragmatics linguistics. Even though ‚Äúcookies‚Äù consistent eating cookies, pragmatic listener infer person says ate eat (otherwise said ).","code":"state_matrix = np.array([[0,1,1],\n                         [0,0,1],\n                         [1,0,0],\n                         [1,1,1]])\n                         \nsignals = [\"some\", \"all\", \"no\", \"silent\"]\nstates = [\"0\",\"1\",\"2\"]def print_state_matrix(m, x_labels=None, y_labels=None):\n  fig, ax = plt.subplots()\n  im = ax.imshow(m)\n  \n  if (x_labels and y_labels):\n    ax.set_xticks(np.arange(len(x_labels)), labels=x_labels)\n    ax.set_yticks(np.arange(len(y_labels)), labels=y_labels)\n  \n    for i in range(len(y_labels)):\n      for j in range(len(x_labels)):\n        text = ax.text(j, i, round(m[i, j],2),\n          ha=\"center\", va=\"center\", color=\"w\")\n  \n  fig.tight_layout()\n  plt.show()\n\n# Print out the matrix\nprint_state_matrix(state_matrix, states, signals)def normalize_rows(m):\n  normalized_m = np.zeros((m.shape))\n  row_num = 0\n  for row in m:\n    normalized_m[row_num,:] = row / sum(row)\n    row_num += 1\n    \n  return(normalized_m)literal_listener = normalize_rows(state_matrix)\nprint_state_matrix(literal_listener, states, signals)# First we apply a softmax decision function\nalpha = 1 # paramater controls how close to maximizing speaker is\npragmatic_speaker = np.exp(np.log(literal_listener)*alpha)## <string>:1: RuntimeWarning: divide by zero encountered in logfor col in range(pragmatic_speaker.shape[1]):\n  pragmatic_speaker[:,col] = (pragmatic_speaker[:,col] /\n    sum(pragmatic_speaker[:,col]))\n\nprint_state_matrix(pragmatic_speaker, states, signals)pragmatic_listener = normalize_rows(pragmatic_speaker)\nprint_state_matrix(pragmatic_listener, states, signals)"},{"path":"pragmatics.html","id":"does-this-model-match-human-behavior","chapter":"7 Language pragmatics","heading":"7.3 Does this model match human behavior? üîµ","text":"word, yes. One first papers look Michael Frank Noah Goodman. applied model simple task picking shape small set tested model‚Äôs predictions behavioral experiment.","code":""},{"path":"pragmatics.html","id":"the-task","chapter":"7 Language pragmatics","heading":"7.3.1 The task","text":"speaker: ‚ÄúImagine talking someone want refer middle object. word use: blue circle?‚Äù üü¶ üîµ üü©listener: ‚ÄúImagine someone talking uses word blue refer one objects. object talking ?‚Äù üü¶ üîµ üü©task experiment, researchers varied actual set objects systematic way.","code":""},{"path":"pragmatics.html","id":"the-model-1","chapter":"7 Language pragmatics","heading":"7.3.2 The model","text":"model virtually identical cookie jar model just worked . assume speakers try choose words maximize listener‚Äôs utility, measured surprisal.literal listener case assume object consistent speaker‚Äôs chosen word one referring .assumptions, researchers derive probability speaker choosing word (details paper) :\\[\nP(w|r_s,C) = \\frac{|w|^{-1}}{\\sum_{w^\\prime \\W} {|w^\\prime|}^{-1}}\n\\]\\(w\\): speaker‚Äôs chosen word.\\(r_s\\): object speaker meant refer .\\(C\\): set objects.\\(|w|\\): number objects \\(w\\) apply .\\(W\\): set words apply object speaker meant refer .Regarding \\(W\\), imagine speaker wanted refer blue circle. case, \\(W = \\{ \\text{blue}, \\text{circle} \\}\\), either word apply object.According model, speakers tend choose words uniquely identify object set. example, set , blue applies two objects (blue square blue circle), circle applies one, circle get higher probability.","code":""},{"path":"pragmatics.html","id":"homework-4-implement-the-model","chapter":"7 Language pragmatics","heading":"7.3.3 Homework 4: Implement the model","text":"haven‚Äôt provided details model assignment finish implementation , run simulations, collect small amount real data compare model .","code":""},{"path":"social-cognition.html","id":"social-cognition","chapter":"8 Social cognition","heading":"8 Social cognition","text":"Watch video . see?‚Äôre like people, see just lifeless shapes moving around. see whole drama play involving characters goals emotions.animation, 1944 study Heider Simmel, excellent example capacity social cognition: thinking people. constantly attributing goals, beliefs, desires, feelings others. fact willing simple shapes suggests may able help .someone takes action, makes decision, makes facial expression, infer ‚Äôre thinking feeling? chapter, ‚Äôll look two computational approaches answering question.","code":""},{"path":"social-cognition.html","id":"inverse-decision-making","chapter":"8 Social cognition","heading":"8.1 Inverse decision-making üçë","text":"social cognition problems basic character: observe something people, want infer underlying cause thing observed. example, see someone take action, want infer goal motivated action.basic idea behind many approaches understanding people reason people kind mental model people act essentially run model backward (invert) model infer information don‚Äôt get see.Consider simple decision-making situation. Rue likes peaches oranges oranges apples:üçë > üçä > üçéNow give Rue choice three fruits. one likely choose?Probably peach right? mean, might choose orange apple happens mood one , given information , peach good guess.Now imagine don‚Äôt know Rue‚Äôs fruit preferences. ‚Äôs offered following fruits pick :üçë üçä üçéchooses peach üçë. guess relative preferences fruits, think?least, might guess likes peaches oranges apples. ?assume people‚Äôs choices guided preferences people‚Äôs actions basically rational. seeing Rue make one choice set options gives information likes.Can formalize intuition?4","code":""},{"path":"social-cognition.html","id":"a-preference-learning-model","chapter":"8 Social cognition","heading":"8.1.1 A preference learning model","text":"Let‚Äôs start assumptions:fruit provides utility person takes (recall notion utility introduced Chapter 7).Utilities additive, meaning two peaches provide twice much utility one peach.People choose options proportion utility. ‚Äúsoftmax‚Äù assumption used rational speech act model.assumptions combine give us choice model, model people make choices different options. (‚Äôve limited focus fruits, model applied anything.)\\[\n\\begin{equation}\np(c = o_j|\\mathbf{u}, \\mathbf{}) = \\frac{\\exp(U_j)}{\\sum^n_{k=1}\\exp(U_k)}\n\\tag{8.1}\n\\end{equation}\n\\]model, \\(c\\) choice, \\(o_j\\) refers option \\(j\\), \\(\\mathbf{}\\) vector specifying option attributes (features), \\(\\mathbf{u}\\) vector utilities decision-maker assigns attribute, \\(U_j\\) summed total utility option.","code":""},{"path":"social-cognition.html","id":"inverting-the-choice-model","chapter":"8 Social cognition","heading":"8.1.1.1 Inverting the model","text":"Remember goal infer people‚Äôs preferences, predict choices. , see someone make choice fruits, infer fruits like best? can invert Equation (8.1) using Bayes‚Äôs rule:\\[\n\\begin{equation}\np(\\mathbf{u}|c,\\mathbf{}) = \\frac{p(c|\\mathbf{u},\\mathbf{})p(\\mathbf{u})}{p(c|\\mathbf{})}\n\\tag{8.2}\n\\end{equation}\n\\]Suppose Rue choice following options:Option 1: üçë üçé üçäOption 2: üçåJules choice following options:Option 1: üçëOption 2: üçé üçä üçåRue Jules pick Option 1, includes peach üçë. Based choices alone, think likes peaches ? (, least, person‚Äôs choice make confident like peaches?)Let‚Äôs apply inverse decision-making model two choices.‚Äôll start encoding choices using binary encoding scheme element list represents fruit element set 1 fruit present option.Let‚Äôs define function calculates choice probability Equation (8.1). likelihood function.Let‚Äôs first assume like fruits equally (get utility 1 fruit). probability Rue Jules making choices?Unsurprisingly, Rue‚Äôs choice much probable. like peaches four times much fruits?increases Rue‚Äôs choice probability just 10%, increases Jules‚Äôs choice probability 60%.invert model, also need specify prior probability, \\(p(\\mathbf{u})\\). ‚Äôll assume people generally like fruit, likely disagree much like different fruits, ‚Äôs possibility people dislike like certain fruits.can capture idea assuming utilities fruits normally distributed positive mean. Let‚Äôs say mean 2 standard deviation 0.5:Additionally, ‚Äôll make simplifying assumption utilities different fruits independent. means knowing someone likes pineapples doesn‚Äôt tell anything much ‚Äôll like grapefruits.Finally, need way compute \\(p(c|\\mathbf{})\\). words, seeing Rue make choice, probability anyone preferences making choice?Previously, got around computing denominators like normalizing. works can enumerate full hypothesis space. time, space hypotheses (possible assignments utilities fruits) continuous ‚Äôs easy integrate . need something else.","code":"import numpy as np\n\n# Fruit order:\n# 1. peach\n# 2. apple\n# 3. orange\n# 4. banana\nchoice_rue = np.array([[1,1,1,0],\n                       [0,0,0,1]])\nchoice_jules = np.array([[1,0,0,0],\n                         [0,1,1,1]])def compute_choice_prob(j, u, options): \n  '''Returns the probability of choosing option j\n     from the set options.\n     \n     Parameters:\n       j (int): the chosen option\n       u (array): a vector of utilities assigned to the\n         attributes in the options\n       options (array): a matrix in which each row is an\n         option, and each option is a binary array of the \n         attributes in that option\n         \n     Returns:\n       p (float): probability of choosing option j\n  '''\n  \n  # Compute the total utility of each option\n  total_u = np.sum(u*options, axis=1)\n  \n  p = np.exp(total_u[j]) / np.sum(np.exp(total_u))\n  return(p)utilities = np.array([1,1,1,1])\n\nprint(\"Rue's choice probability:\" + \n  str(compute_choice_prob(0, utilities, choice_rue)))## Rue's choice probability:0.8807970779778824print(\"Jules's choice probability:\" + \n  str(compute_choice_prob(0, utilities, choice_jules)))## Jules's choice probability:0.11920292202211755utilities = np.array([4,1,1,1])\n\nprint(\"Rue's choice probability:\" + \n  str(compute_choice_prob(0, utilities, choice_rue)))## Rue's choice probability:0.9933071490757152print(\"Jules's choice probability:\" + \n  str(compute_choice_prob(0, utilities, choice_jules)))## Jules's choice probability:0.7310585786300048from scipy.stats import norm\nimport matplotlib.pyplot as plt\n\nu_mean = 2\nu_sd = 0.5\n\nu = np.linspace(-1,5,1000)\npu = norm.pdf(u,u_mean,u_sd)\n\nfig, ax = plt.subplots(1,1)\nax.plot(u, pu)\nax.set_xlabel(\"u\")\nax.set_title(\"Prior probability of u\")\n\nplt.show()"},{"path":"social-cognition.html","id":"monte-carlo","chapter":"8 Social cognition","heading":"8.1.1.2 Monte Carlo estimation","text":"‚Äôll use estimation method known Monte Carlo relies random sampling.‚Äôs basic idea applies problem. want know overall probability someone general population making certain choice. don‚Äôt know , know distribution utilities general population. ‚Äôs prior \\(p(\\mathbf{u})\\) just defined. can get close approximation want using following algorithm:Draw random sample \\(\\mathbf{u}\\) \\(p(\\mathbf{u})\\).Compute \\(p(c|\\mathbf{u}, \\mathbf{})\\).Repeat many times.Calculate mean choice probability trials.Let‚Äôs write function .Let‚Äôs compare probability someone making choices Rue Jules made.Unsurprisingly, Jules‚Äôs choice much higher baseline probability.","code":"def estimate_marginal_likelihood(j, options, n_samples): \n  '''Returns an estimate of the probability of choosing option j\n     from the set options using Monte Carlo estimation.\n     \n     Parameters:\n       j (int): the chosen option\n       options (array): a matrix in which each row is an\n         option, and each option is a binary array of the \n         attributes in that option\n       n_samples (int): number of Monte Carlo samples to\n         collect\n         \n     Returns:\n       (float): probability of choosing option j\n  '''\n  \n  n_attributes = len(options[0])\n  mc_samples = np.zeros(n_samples)\n  \n  for i in range(n_samples):\n    # draw random u sample\n    u = np.random.normal(u_mean,u_sd,n_attributes)\n    mc_samples[i] = compute_choice_prob(j, u, options)\n  \n  return(np.mean(mc_samples))print(\"Rue's choice probability: \" + \n  str(estimate_marginal_likelihood(0, choice_rue, 2000)))## Rue's choice probability: 0.9714736193915378print(\"Jules's choice probability: \" +\n  str(estimate_marginal_likelihood(0, choice_jules, 2000)))## Jules's choice probability: 0.029788364556330958"},{"path":"social-cognition.html","id":"putting-it-all-together","chapter":"8 Social cognition","heading":"8.1.1.3 Putting it all together","text":"Now ‚Äôll infer Rue‚Äôs Jules‚Äôs preferences choices using Monte Carlo estimation .Let‚Äôs look results.model‚Äôs inferences require interpretation. model certain Rue preference peach, distribution Jules‚Äôs preference peach much longer tail.makes sense. mostly positive prior distribution, ‚Äôs reasonable assume default Rue likes peaches.Jules‚Äôs choice bit odd, ‚Äôs harder make sense . One way explain simply made mistake (, saw , ‚Äôs improbable choice). Another way explain assuming strong preference peaches. (course, model doesn‚Äôt take account countless reasons Jules might prefer one fruit three.)","code":"# Set random state for reproducibility\nnp.random.RandomState(2022) ## RandomState(MT19937) at 0x134F4DE40n_samples = 2000 # number of Monte Carlo samples\nn_attributes = len(choice_rue[0])\npeach = 0\nbanana = 3\n\nmarginal_likelihood_rue = estimate_marginal_likelihood(0, choice_rue, n_samples)\nmarginal_likelihood_jules = estimate_marginal_likelihood(0, choice_jules, n_samples)\n\nmc_samples_rue = np.zeros(n_samples)\nmc_samples_jules = np.zeros(n_samples)\n\nfor i in range(n_samples):\n  # Draw u samples for Rue and Jules\n  u_rue = np.random.normal(u_mean,u_sd,n_attributes)\n  u_jules = np.random.normal(u_mean,u_sd,n_attributes)\n  \n  # Compute posterior for peach utility for each person\n  pu_rue = ((compute_choice_prob(0, u_rue, choice_rue) * u_rue) / \n    marginal_likelihood_rue)\n  pu_jules = ((compute_choice_prob(0, u_jules, choice_jules) * u_jules) /\n    marginal_likelihood_jules)\n\n  mc_samples_rue[i] = pu_rue[peach]\n  mc_samples_jules[i] = pu_jules[peach]import pandas as pd\nimport seaborn as sb\n\nsamples = pd.DataFrame(data = {'Rue': mc_samples_rue,\n                               'Jules': mc_samples_jules})\n# reorganize the data\nsamples = samples.melt(value_vars=[\"Jules\",\"Rue\"], \n  var_name=\"person\", value_name=\"u\")\n# filter out extreme samples\nsamples_filtered = samples[samples[\"u\"] < 50]\n\nfig, ax = plt.subplots(1,1)\ng = sb.displot(data=samples_filtered, x=\"u\", hue=\"person\", kind=\"kde\")\n\nplt.show()"},{"path":"social-cognition.html","id":"homework-5-your-turn","chapter":"8 Social cognition","heading":"8.1.2 Homework 5: Your turn","text":"next assignment, ‚Äôll fully implement version model compare data collected (along Chris Lucas Charles Kemp).\nFigure 8.1: Cards used preference learning task Jern, Lucas, & Kemp (2017).\n","code":""},{"path":"social-cognition.html","id":"na√Øve-utility-calculus","chapter":"8 Social cognition","heading":"8.2 Na√Øve utility calculus üßÆ","text":"Suppose Rue Jules make choice two fruits:Option 1: üçëOption 2: üçåchoose peach üçë. difference , Rue, fruits placed bowl front . Jules, peach bowl front , banana high shelf another room.cases, ‚Äôd probably assume like peaches, might conclude ‚Äôs weaker evidence Jules likes . may chosen peach just convenient.(pretty contrived) example shows people‚Äôs choices function rewards (preferences) costs. ‚Äôre thinking people‚Äôs behavior, take potential rewards costs account understand ‚Äôre things.idea best articulated formalized Julian Jara-Ettinger collaborators. incorporated computational model.Specifically, suppose person plan \\(p\\) achieve outcome \\(o\\). can define utility \\(U\\) :\\[\nU(o,p) = R(o) - C(p)\n\\]\\(R(\\cdot)\\) reward outcome \\(C(\\cdot)\\) cost incur carrying plan.","code":""},{"path":"social-cognition.html","id":"markov-decision-processes","chapter":"8 Social cognition","heading":"8.2.1 Markov decision processes","text":"Earlier, considered single decisions happened isolation. lots behavior involves series small decisions happen sequence: order someone prepares cooks dish, route someone takes get workplace home, sections book chapter write first.series decisions plan \\(p\\). useful computational framework choosing optimal plans Markov decision process.concrete, let‚Äôs focus specific environment Jara-Ettinger et al (2020): 7x7 grid agents can move direction, one step time.\nFigure 8.2: Example grid agent path Jara-Ettinger et al (2020).\nlocations grid states \\(S\\). Whenever agent state \\(s\\), can take action \\(\\\\), moving adjacent state. agent goal state mind (figure , ‚Äôs green square). want optimal policy takes state returns optimal action getting toward goal state. can computed follows.First, compute state‚Äôs optimal value \\(V^*(s)\\), using recursive equation:\\[\nV^*(s) = \\text{max}_a\\gamma \\sum_{s^\\prime} P_{s,}(s^\\prime) V^*(s^\\prime) + R(,s) - C(,s)\n\\]\\(P_{s,}(s^\\prime)\\) ‚Äúprobability agent state \\(s^\\prime\\) takes action \\(\\) state \\(s\\) \\(\\gamma \\{0,1}\\).‚ÄùEssentially, equation requires going every state grid, summing rewards costs, plus expected downstream rewards costs state.Solving recursive function outside scope book, standard methods even Python packages solving MDPs.action policy defined :\\[\n\\begin{equation}\np(|s) \\propto \\exp \\left( \\sum_{s^\\prime} P_{s,}(s^\\prime) V^*(s^\\prime) \\right)\n\\tag{8.3}\n\\end{equation}\n\\]\nmodel, introduce notion ‚Äúsoftmax‚Äù choice rule, rather assume agents perfectly optimal. makes sense goal model people reason others, people often incomplete information people.","code":""},{"path":"social-cognition.html","id":"inverting-the-mdp","chapter":"8 Social cognition","heading":"8.2.2 Inverting the MDP","text":"seeing agent take series actions \\(\\mathbf{}\\), can infer costs rewards pretty much way , applying Bayes‚Äôs rule.\\[\np(C,R|\\mathbf{}) \\propto p(\\mathbf{}|C,R) p(C,R)\n\\]model, use uniform priors costs rewards. Computing \\(p(\\mathbf{}|C,R)\\) means just applying Equation (8.3) repeatedly action agent took sequence. (researchers also modeled notion sub-goals ‚Äôve omitted.)","code":""},{"path":"social-cognition.html","id":"results-1","chapter":"8 Social cognition","heading":"8.2.3 Results","text":"researchers tested model series experiments people saw paths agents took across grid rate costs rewards agent assigned different states.Results one experiment .\nFigure 8.3: Partial results Experiment 1b Jara-Ettinger et al (2020).\ncost reward ratings separately normalized mean zero. combined plots.Compared simpler alternative models, full na√Øve utility calculus model provided best fit people‚Äôs judgments.","code":""},{"path":"iterated-learning.html","id":"iterated-learning","chapter":"9 Iterated learning","heading":"9 Iterated learning","text":"someone told defecated water drinking , probably wouldn‚Äôt impressed. Unless lived 1600s, ‚Äúdefecate‚Äù meant ‚Äúpurify something‚Äù.Language changes. Words change. Concepts change. way predict conceptual drift always happening culture?Yeah, maybe, kinda! first, math.","code":""},{"path":"iterated-learning.html","id":"markov-chains","chapter":"9 Iterated learning","heading":"9.1 Markov chains üîó","text":"Words concepts change time. Markov chains provide useful modeling framework time-dependent data.name suggests, Markov chain chain, specifically states. time step, moves new state. Markov chain meets following conditions:system can finite number states.state step chain depends previous state. (dependence can probabilistic.)example, consider game Telephone, one person whispers word next person, whisper hear next person, . set possible words states (finite admittedly large).word Person \\(n+1\\) hears clearly depends word Person \\(n\\) whispered. might miscommunication: probability Person \\(n+1\\) accurately hears word probably less 1, similar-sounding words getting higher probability dissimilar words.","code":""},{"path":"iterated-learning.html","id":"examples","chapter":"9 Iterated learning","heading":"9.1.1 Examples","text":"Consider simple chain colors either red blue. , state space \\(S = \\{ \\text{red, blue} \\}\\). specify Markov chain, need define transition probabilities: probabilities moving state every state.Let \\(t_{ij}\\) probability transitioning state \\(\\) state \\(j\\). example, \\(t_{RB} = 0.3\\) probability transitioning red state blue state 0.3. can write transition probabilities matrix.","code":""},{"path":"iterated-learning.html","id":"markov-exmaple1","chapter":"9 Iterated learning","heading":"9.1.1.1 Example 1","text":"‚Äôs transition probability matrix.example, \\(t_{RB} = t_{BR} = 1\\). means chain deterministically alternate red blue. Note rows transition matrix must sum 1 represent possible states chain might transition given current state. Although columns also sum 1 example, requirement.Example sequence: R B R B R B ‚Ä¶","code":""},{"path":"iterated-learning.html","id":"markov-example2","chapter":"9 Iterated learning","heading":"9.1.1.2 Example 2","text":"example, ‚Äôd expect sequence flip back forth red blue, deterministically. every step, sequence likely flip stay current color.Example sequence: R B R R B R R B R B R ‚Ä¶","code":""},{"path":"iterated-learning.html","id":"markov-example3","chapter":"9 Iterated learning","heading":"9.1.1.3 Example 3","text":"example, matter state ‚Äôre , 0.75 probability moving red 0.25 probability moving blue. Therefore, example, ‚Äôd expect chain spend 3/4 time red state 1/4 time blue state. example makes clear columns need sum 1.Example sequence: R R B B R R B B R R R ‚Ä¶","code":""},{"path":"iterated-learning.html","id":"stationary-distributions","chapter":"9 Iterated learning","heading":"9.1.2 Stationary distributions","text":"common question given Markov chain -called stationary distribution, proportion time chain spend state runs long time. Markov chains small number states, computing stationary distribution easy algebra. Consider following example.Let‚Äôs define transition matrix Markov chain two states, \\(s_1\\) \\(s_2\\):\\[\n\\begin{equation}\nT = \\left(\n\\begin{matrix}\nt_{11} & t_{12} \\\\\nt_{21} & t_{22}\n\\end{matrix} \\right)\n\\end{equation}\n\\]stationary distribution corresponds probability chain state \\(s_1\\) versus \\(s_2\\). define \\(\\theta_1\\) probability chain state \\(s_1\\) \\(\\theta_2\\) probability chain state \\(s_2\\). can define \\(\\theta_1\\) \\(\\theta_2\\) recursively follows:\\[\n\\begin{eqnarray}\n\\theta_1 = t_{11} \\theta_1 + t_{21} \\theta_2 \\\\\n\\theta_2 = t_{22} \\theta_2 + t_{12} \\theta_1 \\\\\n\\end{eqnarray}\n\\]equations essentially say probability state probability state staying state plus probability state switching states. use following facts solve \\(\\theta\\)s:\\[\n\\begin{eqnarray}\n\\theta_1 + \\theta_2 = 1 \\\\\nt_{11} + t_{12} = 1 \\\\\nt_{21} + t_{22} = 1\n\\end{eqnarray}\n\\]Using algebra, get following expressions:\\[\n\\begin{eqnarray}\n\\theta_1 = \\frac{t_{21}}{t_{12}+t_{21}} \\\\\n\\theta_2 = \\frac{t_{12}}{t_{21}+t_{12}}\n\\end{eqnarray}\n\\]can see expressions give sensible results applying red-blue examples .Example 1, unsurprisingly, \\(\\theta_R = \\theta_B = 0.5\\). sequence alternates, red half time blue half time. Example 2, \\(\\theta_R = \\theta_B = 0.5\\). Even though alternating deterministic, matter state sequence , equal probability switch state. Example 3, \\(\\theta_R = 0.75\\) \\(\\theta_B = 0.25\\).","code":""},{"path":"iterated-learning.html","id":"hmms","chapter":"9 Iterated learning","heading":"9.2 Hidden Markov models üôà","text":"Now let‚Äôs consider complicated situation step Markov chain generates observation. learner gets see observations, states . situation, say states ‚Äúhidden,‚Äù hence name hidden Markov model.Hidden Markov models sometimes used model language production, hidden states parts speech (like noun, verb, adjective, complex things like noun phrases, objects, subjects). actually get observe readers listeners, however, words (like ‚Äúdog,‚Äù ‚Äúeats,‚Äù ‚Äúplay‚Äù). English, many words ambiguous can potentially belong different parts speech. ‚ÄúPlay,‚Äù example noun (‚Äúactors performed play‚Äù) verb (‚Äúchildren play sand‚Äù). constantly decoding language observe infer parts speech words.","code":""},{"path":"iterated-learning.html","id":"formal-definition","chapter":"9 Iterated learning","heading":"9.2.1 Formal definition","text":"hidden Markov model includes following pieces:set states \\(S = \\{s_1, \\ldots, s_N\\}\\)set observations \\(O = \\{o_1, \\ldots, o_M\\}\\)set initial state probabilities \\(\\Pi = \\{\\pi_1, \\ldots, \\pi_N\\}\\), \\(\\pi_i\\) probability state \\(\\) begin first state sequenceA matrix state transition probabilities \\(T\\), \\(t_{ij}\\) matrix probability transitioning state \\(s_i\\) state \\(s_j\\)matrix ‚Äúemission‚Äù probabilities \\(B\\), \\(b_{ik}\\) matrix probability producing observation \\(o_k\\) state \\(s_i\\)observation sequence \\(Y = (y_1, \\ldots, y_T)\\), \\(T\\) length observed sequence dataAn state sequence observed \\(X = (x_1, \\ldots, x_T)\\)\nFigure 9.1: hidden Markov model. Source: Wikipedia (public domain).\n","code":""},{"path":"iterated-learning.html","id":"inference","chapter":"9 Iterated learning","heading":"9.2.2 Inference","text":"Usually purpose using HMM infer \\(X\\) \\(Y\\). standard algorithm called Viterbi algorithm, details beyond scope book.many existing programming packages available working HMMs. Python package hmmlearn, includes predict() function, predict probable sequence hidden states \\(X\\), given sequence observations \\(Y\\) fully specified Hidden Markov model. Note function always correct. model probabilistic observations can produced one state (example, word ‚Äúplay‚Äù produced either noun verb state), cases impossible certain state observation came .","code":""},{"path":"iterated-learning.html","id":"cultural-transmission","chapter":"9 Iterated learning","heading":"9.3 Cultural transmission üó£","text":"‚Äúdefecate‚Äù changing meaning time?Well can think communication time one giant long-term game Telephone. important question can ask : stationary distribution chains?subject line research computational cognitive science. basic model combines assumptions ‚Äôve seen assumptions hidden Markov models just learned chapter.First, assume people trying learn concept seeing example \\(x_i\\) getting label \\(y_i\\) trying generalize label instances forming hypothesis \\(h\\) things \\(y_i\\) applies . problem encountered generalization chapter, can assume person forms beliefs \\(h\\) using Bayesian inference.Now let‚Äôs add communication factor. person \\(n+1\\) gets label \\(y_n\\) someone else Person \\(n\\), forms hypothesis \\(h_{n+1}\\) labels new object \\(x_{n+1}\\) next person. process repeats, creating Markov chain.simplify chain, end sequence states state hypothesis concept.chain converge ?turns stationary distribution chain prior \\(P(h)\\). people make mistakes communication noisy, time, labels \\(y_n\\) become virtually useless people converge shared prior beliefs.","code":""},{"path":"iterated-learning.html","id":"using-iterated-learning-to-identify-peoples-priors","chapter":"9 Iterated learning","heading":"9.3.1 Using iterated learning to identify people‚Äôs priors","text":"reason, can use kind Telephone game way find people‚Äôs shared prior beliefs .study lead Michael Kalish tested idea function learning: learning predict \\(y\\) values \\(x\\) values.task, people given \\(x\\) values predict corresponding \\(y\\) values using slider. got feedback 50 training examples. additional 25 trials without feedback.25 test trials doubled used 50 training examples next person, . functions people eventually learn?figure shows several representative chains. researchers found regardless initial training data , chains usually converged linear increasing function, suggesting people expecting.\nFigure 9.2: Iterated function learning results Kalish et al.¬†(2007).\nmethod applied domains well. example, study lead Jordan Suchow, people reproduce accurately possible positions characters word. initial positions randomly positioned, person reproduced passed next person chain.\nFigure 9.3: Iterated typestting results Suchow et al.¬†(2016).\nresearchers found , time, people drifted much closer equal spaced letters, much legible. Moreover, averaged responses across chains (DZP2 figure), found people‚Äôs responses even better equal spacing, getting closer Linotype, recommended designers.","code":""},{"path":"causal-inference.html","id":"causal-inference","chapter":"10 Causal inference","heading":"10 Causal inference","text":"world full weird interesting relationships. Like one.\nFigure 10.1: Correlation Maine divorce rates per capita margarine consumption. Source: tylervigen.com.\nweird relationships causal relationships can tell? (cutting back margarine reduce chances divorce? Probably .)Causal inference , fact, major area research statistics machine learning. ‚Äôll just focus question people decide causes .‚Äôll use computational framework making optimal probabilistic causal judgments called Bayesian networks, Bayes nets short. Comparing people‚Äôs judgments Bayes net predictions allows us see optimal () people . Additionally, ‚Äôll see, Bayes nets well suited intervention, one way people learn causes.","code":""},{"path":"causal-inference.html","id":"bayes-nets","chapter":"10 Causal inference","heading":"10.1 Bayes nets ‚û°Ô∏è","text":"Bayes net graph describes dependencies variables situation.example, let‚Äôs make Bayes net problem Chapter 5 inferring bias coin. problem, three key variables: bias \\(\\theta\\), total number flips \\(n\\), number heads \\(k\\). can represent Bayes net.\nFigure 10.2: Bayes net representation coin bias problem.\nBayes nets, shaded notes represent variables known observed, unshaded nodes represent variables unknown. know many times coin flipped came heads, don‚Äôt directly know bias \\(\\theta\\).extend Bayes net capture generalization problem predicting outcome next coin flip \\(x\\).\nFigure 10.3: Bayes net representation biased coin generalization problem.\ncomplete Bayes net also specifies probability distribution variable. example , \\(k\\) function \\(\\theta\\) \\(n\\). learned Chapter 5, Binomial distribution. also need specify prior probability unknown variables, like \\(\\theta\\). Previously assumed distributed according Beta distribution. \\(x\\) just single coin flip ‚Äì ‚Äôs special case Binomial distribution called Bernoulli distribution \\(n = 1\\). sum :\\(\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\)\\(k \\sim \\text{Binomial}(n,\\theta)\\)\\(x \\sim \\text{Binomial}(n=1, \\theta)\\)","code":""},{"path":"causal-inference.html","id":"causal-intervention","chapter":"10 Causal inference","heading":"10.2 Causal intervention ü™ö","text":"Consider following two Bayes nets.\nFigure 10.4: Two Bayes nets three variables.\nconcrete, let‚Äôs say variables \\(s\\) \\(x\\) represent levels hormones sonin xanthan, respectively. Variable \\(z\\) unknown variable.common cause network -called sonin (\\(s\\)) xanthan (\\(x\\)) causally dependent \\(z\\). chain network -called variables form causal chain \\(s\\) \\(x\\) \\(z\\). (Note directions arrows two Bayes nets.)Let‚Äôs see kind data Bayes nets produce. Let‚Äôs assume root node network (\\(z\\) common cause, \\(x\\) chain) follows normal distribution mean 0 SD 1. link network follows normal distribution mean equal value parent node SD 1.\\(z\\) represents unknown variable, let‚Äôs plot just \\(s\\) \\(x\\).Clearly, data isn‚Äôt identical two cases, data generated Bayes nets results strong positive correlation \\(s\\) \\(x\\).Imagine didn‚Äôt know data generated just got one plots. use tell whether produced common cause structure chain structure?Sorry, . üòî Just knowing data positively correlated doesn‚Äôt give enough information figure \\(s\\) \\(x\\) causally related.manipulate variables? , intervene sonin levels see effect xanthan levels?increase sonin levels üìà xanthan levels also increase üìà, causal structure must chain.increase sonin levels üìà xanthan levels don‚Äôt change ‚ùå, causal structure can‚Äôt chain (therefore must common cause, ‚Äôs option ‚Äôre considering).","code":"import numpy as np\n\nn_samples = 10000\n\n# Common cause\nz_mu = 0\nsd = 1\n\nz_samples_cc = np.zeros(n_samples)\ns_samples_cc = np.zeros(n_samples)\nx_samples_cc = np.zeros(n_samples)\n\nfor i in range(n_samples):\n  z_samples_cc[i] = np.random.normal(z_mu, sd)\n  s_samples_cc[i] = np.random.normal(z_samples_cc[i], sd)\n  x_samples_cc[i] = np.random.normal(s_samples_cc[i], sd)\n\n# Chain\nx_mu = 0\n\nz_samples_chain = np.zeros(n_samples)\ns_samples_chain = np.zeros(n_samples)\nx_samples_chain = np.zeros(n_samples)\n\nfor i in range(n_samples):\n  x_samples_chain[i] = np.random.normal(x_mu, sd)\n  z_samples_chain[i] = np.random.normal(x_samples_chain[i], sd)\n  s_samples_chain[i] = np.random.normal(z_samples_chain[i], sd)import matplotlib.pyplot as plt\n\nfig, (ax1, ax2) = plt.subplots(1,2)\nax1.scatter(s_samples_cc, x_samples_cc, alpha = 0.1)\nax1.set_xlabel(\"s\")\nax1.set_ylabel(\"x\")\nax1.set_title(\"Common cause\")\n\nax2.scatter(s_samples_chain, x_samples_chain, alpha = 0.1)\nax2.set_xlabel(\"s\")\nax2.set_title(\"Chain\")\n\nplt.show()"},{"path":"causal-inference.html","id":"graph-surgery","chapter":"10 Causal inference","heading":"10.2.1 Graph surgery","text":"intuition can illustrated visually Bayes nets performing ‚Äúsurgery‚Äù graphs. works like :Remove incoming connections variable ‚Äôre intervening .‚Äôs still path variable intervened another variable, still expect variables related.Let‚Äôs apply idea common cause chain Bayes nets.\nFigure 10.5: Graph surgery applied common cause chain Bayes nets. invervention indicated red arrow.\nintervening sonin levels (\\(s\\)), remove connection \\(s\\) common cause network, connections chain network. resulting Bayes nets show expect see resulting change xanthan chain, common cause.","code":""},{"path":"causal-inference.html","id":"do-people-intuitively-understand-the-logic-of-casual-intervention","chapter":"10 Causal inference","heading":"10.2.2 Do people intuitively understand the logic of casual intervention?","text":"study Michael Waldmann York Hagmayer presented people either common cause chain structure. told sonin xanthan hormone levels chimps got example data allowed learn hormone levels positively correlated.either assigned seeing condition. People condition asked imagine 20 chimps sonin levels raised (lowered). predicted many chimps elevated xanthan levels. People seeing condition got essentially information just learned chimps‚Äô sonin levels high (intentionally raised).Average results model predictions .\nFigure 10.6: Results Waldmann & Hagmayer (2005), Experiment 2.\nPeople‚Äôs judgments mostly followed Bayes net model predictions. common cause case, sonin levels artificially raised, relationship sonin xanthan decoupled, model reverts base rate prediction xanthan levels (people, part). just observing elevated sonin levels, model expect positive relationship hold.chain case, predictions seeing , intervening doesn‚Äôt change anything relationship sonin xanthan. People‚Äôs judgments indicate understood .","code":""},{"path":"causal-inference.html","id":"structure-strength","chapter":"10 Causal inference","heading":"10.3 Causal structure and strength üèóüí™","text":"Bayes nets can also account people judge strength evidence causal relationship seeing data. idea Tom Griffiths Josh Tenenbaum explored 2005 computational study.‚Äôs basic problem considered. Suppose researchers perform experiment rats test whether drug causes gene expressed. control group rats doesn‚Äôt get injection experimental group . record number rats group express gene.possible results experiment 8 rats group. table shows many rats group expressed gene.hypothetical experiments, strongly say drug causes gene expressed?cases included experiment conducted Marc Buehner Patricia Cheng. ‚Äôs full set averaged human results.\nFigure 10.7: Results Buehner & Cheng (1997), Experiment 1B, reproduced Griffiths & Tenenbaum (2005). p(e+|c+) probability effect present (e.g., gene expressed) given cause present (e.g., drug administered). p(e+|c-) means probability effect present given cause absent.\nFocusing just cases table, average, people judged drug less likely causal effect gene total number rats expressing gene decreased, even difference number rats expressing gene conditions held constant.","code":""},{"path":"causal-inference.html","id":"the-causal-support-model","chapter":"10 Causal inference","heading":"10.3.1 The causal support model","text":"Maybe people reason problems performing kind model selection two Bayes nets .\nFigure 10.8: Causal inference model selection. one model, connection potential cause effect; , two variables unconnected. (Image Griffiths & Tenenbaum (2005).)\nBayes nets three variables: effect \\(E\\), cause \\(C\\), background cause \\(B\\). problem, effect cause refer gene drug. inclusion background cause account unknown factors might cause gene expressed without drug.problem people faced deciding two models best supported data \\(D\\) ‚Äì number times effect occurred without potential cause.can done Bayesian inference:\\[\nP(\\text{Graph } ) \\propto P(D|\\text{Graph } ) P(\\text{Graph } )\n\\]two possible networks, can compute relative evidence one Bayes net ratio. can take log expression simplify :\\[\n\\log \\frac{P(\\text{Graph } 1|D)}{P(\\text{Graph } 0|D)} = \\log \\frac{P(D | \\text{Graph } 1)}{P(D | \\text{Graph } 0)} + \\log \\frac{P(\\text{Graph } 1)}{P(\\text{Graph } 0)}\n\\]Regardless prior probabilities assign two graphs, relative evidence one graph entirely determined log-likelihood ratio. defined causal support: \\(\\log \\frac{P(D | \\text{Graph } 1)}{P(D | \\text{Graph } 0)}\\).Computing \\(P(D | \\text{Graph } 1)\\) requires fully specifying Bayes net. ‚Äôll assume \\(P(E|B) = w_0\\) \\(P(E|C) = w_1\\). \\(B\\) \\(C\\) present, ‚Äôll assume contribute independently causing \\(E\\), therefore operate like probabilistic function:\\[\nP(e^+|b,c; w_o, w_1) = 1 - (1-w_0)^b (1-w_1)^c\n\\], B C present, \\(b\\) \\(c\\) set 1, absent, \\(b\\) \\(c\\) set 0.likelihood Graph 1 therefore\\[\nP(D | w_0, w_1, \\text{Graph } 1) = \\prod_{e,c} P(e|b^+,c; w_o, w_1)^{N(e,c)}\n\\]product possible settings \\(e\\) \\(c\\) (effect absent/cause absent, effect absent/cause present, ‚Ä¶) \\(N(e,c)\\) values counts times outcomes happened data \\(D\\).‚Äôs function compute .Causal support doesn‚Äôt actually depend directly parameters \\(w_0\\) \\(w_1\\). reason ultimately don‚Äôt care values parameters just want draw inference higher level best-fitting Bayes net.Mathematically speaking, \\(w_0\\) \\(w_1\\) averaged model, idea first saw Chapter 3. can accomplish using Monte Carlo approximation, introduced Chapter 8.Finally, let‚Äôs put together writing function computes causal support.Now let‚Äôs see model predicts four cases considered earlier.labels x-axis indicate control condition counts, followed experimental condition counts.can see model‚Äôs predictions largely follow pattern data earlier. model favors Graph 1 two leftmost cases, essentially uncertain third case, begins think evidence favors causal relationship rightmost case.rest Griffiths & Tenenbaum paper shows causal support able capture subtle aspects people‚Äôs causal judgments models don‚Äôt incorporate structure strength fail capture.","code":"def compute_likelihood(data, w0, w1, graph):\n  '''Returns likelihood of data for a given graph\n     \n     Parameters:\n       data (list): observation counts in this order:\n         N(c-,e-), N(c-,e+), N(c+,e-), N(c+,e+)\n       w0 (float): probability of background cause producing effect\n       w1 (float): probability of cause of interest producing effect\n       graph (int): 0 (Graph 0) or 1 (Graph 1)\n\n     Returns:\n       (float): probability of data\n  '''\n  \n  if graph == 0:\n    # e-\n    p = (1-w0)**(data[0]+data[2])\n    # e+\n    p = p * w0**(data[1]+data[3])\n  elif graph == 1:\n    # c-, e-\n    p = (1-w0)**data[0]\n    # c-, e+\n    p = p * w0**data[1]\n    # c+, e-\n    p = p * (1 - (w0 + w1 - w0*w1))**data[2]\n    # c+, e+\n    p = p * (w0 + w1 - w0*w1)**data[3]\n  else:\n    # error!\n    return(0)\n  \n  return(p)def estimate_likelihood(graph_number, data): \n  '''Returns an estimate of the probability of observing the data\n     under the specified graph using Monte Carlo estimation.\n     \n     Parameters:\n       graph_number (int): either 0 (Graph 0) or 1 (Graph 1)\n       data (list): observation counts in this order:\n         N(c-,e-), N(c-,e+), N(c+,e-), N(c+,e+)\n\n     Returns:\n       (float): probability of observing data\n  '''\n  from numpy.random import default_rng\n  \n  n_samples = 5000\n  rng = default_rng(2022)\n  mc_samples = np.zeros(n_samples)\n  \n  for i in range(n_samples):\n    # Sample values for w0 and w1 from a uniform distribution\n    w0 = rng.random()\n    w1 = rng.random()\n    \n    mc_samples[i] = compute_likelihood(data, w0, w1, graph_number)\n  \n  return(1/n_samples * np.sum(mc_samples))def causal_support(data):\n  '''Returns a causal support value for a given data set.\n     Causal support is a measure of how strongly a data\n     set indicates that there is evidence for a causal\n     effect.\n     \n     Parameters:\n       data (list): observation counts in this order:\n         N(c-,e-), N(c-,e+), N(c+,e-), N(c+,e+)\n\n     Returns:\n       (float): strength of evidence for causal relationship\n  '''\n  import numpy as np\n  \n  return (np.log(estimate_likelihood(1,data=data) / \n                 estimate_likelihood(0,data=data)))causal_support_predictions = [causal_support([2,6,0,8]),\n                              causal_support([4,4,2,6]),\n                              causal_support([6,2,4,4]),\n                              causal_support([8,0,6,2])]\n                              \nlabels = [\"6/8, 8/8\", \"4/8, 6/8\", \"2/8, 4/8\", \"0/8, 2/8\"]\nfig, ax = plt.subplots()\nax.bar(labels, causal_support_predictions)## <BarContainer object of 4 artists>ax.set_xlabel(\"Data\")\nax.set_ylabel(\"Model prediction\")\n\nplt.show()"}]
