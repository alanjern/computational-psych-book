[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Computational Psychology",
    "section": "",
    "text": "Welcome"
  },
  {
    "objectID": "index.html#thanks",
    "href": "index.html#thanks",
    "title": "Introduction to Computational Psychology",
    "section": "Thanks",
    "text": "Thanks\nThis book borrows inspiration and content (especially Chapters 2 and 7) from Fausto Carcassi‚Äôs Introduction to Cognitive Modelling in R book and I am tremendously grateful to him for sharing his book publicly. Hopefully this resource will be equally helpful to others.\nA number of the modeling examples and homework assignments are adapted from code written by Danielle Navarro from previous iterations of her Computational Cognitive Science course with Andy Perfors. I am very grateful to both of them for making all of their materials public (and well documented).\nYour feedback is welcome and encouraged."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Introduction to Computational Psychology",
    "section": "License",
    "text": "License\n\n\n\n\n\nAnyone is free to reuse and adapt this book for their own non-commercial purposes, with attribution. If you do use this book in any way, please tell me about it."
  },
  {
    "objectID": "01-intro.html#representations",
    "href": "01-intro.html#representations",
    "title": "1¬† Why computational modeling?",
    "section": "1.1 Representations üî∏",
    "text": "1.1 Representations üî∏\nWe don‚Äôt perceive the world as it truly is. As one example, the visible spectrum that our eyes can detect is just a fraction of the full electromagnetic spectrum. Similarly, we can only hear a narrow range of audible sound frequencies. In other words, we perceive an incomplete picture of the surrounding world.\n\n\n\nSource: Abstruse Goose.\n\n\nSimilarly, we are constantly making assumptions about the things we see and hear and using those assumptions to fill in gaps.\nWhat we have in our heads is a kind of model of the world around us ‚Äì what cognitive scientists call a mental representation. These representations help us to reach rapid conclusions about things involving language, causes and effects, concepts, mental states, and many other aspects of cognition.\nSome of the key questions for cognitive scientists who use computational models are:\n\nWhat mental representations do we rely on?\nHow do our minds use these representations to learn when we get new information?\nWhat kind of information do we get and how do our expectations about the kind of information we‚Äôre getting to affect how we use it?\n\nThis book will elaborate, with examples, on each of these questions."
  },
  {
    "objectID": "01-intro.html#homework-1-build-your-first-computational-model",
    "href": "01-intro.html#homework-1-build-your-first-computational-model",
    "title": "1¬† Why computational modeling?",
    "section": "1.2 Homework 1: Build your first computational model üíª",
    "text": "1.2 Homework 1: Build your first computational model üíª\n\n\n\n\n\nTo get some initial experience with computational modeling, you‚Äôll build and experiment with a simple model of classical conditioning developed by Robert Rescorla and Allan Wagner ‚Äì now called the Rescorla-Wagner model.\nAll homework assignments for this book will be done in Google Colab. Click the button above at the top of this section view Homework 1.\nIf you‚Äôre unfamiliar with Colab (or Jupyter Notebooks), watch this brief introduction video.\n\n\n\n\n\n\n\nNote\n\n\n\nYou‚Äôll have to make a copy of the notebook saved to your own Drive in order to edit it."
  },
  {
    "objectID": "10-causal-inference.html#bayes-nets",
    "href": "10-causal-inference.html#bayes-nets",
    "title": "10¬† Causal inference",
    "section": "10.1 Bayes nets ‚û°Ô∏è",
    "text": "10.1 Bayes nets ‚û°Ô∏è\nA Bayes net is a graph that describes the dependencies between all the variables in a situation.\nFor example, let‚Äôs make a Bayes net for the problem in Chapter 5 of inferring the bias of a coin. In that problem, there were three key variables: the bias \\(\\theta\\), the total number of flips \\(n\\), and the number of heads \\(k\\). We can represent this as a Bayes net.\n\n\n\nBayes net representation of the coin bias problem.\n\n\nIn Bayes nets, shaded notes represent variables that are known or observed, and unshaded nodes represent variables that are unknown. We know how many times the coin was flipped and came up heads, but we don‚Äôt directly know the bias \\(\\theta\\).\nWe could extend this Bayes net to capture the generalization problem of predicting the outcome of the next coin flip \\(x\\).\n\n\n\nBayes net representation of the biased coin generalization problem.\n\n\nA complete Bayes net also specifies a probability distribution for each variable. In the example above, \\(k\\) is a function of \\(\\theta\\) and \\(n\\). As we learned in Chapter 5, this is a Binomial distribution. We also need to specify a prior probability over any unknown variables, like \\(\\theta\\). Previously we assumed it was distributed according to a Beta distribution. \\(x\\) is just a single coin flip ‚Äì it‚Äôs a special case of a Binomial distribution called a Bernoulli distribution in which \\(n = 1\\). To sum up:\n\n\\(\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\)\n\\(k \\sim \\text{Binomial}(n,\\theta)\\)\n\\(x \\sim \\text{Binomial}(n=1, \\theta)\\)"
  },
  {
    "objectID": "10-causal-inference.html#causal-intervention",
    "href": "10-causal-inference.html#causal-intervention",
    "title": "10¬† Causal inference",
    "section": "10.2 Causal intervention ü™ö",
    "text": "10.2 Causal intervention ü™ö\nConsider the following two Bayes nets.\n\n\n\nTwo Bayes nets with three variables.\n\n\nTo be concrete, let‚Äôs say that variables \\(s\\) and \\(x\\) represent levels of hormones sonin and xanthan, respectively. Variable \\(z\\) is an unknown variable.\nThe common cause network is so-called because sonin (\\(s\\)) and xanthan (\\(x\\)) are both causally dependent on \\(z\\). The chain network is so-called because all the variables form a causal chain from \\(s\\) to \\(x\\) to \\(z\\). (Note the directions of the arrows in the two Bayes nets.)\nLet‚Äôs see what kind of data these Bayes nets produce. Let‚Äôs assume that each root node of a network (\\(z\\) in the common cause, \\(x\\) in the chain) follows a normal distribution with mean 0 and SD 1. Each link in a network follows a normal distribution with mean equal to the value of its parent node and SD 1.\n\nimport numpy as np\n\nn_samples = 10000\n\n# Common cause\nz_mu = 0\nsd = 1\n\nz_samples_cc = np.zeros(n_samples)\ns_samples_cc = np.zeros(n_samples)\nx_samples_cc = np.zeros(n_samples)\n\nfor i in range(n_samples):\n  z_samples_cc[i] = np.random.normal(z_mu, sd)\n  s_samples_cc[i] = np.random.normal(z_samples_cc[i], sd)\n  x_samples_cc[i] = np.random.normal(s_samples_cc[i], sd)\n\n# Chain\nx_mu = 0\n\nz_samples_chain = np.zeros(n_samples)\ns_samples_chain = np.zeros(n_samples)\nx_samples_chain = np.zeros(n_samples)\n\nfor i in range(n_samples):\n  x_samples_chain[i] = np.random.normal(x_mu, sd)\n  z_samples_chain[i] = np.random.normal(x_samples_chain[i], sd)\n  s_samples_chain[i] = np.random.normal(z_samples_chain[i], sd)\n\nBecause \\(z\\) represents an unknown variable, let‚Äôs plot just \\(s\\) and \\(x\\).\n\nimport matplotlib.pyplot as plt\n\nfig, (ax1, ax2) = plt.subplots(1,2)\nax1.scatter(s_samples_cc, x_samples_cc, alpha = 0.1)\nax1.set_xlabel(\"s\")\nax1.set_ylabel(\"x\")\nax1.set_title(\"Common cause\")\n\nax2.scatter(s_samples_chain, x_samples_chain, alpha = 0.1)\nax2.set_xlabel(\"s\")\nax2.set_title(\"Chain\")\n\nplt.show()\n\n\n\n\nClearly, the data isn‚Äôt identical in the two cases, but data generated by both Bayes nets results in a strong positive correlation between \\(s\\) and \\(x\\).\nImagine you didn‚Äôt know how this data was generated and you just got one of these plots. Could you use it to tell whether it was produced by a common cause structure or a chain structure?\nSorry, but no. üòî Just knowing the data are positively correlated doesn‚Äôt give you enough information to figure out how \\(s\\) and \\(x\\) are causally related.\nBut what if you could manipulate the variables? That is, what if you could intervene on sonin levels and see what effect it had on xanthan levels?\n\nIf you increase the sonin levels üìà and the xanthan levels also increase üìà, then the causal structure must be a chain.\nIf you increase the sonin levels üìà and the xanthan levels don‚Äôt change ‚ùå, then the causal structure can‚Äôt be a chain (and therefore must be a common cause, because that‚Äôs the only other option we‚Äôre considering).\n\n\n10.2.1 Graph surgery\nThis intuition can be illustrated visually on the Bayes nets by performing ‚Äúsurgery‚Äù on the graphs. It works like this:\n\nRemove all incoming connections to the variable you‚Äôre intervening on.\nIf there‚Äôs still a path between the variable you intervened on and another variable, then you should still expect those variables to be related.\n\nLet‚Äôs apply this idea to our common cause and chain Bayes nets.\n\n\n\nGraph surgery applied to the common cause and chain Bayes nets. The invervention is indicated by the red arrow.\n\n\nAfter intervening on sonin levels (\\(s\\)), we remove the connection to \\(s\\) in the common cause network, but no connections in the chain network. The resulting Bayes nets show why we should expect to see a resulting change in xanthan for the chain, but not the common cause.\n\n\n10.2.2 Do people intuitively understand the logic of casual intervention?\nA study by Michael Waldmann and York Hagmayer presented people with either the common cause or the chain structure. They were told that sonin and xanthan were hormone levels in chimps and they got some example data that allowed them to learn that the hormone levels were positively correlated.\nThen they were either assigned to a doing or seeing condition. People in the doing condition were asked to imagine that 20 chimps had their sonin levels raised (or lowered). They then predicted how many of the chimps would have elevated xanthan levels. People in the seeing condition got essentially the same information but just learned that the chimps‚Äô sonin levels were high (not that it had been intentionally raised).\nAverage results and model predictions are below.\n\n\n\nResults from Waldmann & Hagmayer (2005), Experiment 2.\n\n\nPeople‚Äôs judgments mostly followed those of the Bayes net model predictions. In the common cause case, when the sonin levels are artificially raised, the relationship between sonin and xanthan is decoupled, so the model reverts to a base rate prediction about xanthan levels (and so did people, for the most part). But when just observing elevated sonin levels, the model should expect the positive relationship to hold.\nIn the chain case, the predictions are the same for seeing or doing, because intervening doesn‚Äôt change anything about the relationship between sonin and xanthan. People‚Äôs judgments indicate that they understood this."
  },
  {
    "objectID": "10-causal-inference.html#structure-strength",
    "href": "10-causal-inference.html#structure-strength",
    "title": "10¬† Causal inference",
    "section": "10.3 Causal structure and strength üèóüí™",
    "text": "10.3 Causal structure and strength üèóüí™\nBayes nets can also account for how people judge the strength of evidence for a causal relationship after seeing some data. This was the idea that Tom Griffiths and Josh Tenenbaum explored in a 2005 computational study.\nHere‚Äôs the basic problem they considered. Suppose researchers perform an experiment with rats to test whether a drug causes a gene to be expressed. A control group of rats doesn‚Äôt get the injection and the experimental group does. They record the number of rats in each group that express the gene.\nHere are some possible results from an experiment with 8 rats in each group. The table shows how many rats in each group expressed the gene.\n\n\n\nControl üêÄ\nExperimental üêÄ\n\n\n\n\n6/8 üß¨\n8/8 üß¨\n\n\n4/8 üß¨\n6/8 üß¨\n\n\n2/8 üß¨\n4/8 üß¨\n\n\n0/8 üß¨\n2/8 üß¨\n\n\n\nIn each of these hypothetical experiments, how strongly would you say that the drug causes the gene to be expressed?\nThese are a few of the cases that were included in an experiment conducted by Marc Buehner and Patricia Cheng. Here‚Äôs the full set of averaged human results.\n\n\n\nResults from Buehner & Cheng (1997), Experiment 1B, reproduced by Griffiths & Tenenbaum (2005). p(e+|c+) is the probability of the effect being present (e.g., the gene being expressed) given that the cause is present (e.g., the drug was administered). p(e+|c-) means the probability of the effect being present given that the cause is absent.\n\n\nFocusing just on the cases in the table, on average, people judged that the drug was less likely to have a causal effect on the gene as the total number of rats expressing the gene decreased, even when the difference in number of rats expressing the gene between conditions was held constant.\n\n10.3.1 The causal support model\nMaybe people reason about these problems by performing a kind of model selection between the two Bayes nets below.\n\n\n\nCausal inference as model selection. In one model, there is a connection from the potential cause to the effect; in the other, the two variables are unconnected. (Image from Griffiths & Tenenbaum (2005).)\n\n\nThese Bayes nets each have three variables: an effect \\(E\\), a cause \\(C\\), and a background cause \\(B\\). For our problem, the effect and cause refer to the gene and the drug. The inclusion of the background cause is to account for unknown factors that might cause the gene to be expressed without the drug.\nThe problem people are faced with is deciding which of these two models is best supported by the data \\(D\\) ‚Äì the number of times the effect occurred with and without the potential cause.\nThis can be done with Bayesian inference:\n\\[\nP(\\text{Graph } i) \\propto P(D|\\text{Graph } i) P(\\text{Graph } i)\n\\]\nBecause there are only two possible networks, we can compute the relative evidence for one Bayes net over the other as a ratio. We can then take the log of the expression to simplify it:\n\\[\n\\log \\frac{P(\\text{Graph } 1|D)}{P(\\text{Graph } 0|D)} = \\log \\frac{P(D | \\text{Graph } 1)}{P(D | \\text{Graph } 0)} + \\log \\frac{P(\\text{Graph } 1)}{P(\\text{Graph } 0)}\n\\]\nRegardless of what prior probabilities we assign to the two graphs, the relative evidence for one graph over the other is entirely determined by the log-likelihood ratio. This is defined as causal support: \\(\\log \\frac{P(D | \\text{Graph } 1)}{P(D | \\text{Graph } 0)}\\).\nComputing \\(P(D | \\text{Graph } 1)\\) requires fully specifying the Bayes net. We‚Äôll assume that \\(P(E|B) = w_0\\) and \\(P(E|C) = w_1\\). When both \\(B\\) and \\(C\\) are present, we‚Äôll assume they contribute independently to causing \\(E\\), and therefore operate like a probabilistic OR function:\n\\[\nP(e^+|b,c; w_o, w_1) = 1 - (1-w_0)^b (1-w_1)^c\n\\]\nHere, when the B or C are present, \\(b\\) and \\(c\\) are set to 1, and when they are absent, \\(b\\) and \\(c\\) are set to 0.\nThe likelihood for Graph 1 is therefore\n\\[\nP(D | w_0, w_1, \\text{Graph } 1) = \\prod_{e,c} P(e|b^+,c; w_o, w_1)^{N(e,c)}\n\\]\nwhere the product is over the possible settings of \\(e\\) and \\(c\\) (effect absent/cause absent, effect absent/cause present, ‚Ä¶) and the \\(N(e,c)\\) values are counts of times that these outcomes happened in the data \\(D\\).\nHere‚Äôs a function to compute this.\n\ndef compute_likelihood(data, w0, w1, graph):\n  '''Returns likelihood of data for a given graph\n     \n     Parameters:\n       data (list): observation counts in this order:\n         N(c-,e-), N(c-,e+), N(c+,e-), N(c+,e+)\n       w0 (float): probability of background cause producing effect\n       w1 (float): probability of cause of interest producing effect\n       graph (int): 0 (Graph 0) or 1 (Graph 1)\n\n     Returns:\n       (float): probability of data\n  '''\n  \n  if graph == 0:\n    # e-\n    p = (1-w0)**(data[0]+data[2])\n    # e+\n    p = p * w0**(data[1]+data[3])\n  elif graph == 1:\n    # c-, e-\n    p = (1-w0)**data[0]\n    # c-, e+\n    p = p * w0**data[1]\n    # c+, e-\n    p = p * (1 - (w0 + w1 - w0*w1))**data[2]\n    # c+, e+\n    p = p * (w0 + w1 - w0*w1)**data[3]\n  else:\n    # error!\n    return(0)\n  \n  return(p)\n\nCausal support doesn‚Äôt actually depend directly on the parameters \\(w_0\\) and \\(w_1\\). The reason is that we ultimately don‚Äôt care what the values of these parameters are because we just want to draw an inference at a higher level about the best-fitting Bayes net.\nMathematically speaking, \\(w_0\\) and \\(w_1\\) are averaged out of the model, an idea we first saw in Chapter 3. We can accomplish this using Monte Carlo approximation, introduced in Chapter 8.\n\ndef estimate_likelihood(graph_number, data): \n  '''Returns an estimate of the probability of observing the data\n     under the specified graph using Monte Carlo estimation.\n     \n     Parameters:\n       graph_number (int): either 0 (Graph 0) or 1 (Graph 1)\n       data (list): observation counts in this order:\n         N(c-,e-), N(c-,e+), N(c+,e-), N(c+,e+)\n\n     Returns:\n       (float): probability of observing data\n  '''\n  from numpy.random import default_rng\n  \n  n_samples = 5000\n  rng = default_rng(2022)\n  mc_samples = np.zeros(n_samples)\n  \n  for i in range(n_samples):\n    # Sample values for w0 and w1 from a uniform distribution\n    w0 = rng.random()\n    w1 = rng.random()\n    \n    mc_samples[i] = compute_likelihood(data, w0, w1, graph_number)\n  \n  return(1/n_samples * np.sum(mc_samples))\n\nFinally, let‚Äôs put it all together by writing a function that computes causal support.\n\ndef causal_support(data):\n  '''Returns a causal support value for a given data set.\n     Causal support is a measure of how strongly a data\n     set indicates that there is evidence for a causal\n     effect.\n     \n     Parameters:\n       data (list): observation counts in this order:\n         N(c-,e-), N(c-,e+), N(c+,e-), N(c+,e+)\n\n     Returns:\n       (float): strength of evidence for causal relationship\n  '''\n  import numpy as np\n  \n  return (np.log(estimate_likelihood(1,data=data) / \n                 estimate_likelihood(0,data=data)))\n\nNow let‚Äôs see what the model predicts for the four cases we considered earlier.\n\ncausal_support_predictions = [causal_support([2,6,0,8]),\n                              causal_support([4,4,2,6]),\n                              causal_support([6,2,4,4]),\n                              causal_support([8,0,6,2])]\n                              \nlabels = [\"6/8, 8/8\", \"4/8, 6/8\", \"2/8, 4/8\", \"0/8, 2/8\"]\nfig, ax = plt.subplots()\nax.bar(labels, causal_support_predictions)\nax.set_xlabel(\"Data\")\nax.set_ylabel(\"Model prediction\")\n\nplt.show()\n\n\n\n\nThe labels on the x-axis indicate the control condition counts, followed by the experimental condition counts.\nYou can see that the model‚Äôs predictions largely follow the pattern in the data from earlier. The model favors Graph 1 in the two leftmost cases, is essentially uncertain in the third case, and begins to think the evidence favors no causal relationship in the rightmost case.\nThe rest of the Griffiths & Tenenbaum paper shows how causal support is able to capture some subtle aspects of people‚Äôs causal judgments that other models that don‚Äôt incorporate both structure and strength fail to capture."
  }
]