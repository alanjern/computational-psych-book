[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Computational Psychology",
    "section": "",
    "text": "Welcome"
  },
  {
    "objectID": "index.html#thanks",
    "href": "index.html#thanks",
    "title": "Introduction to Computational Psychology",
    "section": "Thanks",
    "text": "Thanks\nThis book borrows inspiration and content (especially Chapters 2 and 7) from Fausto Carcassi‚Äôs Introduction to Cognitive Modelling in R book and I am tremendously grateful to him for sharing his book publicly. Hopefully this resource will be equally helpful to others.\nA number of the modeling examples and homework assignments are adapted from code written by Danielle Navarro from previous iterations of her Computational Cognitive Science course with Andy Perfors. I am very grateful to both of them for making all of their materials public (and well documented).\nYour feedback is welcome and encouraged."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Introduction to Computational Psychology",
    "section": "License",
    "text": "License\n\n\n\n\n\nAnyone is free to reuse and adapt this book for their own non-commercial purposes, with attribution. If you do use this book in any way, please tell me about it."
  },
  {
    "objectID": "01-intro.html#representations",
    "href": "01-intro.html#representations",
    "title": "1¬† Why computational modeling?",
    "section": "1.1 Representations üî∏",
    "text": "1.1 Representations üî∏\nWe don‚Äôt perceive the world as it truly is. As one example, the visible spectrum that our eyes can detect is just a fraction of the full electromagnetic spectrum. Similarly, we can only hear a narrow range of sound frequencies. In other words, we perceive an incomplete picture of the world.\n\n\n\nSource: Abstruse Goose.\n\n\nEven for the parts of the world that we can perceive, the information we get through our senses is often incomplete, so we are constantly making assumptions to fill in gaps.\nAs a result, what we have in our heads is a kind of model of the world around us ‚Äì what cognitive scientists call a mental representation. These representations help us to reach rapid conclusions about things involving language, causes and effects, concepts, mental states, and many other aspects of cognition.\nSome of the key questions for cognitive scientists who use computational models are:\n\nWhat mental representations do we rely on?\nHow do our minds use these representations to learn when we get new information?\nWhat kind of information do we get and how do our expectations about the kind of information we‚Äôre getting to affect how we use it?\n\nThis book focuses on each of these questions."
  },
  {
    "objectID": "01-intro.html#homework-1-build-your-first-computational-model",
    "href": "01-intro.html#homework-1-build-your-first-computational-model",
    "title": "1¬† Why computational modeling?",
    "section": "1.2 Homework 1: Build your first computational model üíª",
    "text": "1.2 Homework 1: Build your first computational model üíª\nTo get some initial experience with computational modeling, you‚Äôll build and experiment with a simple model of classical conditioning developed by Robert Rescorla and Allan Wagner ‚Äì now called the Rescorla-Wagner model.\nAll homework assignments for this book will be done in Google Colab. Click the button below to view Homework 1.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou‚Äôll have to make a copy of the notebook saved to your own Drive in order to edit it.\n\n\nIf you‚Äôre unfamiliar with Colab (or Jupyter Notebooks), watch this brief introduction video.\n\nThis video gives some extra tips for working with Colab."
  },
  {
    "objectID": "10-causal-inference.html#bayes-nets",
    "href": "10-causal-inference.html#bayes-nets",
    "title": "10¬† Causal inference",
    "section": "10.1 Bayes nets ‚û°Ô∏è",
    "text": "10.1 Bayes nets ‚û°Ô∏è\nA Bayes net is a graph that describes the dependencies between all the variables in a situation.\nFor example, let‚Äôs make a Bayes net for the problem in Chapter 5 of inferring the bias of a coin. In that problem, there were three key variables: the bias \\(\\theta\\), the total number of flips \\(n\\), and the number of heads \\(k\\). We can represent this as a Bayes net.\n\n\n\nBayes net representation of the coin bias problem.\n\n\nIn Bayes nets, shaded notes represent variables that are known or observed, and unshaded nodes represent variables that are unknown. We know how many times the coin was flipped and came up heads, but we don‚Äôt directly know the bias \\(\\theta\\).\nWe could extend this Bayes net to capture the generalization problem of predicting the outcome of the next coin flip \\(x\\).\n\n\n\nBayes net representation of the biased coin generalization problem.\n\n\nA complete Bayes net also specifies a probability distribution for each variable. In the example above, \\(k\\) is a function of \\(\\theta\\) and \\(n\\). As we learned in Chapter 5, this is a Binomial distribution. We also need to specify a prior probability over any unknown variables, like \\(\\theta\\). Previously we assumed it was distributed according to a Beta distribution. \\(x\\) is just a single coin flip ‚Äì it‚Äôs a special case of a Binomial distribution called a Bernoulli distribution in which \\(n = 1\\). To sum up:\n\n\\(\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\)\n\\(k \\sim \\text{Binomial}(n,\\theta)\\)\n\\(x \\sim \\text{Binomial}(n=1, \\theta)\\)"
  },
  {
    "objectID": "10-causal-inference.html#causal-intervention",
    "href": "10-causal-inference.html#causal-intervention",
    "title": "10¬† Causal inference",
    "section": "10.2 Causal intervention ü™ö",
    "text": "10.2 Causal intervention ü™ö\nConsider the following two Bayes nets.\n\n\n\nTwo Bayes nets with three variables.\n\n\nTo be concrete, let‚Äôs say that variables \\(s\\) and \\(x\\) represent levels of hormones sonin and xanthan, respectively. Variable \\(z\\) is an unknown variable.\nThe common cause network is so-called because sonin (\\(s\\)) and xanthan (\\(x\\)) are both causally dependent on \\(z\\). The chain network is so-called because all the variables form a causal chain from \\(s\\) to \\(x\\) to \\(z\\). (Note the directions of the arrows in the two Bayes nets.)\nLet‚Äôs see what kind of data these Bayes nets produce. Let‚Äôs assume that each root node of a network (\\(z\\) in the common cause, \\(x\\) in the chain) follows a normal distribution with mean 0 and SD 1. Each link in a network follows a normal distribution with mean equal to the value of its parent node and SD 1.\n\nimport numpy as np\n\nn_samples = 10000\n\n# Common cause\nz_mu = 0\nsd = 1\n\nz_samples_cc = np.zeros(n_samples)\ns_samples_cc = np.zeros(n_samples)\nx_samples_cc = np.zeros(n_samples)\n\nfor i in range(n_samples):\n  z_samples_cc[i] = np.random.normal(z_mu, sd)\n  s_samples_cc[i] = np.random.normal(z_samples_cc[i], sd)\n  x_samples_cc[i] = np.random.normal(s_samples_cc[i], sd)\n\n# Chain\nx_mu = 0\n\nz_samples_chain = np.zeros(n_samples)\ns_samples_chain = np.zeros(n_samples)\nx_samples_chain = np.zeros(n_samples)\n\nfor i in range(n_samples):\n  x_samples_chain[i] = np.random.normal(x_mu, sd)\n  z_samples_chain[i] = np.random.normal(x_samples_chain[i], sd)\n  s_samples_chain[i] = np.random.normal(z_samples_chain[i], sd)\n\nBecause \\(z\\) represents an unknown variable, let‚Äôs plot just \\(s\\) and \\(x\\).\n\nimport matplotlib.pyplot as plt\n\nfig, (ax1, ax2) = plt.subplots(1,2)\nax1.scatter(s_samples_cc, x_samples_cc, alpha = 0.1)\nax1.set_xlabel(\"s\")\nax1.set_ylabel(\"x\")\nax1.set_title(\"Common cause\")\n\nax2.scatter(s_samples_chain, x_samples_chain, alpha = 0.1)\nax2.set_xlabel(\"s\")\nax2.set_title(\"Chain\")\n\nplt.show()\n\n\n\n\nClearly, the data isn‚Äôt identical in the two cases, but data generated by both Bayes nets results in a strong positive correlation between \\(s\\) and \\(x\\).\nImagine you didn‚Äôt know how this data was generated and you just got one of these plots. Could you use it to tell whether it was produced by a common cause structure or a chain structure?\nSorry, but no. üòî Just knowing the data are positively correlated doesn‚Äôt give you enough information to figure out how \\(s\\) and \\(x\\) are causally related.\nBut what if you could manipulate the variables? That is, what if you could intervene on sonin levels and see what effect it had on xanthan levels?\n\nIf you increase the sonin levels üìà and the xanthan levels also increase üìà, then the causal structure must be a chain.\nIf you increase the sonin levels üìà and the xanthan levels don‚Äôt change ‚ùå, then the causal structure can‚Äôt be a chain (and therefore must be a common cause, because that‚Äôs the only other option we‚Äôre considering).\n\n\n10.2.1 Graph surgery\nThis intuition can be illustrated visually on the Bayes nets by performing ‚Äúsurgery‚Äù on the graphs. It works like this:\n\nRemove all incoming connections to the variable you‚Äôre intervening on.\nIf there‚Äôs still a path between the variable you intervened on and another variable, then you should still expect those variables to be related.\n\nLet‚Äôs apply this idea to our common cause and chain Bayes nets.\n\n\n\nGraph surgery applied to the common cause and chain Bayes nets. The invervention is indicated by the red arrow.\n\n\nAfter intervening on sonin levels (\\(s\\)), we remove the connection to \\(s\\) in the common cause network, but no connections in the chain network. The resulting Bayes nets show why we should expect to see a resulting change in xanthan for the chain, but not the common cause.\n\n\n10.2.2 Do people intuitively understand the logic of casual intervention?\nA study by Michael Waldmann and York Hagmayer presented people with either the common cause or the chain structure. They were told that sonin and xanthan were hormone levels in chimps and they got some example data that allowed them to learn that the hormone levels were positively correlated.\nThen they were either assigned to a doing or seeing condition. People in the doing condition were asked to imagine that 20 chimps had their sonin levels raised (or lowered). They then predicted how many of the chimps would have elevated xanthan levels. People in the seeing condition got essentially the same information but just learned that the chimps‚Äô sonin levels were high (not that it had been intentionally raised).\nAverage results and model predictions are below.\n\n\n\nResults from Waldmann & Hagmayer (2005), Experiment 2.\n\n\nPeople‚Äôs judgments mostly followed those of the Bayes net model predictions. In the common cause case, when the sonin levels are artificially raised, the relationship between sonin and xanthan is decoupled, so the model reverts to a base rate prediction about xanthan levels (and so did people, for the most part). But when just observing elevated sonin levels, the model should expect the positive relationship to hold.\nIn the chain case, the predictions are the same for seeing or doing, because intervening doesn‚Äôt change anything about the relationship between sonin and xanthan. People‚Äôs judgments indicate that they understood this."
  },
  {
    "objectID": "10-causal-inference.html#structure-strength",
    "href": "10-causal-inference.html#structure-strength",
    "title": "10¬† Causal inference",
    "section": "10.3 Causal structure and strength üèóüí™",
    "text": "10.3 Causal structure and strength üèóüí™\nBayes nets can also account for how people judge the strength of evidence for a causal relationship after seeing some data. This was the idea that Tom Griffiths and Josh Tenenbaum explored in a 2005 computational study.\nHere‚Äôs the basic problem they considered. Suppose researchers perform an experiment with rats to test whether a drug causes a gene to be expressed. A control group of rats doesn‚Äôt get the injection and the experimental group does. They record the number of rats in each group that express the gene.\nHere are some possible results from an experiment with 8 rats in each group. The table shows how many rats in each group expressed the gene.\n\n\n\nControl üêÄ\nExperimental üêÄ\n\n\n\n\n6/8 üß¨\n8/8 üß¨\n\n\n4/8 üß¨\n6/8 üß¨\n\n\n2/8 üß¨\n4/8 üß¨\n\n\n0/8 üß¨\n2/8 üß¨\n\n\n\nIn each of these hypothetical experiments, how strongly would you say that the drug causes the gene to be expressed?\nThese are a few of the cases that were included in an experiment conducted by Marc Buehner and Patricia Cheng. Here‚Äôs the full set of averaged human results.\n\n\n\nResults from Buehner & Cheng (1997), Experiment 1B, reproduced by Griffiths & Tenenbaum (2005). p(e+|c+) is the probability of the effect being present (e.g., the gene being expressed) given that the cause is present (e.g., the drug was administered). p(e+|c-) means the probability of the effect being present given that the cause is absent.\n\n\nFocusing just on the cases in the table, on average, people judged that the drug was less likely to have a causal effect on the gene as the total number of rats expressing the gene decreased, even when the difference in number of rats expressing the gene between conditions was held constant.\n\n10.3.1 The causal support model\nMaybe people reason about these problems by performing a kind of model selection between the two Bayes nets below.\n\n\n\nCausal inference as model selection. In one model, there is a connection from the potential cause to the effect; in the other, the two variables are unconnected. (Image from Griffiths & Tenenbaum (2005).)\n\n\nThese Bayes nets each have three variables: an effect \\(E\\), a cause \\(C\\), and a background cause \\(B\\). For our problem, the effect and cause refer to the gene and the drug. The inclusion of the background cause is to account for unknown factors that might cause the gene to be expressed without the drug.\nThe problem people are faced with is deciding which of these two models is best supported by the data \\(D\\) ‚Äì the number of times the effect occurred with and without the potential cause.\nThis can be done with Bayesian inference:\n\\[\nP(\\text{Graph } i) \\propto P(D|\\text{Graph } i) P(\\text{Graph } i)\n\\]\nBecause there are only two possible networks, we can compute the relative evidence for one Bayes net over the other as a ratio. We can then take the log of the expression to simplify it:\n\\[\n\\log \\frac{P(\\text{Graph } 1|D)}{P(\\text{Graph } 0|D)} = \\log \\frac{P(D | \\text{Graph } 1)}{P(D | \\text{Graph } 0)} + \\log \\frac{P(\\text{Graph } 1)}{P(\\text{Graph } 0)}\n\\]\nRegardless of what prior probabilities we assign to the two graphs, the relative evidence for one graph over the other is entirely determined by the log-likelihood ratio. This is defined as causal support: \\(\\log \\frac{P(D | \\text{Graph } 1)}{P(D | \\text{Graph } 0)}\\).\nComputing \\(P(D | \\text{Graph } 1)\\) requires fully specifying the Bayes net. We‚Äôll assume that \\(P(E|B) = w_0\\) and \\(P(E|C) = w_1\\). When both \\(B\\) and \\(C\\) are present, we‚Äôll assume they contribute independently to causing \\(E\\), and therefore operate like a probabilistic OR function:\n\\[\nP(e^+|b,c; w_o, w_1) = 1 - (1-w_0)^b (1-w_1)^c\n\\]\nHere, when the B or C are present, \\(b\\) and \\(c\\) are set to 1, and when they are absent, \\(b\\) and \\(c\\) are set to 0.\nThe likelihood for Graph 1 is therefore\n\\[\nP(D | w_0, w_1, \\text{Graph } 1) = \\prod_{e,c} P(e|b^+,c; w_o, w_1)^{N(e,c)}\n\\]\nwhere the product is over the possible settings of \\(e\\) and \\(c\\) (effect absent/cause absent, effect absent/cause present, ‚Ä¶) and the \\(N(e,c)\\) values are counts of times that these outcomes happened in the data \\(D\\).\nHere‚Äôs a function to compute this.\n\ndef compute_likelihood(data, w0, w1, graph):\n  '''Returns likelihood of data for a given graph\n     \n     Parameters:\n       data (list): observation counts in this order:\n         N(c-,e-), N(c-,e+), N(c+,e-), N(c+,e+)\n       w0 (float): probability of background cause producing effect\n       w1 (float): probability of cause of interest producing effect\n       graph (int): 0 (Graph 0) or 1 (Graph 1)\n\n     Returns:\n       (float): probability of data\n  '''\n  \n  if graph == 0:\n    # e-\n    p = (1-w0)**(data[0]+data[2])\n    # e+\n    p = p * w0**(data[1]+data[3])\n  elif graph == 1:\n    # c-, e-\n    p = (1-w0)**data[0]\n    # c-, e+\n    p = p * w0**data[1]\n    # c+, e-\n    p = p * (1 - (w0 + w1 - w0*w1))**data[2]\n    # c+, e+\n    p = p * (w0 + w1 - w0*w1)**data[3]\n  else:\n    # error!\n    return(0)\n  \n  return(p)\n\nCausal support doesn‚Äôt actually depend directly on the parameters \\(w_0\\) and \\(w_1\\). The reason is that we ultimately don‚Äôt care what the values of these parameters are because we just want to draw an inference at a higher level about the best-fitting Bayes net.\nMathematically speaking, \\(w_0\\) and \\(w_1\\) are averaged out of the model, an idea we first saw in Chapter 3. We can accomplish this using Monte Carlo approximation, introduced in Chapter 8.\n\ndef estimate_likelihood(graph_number, data): \n  '''Returns an estimate of the probability of observing the data\n     under the specified graph using Monte Carlo estimation.\n     \n     Parameters:\n       graph_number (int): either 0 (Graph 0) or 1 (Graph 1)\n       data (list): observation counts in this order:\n         N(c-,e-), N(c-,e+), N(c+,e-), N(c+,e+)\n\n     Returns:\n       (float): probability of observing data\n  '''\n  from numpy.random import default_rng\n  \n  n_samples = 5000\n  rng = default_rng(2022)\n  mc_samples = np.zeros(n_samples)\n  \n  for i in range(n_samples):\n    # Sample values for w0 and w1 from a uniform distribution\n    w0 = rng.random()\n    w1 = rng.random()\n    \n    mc_samples[i] = compute_likelihood(data, w0, w1, graph_number)\n  \n  return(1/n_samples * np.sum(mc_samples))\n\nFinally, let‚Äôs put it all together by writing a function that computes causal support.\n\ndef causal_support(data):\n  '''Returns a causal support value for a given data set.\n     Causal support is a measure of how strongly a data\n     set indicates that there is evidence for a causal\n     effect.\n     \n     Parameters:\n       data (list): observation counts in this order:\n         N(c-,e-), N(c-,e+), N(c+,e-), N(c+,e+)\n\n     Returns:\n       (float): strength of evidence for causal relationship\n  '''\n  import numpy as np\n  \n  return (np.log(estimate_likelihood(1,data=data) / \n                 estimate_likelihood(0,data=data)))\n\nNow let‚Äôs see what the model predicts for the four cases we considered earlier.\n\ncausal_support_predictions = [causal_support([2,6,0,8]),\n                              causal_support([4,4,2,6]),\n                              causal_support([6,2,4,4]),\n                              causal_support([8,0,6,2])]\n                              \nlabels = [\"6/8, 8/8\", \"4/8, 6/8\", \"2/8, 4/8\", \"0/8, 2/8\"]\nfig, ax = plt.subplots()\nax.bar(labels, causal_support_predictions)\nax.set_xlabel(\"Data\")\nax.set_ylabel(\"Model prediction\")\n\nplt.show()\n\n\n\n\nThe labels on the x-axis indicate the control condition counts, followed by the experimental condition counts.\nYou can see that the model‚Äôs predictions largely follow the pattern in the data from earlier. The model favors Graph 1 in the two leftmost cases, is essentially uncertain in the third case, and begins to think the evidence favors no causal relationship in the rightmost case.\nThe rest of the Griffiths & Tenenbaum paper shows how causal support is able to capture some subtle aspects of people‚Äôs causal judgments that other models that don‚Äôt incorporate both structure and strength fail to capture."
  },
  {
    "objectID": "02-bayes.html#basic-probability",
    "href": "02-bayes.html#basic-probability",
    "title": "2¬† Bayesian inference",
    "section": "2.1 Basic probability üé≤",
    "text": "2.1 Basic probability üé≤\n\nConditional probability: \\(P(b|a) = \\frac{P(a,b)}{P(a)}\\)\nChain rule: \\(P(a,b) = P(b|a)P(a)\\)\nMarginalization: \\(P(d) = \\sum_h P(d,h) = \\sum_h P(d|h) P(h)\\)\nBayes‚Äôs rule: \\(P(h|d) = \\frac{P(d|h) P(h)}{P(d)} = \\frac{P(d|h) P(h)}{\\sum_h P(d|h) P(h)}\\). \\(P(d|h)\\) is referred to as the likelihood, \\(P(h)\\) is the prior, and \\(P(h|d)\\) is the posterior."
  },
  {
    "objectID": "02-bayes.html#a-motivating-example-sampling-from-a-bag",
    "href": "02-bayes.html#a-motivating-example-sampling-from-a-bag",
    "title": "2¬† Bayesian inference",
    "section": "2.2 A motivating example: Sampling from a bag üëù",
    "text": "2.2 A motivating example: Sampling from a bag üëù\nSuppose you have a bag full of black and red balls. You can‚Äôt see inside the bag and you don‚Äôt know how many black and red balls are inside, but you know that there are nine total balls in the bag.\nYou want to know how many black balls and red balls there are. There are a finite number of hypotheses: {0 black balls, 1 black ball, 2 black balls, ‚Ä¶, 9 black balls}. Let‚Äôs call these hypotheses \\(B_0\\), \\(B_1\\), etc., respectively.\nYou don‚Äôt know which hypothesis is true, but you might have some idea that some hypotheses are more likely than others. It‚Äôs natural to represent your uncertainty with a probability distribution over the possible unknown states that the world could be in ‚Äì in this case, the ten hypotheses. Each hypothesis gets assigned a probability and the probabilities sum to 1.\nFor starters, let‚Äôs assume that you don‚Äôt have any idea which hypotheses are more likely. In other words, you give every hypothesis the same probability: 1/10 = 0.1. This is called a uniform distribution over hypotheses. This distribution is your prior.\nNow suppose you put your hand in the bag and pull out a ball at random. The possible observations are: {black, red}, Let‚Äôs call them \\(B\\) and \\(R\\), respectively. The probability of observing each color depends on which hypothesis is true, i.e., how many balls of each color are in the bag. For instance, if \\(B_0\\) is true (there are 0 black balls in the bag), then the probability of observing a red ball is 1 (\\(P(R|B_0)=1\\)), and the probability of observing a black ball is 0 (\\(P(B|B_0)=0\\)). These expressions that tell us how probable our observations are, given a specific hypothesis, are your likelihoods.\n\n2.2.1 Sampling from the generative model\nNow we have a distribution over hypotheses (a prior), \\(P(h)\\), and a distribution over observations given each hypothesis (a likelihood), \\(P(d|h)\\). These two things allow us to create a generative model, a model for sampling new data.\nHow do we sample from the generative model? Note that which hypothesis \\(h\\) is true does not depend on the data, while the data \\(d\\) depends on which hypothesis is true. Therefore, we can sample from the generative model using the following two-step process:\n\nSample a hypothesis from the prior.\nSample data given the hypothesis, using the likelihood.\n\nLet‚Äôs first create an array with the probability of each hypothesis:\n\nimport numpy as np\nimport random\n\nrandom.seed(2022) # set random seed to get same results every time\n\nh_priors = np.repeat(0.1,10)\nprint(h_priors)\n\n[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n\n\nNow we‚Äôll do the first step: create an array of 10000 hypotheses sampled from the prior:\n\nprior_samples = np.array(random.choices(np.arange(0,10,1),\n                   weights = h_priors, \n                   k = 10000))\n                   \nprint(prior_samples[0:9]) # printing out just a few\n\n[5 4 3 0 7 9 4 6 8]\n\n\nHere, each number corresponds to one hypothesis: 0 corresponds to \\(B_0\\), 1 to \\(B_1\\), and so on. Each sample represents one possible way (a hypothesis) the world could be. Since the prior was uniform (each hypothesis had the same probability), each hypothesis appears about equally often. We can plot all the samples to verify:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme()\nfig, ax = plt.subplots()\nsns.histplot(prior_samples, bins=10)\nax.set_xlabel('Prior sample')\nax.set_ylabel('Number of samples')\n\nText(0, 0.5, 'Number of samples')\n\n\n\n\n\nNow for the next step. For each sample in prior_samples, we want to sample an observation. To do that, let‚Äôs pause for a second and think about the probability of pulling a black ball given that hypothesis \\(B_3\\) is true, for example. This means that there are 3 black balls and 6 red balls in the bag. So the probability of pulling a black ball from the bag at random will be 3/9.\nGeneralizing this idea, we can get the probability of pulling a black ball from the bag by dividing the elements of prior_samples by 9:\n\np_black = prior_samples / 9\nprint(p_black[0:4]) # print out just a few\n\n[0.55555556 0.44444444 0.33333333 0.        ]\n\n\nNow, to complete our generative model, we just need to sample one value for each element of p_black. Each sample represents a draw from a bag.\n\nball_samples = np.random.binomial(n = np.repeat(1,len(p_black)),\n                                  p = p_black)\nprint(ball_samples[0:9])\n\n[0 1 0 0 1 1 1 1 1]\n\n\nball_samples is 1 for black and 0 for red. Once again, let‚Äôs plot all our samples.\n\nfig, ax = plt.subplots()\nsns.histplot(ball_samples, bins=2)\nax.set_xticks(ticks=[0,1])\nax.set_ylabel(\"Number of samples\")\n\nText(0, 0.5, 'Number of samples')\n\n\n\n\n\nYou can think of this plot representing your overall beliefs about the number of red and black balls in the bag, averaged over all possible hypotheses.\nNot surprisingly, we got about equal numbers of red and black balls. This makes sense: We didn‚Äôt have any prior expectations about whether red or black balls were more likely in the bag.\nHow should your beliefs change after you pull a ball out of the bag? That is, how should you respond to evidence?"
  },
  {
    "objectID": "02-bayes.html#bayesian-updating-learning-from-evidence",
    "href": "02-bayes.html#bayesian-updating-learning-from-evidence",
    "title": "2¬† Bayesian inference",
    "section": "2.3 Bayesian updating: Learning from evidence ü§î",
    "text": "2.3 Bayesian updating: Learning from evidence ü§î\nLet‚Äôs apply Bayes‚Äôs rule to see how to optimally incorporate new data into your beliefs.\n\n2.3.1 Applying Bayes‚Äôs rule to the bag case\nSuppose you have a uniform prior distribution over the 10 hypotheses about balls in the bag. Now you pick a ball and it‚Äôs black. Given this observation \\(B\\), how should you change the probabilities you give to each hypothesis?\nIntuitively, you should now give a little bit more probability to those hypotheses that have more black balls than red balls, because those are the hypotheses that make your observations more likely. Moreover, you can definitively exclude hypothesis \\(B_0\\), because your observation would be impossible if \\(B_0\\) were true. Let‚Äôs calculate this with Bayes‚Äôs rule.\nThe prior is the array h_priors defined above. Given that we have observed \\(B\\), the likelihood should tell us, for each hypothesis, the probability of \\(B\\) given that hypothesis. For example, for \\(B_9\\), the likelihood \\(P(B|B_9) = 1\\). For \\(B_8\\), \\(P(B|B_8) = 8/9\\), because 8 of the 9 balls are black.\nGeneralizing this idea, \\(P(B|B_n) = n/9\\). We can therefore compute the likelihoods for all hypotheses in a vector:\n\nlikelihoods = np.arange(0,10,1) / 9\n\nprint(likelihoods)\n\n[0.         0.11111111 0.22222222 0.33333333 0.44444444 0.55555556\n 0.66666667 0.77777778 0.88888889 1.        ]\n\n\nNow suppose we want to find the probability of hypothesis \\(B_5\\) after observing one draw \\(B\\). Let‚Äôs apply Bayes‚Äôs rule:\n\\[P(B_5 | B) = \\frac{P(B|B_5) P(B_5)}{\\sum_h{p(B|h) P(h)}}\\]\nLet‚Äôs compute the parts we need to calculate \\(P(B_5 | B)\\).\n\n# Prior\np_B5 = h_priors[3]\n\n# Likelihood\nlikelihood_B5 = likelihoods[5]\n\n# Data\np_B = sum(likelihoods*h_priors)\n\n# Posterior\np_B5_given_B = p_B5 * likelihood_B5 / p_B\n\n# Print out results\nprint(\"P(B5) = \" + str(p_B5)) # Prior\nprint(\"P(B|B5) = \" + str(likelihood_B5)) # Likelihood\nprint(\"P(B) = \" + str(p_B)) # Data\nprint(\"P(B5|B) = \" + str(p_B5_given_B)) # Posterior\n\nP(B5) = 0.1\nP(B|B5) = 0.5555555555555556\nP(B) = 0.5\nP(B5|B) = 0.11111111111111112\n\n\nLet‚Äôs update the probabilities for all hypotheses in a more compact way.\n\nposteriors = (likelihoods * h_priors) / sum(likelihoods * h_priors)\n\nfor i in range(len(posteriors)):\n  print(\"P(B\" + str(i) + \"|B) = \" + str(posteriors[i]))\n\nP(B0|B) = 0.0\nP(B1|B) = 0.022222222222222223\nP(B2|B) = 0.044444444444444446\nP(B3|B) = 0.06666666666666667\nP(B4|B) = 0.08888888888888889\nP(B5|B) = 0.11111111111111112\nP(B6|B) = 0.13333333333333333\nP(B7|B) = 0.15555555555555556\nP(B8|B) = 0.17777777777777778\nP(B9|B) = 0.2\n\n\nAs expected, Bayes‚Äôs rule says we should increase the probability we assign to hypotheses with more black balls than red balls. Additionally, let‚Äôs double-check that the posterior probabilities sum to 1 (a requirement for a valid probability distribution).\n\nsum(posteriors)\n\n1.0\n\n\nFinally, let‚Äôs plot the posterior probabilities.\n\nhypotheses = ('B0', 'B1', 'B2', 'B3', 'B4',\n              'B5', 'B6', 'B7', 'B8', 'B9')\ny_pos = np.arange(len(hypotheses))\n\nfig, ax = plt.subplots()\nsns.barplot(posteriors, orient=\"y\")\nax.set_yticks(ticks = y_pos, labels=hypotheses)\nax.set_ylabel('Hypothesis')\nax.set_xlabel('Probability')\n\nText(0.5, 0, 'Probability')\n\n\n\n\n\n\n\n2.3.2 How to avoid calculating P(d)\nIn practice, we generally do not need to calculate the \\(P(d)\\) (the denominator in Bayes‚Äôs rule) explicitly. I‚Äôll give you the general idea why in this section.\nFirst, we create an array of prior probabilities, which has as many components as there are hypotheses. We‚Äôll just reuse h_priors. Note that the probabilities sum to 1, as they should because it‚Äôs a probability distribution.\n\nh_priors\n\narray([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\n\n\nNext, we create a likelihood array. When we did calculations above, we only had an array with the likelihoods for a specific observation. However, we would like to have something that encodes the likelihood function for each possible observation given each possible hypothesis, rather than just for a specific observation.\nIn this example, there are two possible observations: \\(B\\) and \\(R\\). We can encode the likelihood as an \\(m \\times n\\) array where \\(m\\) is the number of hypotheses and \\(n\\) is the number of possible observations. In our case: \\(10 \\times 2\\).\n\nlikelihood_array = np.array((np.arange(0,10,1) / 9,\n                             1-(np.arange(0,10,1) / 9))).T\nprint(likelihood_array)\n\n[[0.         1.        ]\n [0.11111111 0.88888889]\n [0.22222222 0.77777778]\n [0.33333333 0.66666667]\n [0.44444444 0.55555556]\n [0.55555556 0.44444444]\n [0.66666667 0.33333333]\n [0.77777778 0.22222222]\n [0.88888889 0.11111111]\n [1.         0.        ]]\n\n\nNow we multiply the prior and likelihoods together (the numerator of Bayes‚Äôs rule) element-wise (first element gets multiplied with first element, second element by second element, etc.):\n\nprior_array = np.array((h_priors, h_priors)).T\nbayes_numerator = likelihood_array * prior_array\n\nprint(bayes_numerator)\n\n[[0.         0.1       ]\n [0.01111111 0.08888889]\n [0.02222222 0.07777778]\n [0.03333333 0.06666667]\n [0.04444444 0.05555556]\n [0.05555556 0.04444444]\n [0.06666667 0.03333333]\n [0.07777778 0.02222222]\n [0.08888889 0.01111111]\n [0.1        0.        ]]\n\n\nFinally, we want a distribution for each column, i.e., a distribution over hypotheses given each observation. Therefore, we sum each column and then divide each element by the sum of its column:\n\nposteriors = bayes_numerator / np.sum(bayes_numerator, axis = 0)\nprint(posteriors)\n\n[[0.         0.2       ]\n [0.02222222 0.17777778]\n [0.04444444 0.15555556]\n [0.06666667 0.13333333]\n [0.08888889 0.11111111]\n [0.11111111 0.08888889]\n [0.13333333 0.06666667]\n [0.15555556 0.04444444]\n [0.17777778 0.02222222]\n [0.2        0.        ]]\n\n\nAnd that gives us the posterior without us having to explicitly calculate the evidence for each observation!\n\n\n\n\n\n\nNote\n\n\n\nThe general idea is this. Because the denominator of Bayes‚Äôs rule, for a fixed observation, is a constant, you can usually get away with computing \\(P(d|h) P(h)\\) for every possible hypothesis \\(h\\) and then ‚Äúnormalize‚Äù the resulting values so that they sum to 1 (remember that they have to in order for it to be a valid probability distribution)."
  },
  {
    "objectID": "02-bayes.html#bayes-exercises",
    "href": "02-bayes.html#bayes-exercises",
    "title": "2¬† Bayesian inference",
    "section": "2.4 Exercises üìù",
    "text": "2.4 Exercises üìù\n\n2.4.1 Taxi cabs\n80% of the taxi cabs in Simpletown are green and 20% are yellow1. A hit-and-run accident happened at night involving a taxi. A witness claimed that the taxi was yellow. After extensive testing, it is determined that the witness can correctly identify the color of a taxi only 75% of the time under conditions like the ones present during the accident. What is the probability that the taxi was yellow?\n\n\n2.4.2 Flipping coins\nYou observe a sequence of coin flips and want to determine if the coin is a trick coin (always comes up heads) or a normal coin. Let \\(P(\\text{heads}) = \\theta\\). Let \\(h_1\\) be the hypothesis that \\(\\theta = 0.5\\) (fair coin). Let \\(h_2\\) be the hypothesis that \\(\\theta = 1\\) (trick coin).\nFor this problem, we will define something called prior odds, which is the ratio of prior probabilities assigned to two hypotheses: \\(\\frac{P(h_1)}{P(h_2)}\\). Because most coins aren‚Äôt trick coins, we assume that \\(\\frac{P(h_1)}{P(h_2)} = 999\\), indicating a very strong (999 to 1) prior probability in favor of fair coins. We can now compute the posterior odds, the ratio of posterior probabilities for the two hypotheses after observing some data \\(d\\): \\(\\frac{P(h_1|d)}{P(h_2|d)}\\).\nCompute the posterior odds after observing the following sequences of coin flips:\n\nHHTHT\nHHHHH\nHHHHHHHHHH"
  },
  {
    "objectID": "02-bayes.html#solutions",
    "href": "02-bayes.html#solutions",
    "title": "2¬† Bayesian inference",
    "section": "2.5 Solutions",
    "text": "2.5 Solutions\n\n2.5.1 Taxi cabs\nLet \\(h_1\\) be the hypothesis that the taxi is yellow. Let \\(h_2\\) be the hypothesis that the taxi is green. Let data \\(d\\) be the witness report that the taxi was yellow. Given the problem statement, \\(P(h_1) = 0.2\\) and \\(P(h_2) = 0.8\\). The witness is only accurate 75% of the time, so \\(P(d|h1) = 0.75\\) (the witness saw a yellow taxi and correctly identified it) and \\(P(d|h2) = 0.25\\) (the witness saw a green taxi but identified it as yellow). Now we apply Bayes‚Äôs rule:\n\\[\\begin{align}\nP(h_1|d) &= \\frac{P(d|h_1) P(h_1)}{P(d)} \\\\\n&= \\frac{P(d|h_1) P(h_1)}{P(d|h_1) P(h_1) + P(d|h_2) P(h_2)} \\\\\n&= \\frac{(0.75) (0.2)}{(0.75)(0.2) + (0.25)(0.8)} \\approx 0.43\n\\end{align}\\]\nBecause yellow cabs are rare (have low prior probability), it is actually more probable that the cab was green, even though the witness is 75% accurate.\n\n\n2.5.2 Flipping coins\n\nHHTHT\n\\[\n\\begin{align}\n\\frac{P(h_1|d)}{P(h_2|d)} &= \\frac{P(d|h_1)}{P(d|h_2)} \\frac{P(h_1)}{P(h_2)} \\\\\n&= \\frac{(1/2)^5}{0} \\times 999 = \\inf\n\\end{align}\n\\] This sequence isn‚Äôt even possible under \\(h_2\\) so we have infinite evidence in favor of \\(h_1\\).\n\n\nHHHHH\n\\[\n\\begin{align}\n\\frac{P(h_1|d)}{P(h_2|d)} &= \\frac{P(d|h_1)}{P(d|h_2)} \\frac{P(h_1)}{P(h_2)} \\\\\n&= \\frac{(1/2)^5}{1^5} \\times 999 = 31.2\n\\end{align}\n\\]\nThis sequence favors \\(h_1\\) by a factor of about 31. Even five heads in a row can‚Äôt overcome our strong prior favoring \\(h_1\\).\n\n\nHHHHHHHHHH\n\\[\n\\begin{align}\n\\frac{P(h_1|d)}{P(h_2|d)} &= \\frac{P(d|h_1)}{P(d|h_2)} \\frac{P(h_1)}{P(h_2)} \\\\\n&= \\frac{(1/2)^{10}}{1^{10}} \\times 999 = 0.98\n\\end{align}\n\\]\nNow the evidence favors \\(h_2\\) (trick coin) just barely."
  },
  {
    "objectID": "03-generalization.html#hormones",
    "href": "03-generalization.html#hormones",
    "title": "3¬† Generalization",
    "section": "3.1 Healthy hormone levels üíâ",
    "text": "3.1 Healthy hormone levels üíâ\nThis example comes from a 2001 paper by Josh Tenenbaum and Thomas Griffiths1\nThe basic problem: You learn the value of a healthy hormone level (say, 60) that varies on a scale from 1 to 100 (integers only). What is the probability that another value (say, 70) is also healthy?\n\n3.1.1 Setting up a model\n\n3.1.1.1 The hypothesis space\nTo start with, we‚Äôll assume that healthy values lie in a contiguous interval. Using the term from the paper, this interval is the consequential region \\(C\\).\nThe hypothesis space consists of all possible consequential regions. For example, [0,100], [10,19], and [44,45], are all valid hypotheses. The full hypothesis space is every valid interval between 0 and 100.\n\n\n3.1.1.2 Prior\nHow much weight should we assign to each hypothesis? We might have reason to favor shorter intervals over longer ones, for example. In the paper, they use an Erlang prior. Alternatively, for simplicity of calculation, we could again assume a uniform prior distribution, placing equal weight on all hypotheses, like we did in the previous chapter. This is tantamount to making no prior assumptions about which intervals are most probable.\n\n\n3.1.1.3 Likelihood\nSuppose you learn that a healthy patient has a hormone level of 60. What was the likelihood of observing this value, assuming we know which hypothesis is correct? That is, what is \\(P(x = 60 | h)\\). It depends on how we assume the patient was chosen.\n\n3.1.1.3.1 Weak vs.¬†strong sampling\nUnder weak sampling, we assume that each observation was sampled from the full range of possibilities, and it was just a coincidence that we happened to get one from the consequential region (a healthy patient). If that‚Äôs true, then the probability of getting any particular value doesn‚Äôt depend on which hypothesis is true:\n\\[\nP(x|h) = \\frac{1}{L}\n\\]\nwhere \\(L\\) is the length of the range of possible values (100 in our case).\nUnder strong sampling, we assume that each observation was specifically chosen as an example of the consequential region \\(C\\). In other words, someone chose a healthy person and tested their hormone levels as an example for you. In this case, the probability of seeing a particular value depends on the size of the region:\n\\[\nP(x|h) = \\begin{cases}\n  \\frac{1}{|h|} & \\text{if } x \\in h \\\\\n  0 & \\text{otherwise}\n  \\end{cases}\n\\] where \\(|h|\\) is the size of \\(h\\), i.e., the number of values contained in \\(h\\). If you have multiple observations \\(X = \\{x_1, x_2, \\ldots, x_n \\}\\), then \\(P(X|h) = (1/|h|)^n\\). This is because we will assume each sample is independent like a coin flip.\nThe result of the strong sampling assumption is the size principle: among hypotheses that include all of the observed examples, those that are smallest will receive higher posterior probability because they will have higher likelihoods.\n\n\n\nSample hypotheses. The thickness of the lines indicates their likelihood, depicting the size principle. Image from Griffiths & Tenenbaum (2001).\n\n\n\n\n\n3.1.1.4 Posterior\nWe can now simply apply Bayes‚Äôs rule to compute the probability of each hypothesis, given an observation (or set of observations).\n\\[\nP(h|X) = \\frac{P(X|h) P(h)}{\\sum_{h_i} P(X|h_i) P(h_i)}\n\\]\n\n\n\n3.1.2 Generalizing\nWe aren‚Äôt quite finished. Remember that we really want to know the probability of some new value \\(y\\) also being a healthy hormone level. But at this point all we have done is assigned a probability to each interval being the consequential region.\nWhat we want to do is essentially a two-step process:\n\nFor each hypothesis \\(h\\), check to see if \\(y\\) is in it.\nIf it is, check how probable it is that \\(h\\) is the consequential region \\(C\\), given our observations \\(X\\).\n\nThis basic idea is sometimes known as hypothesis averaging because we don‚Äôt actually care which hypothesis is the right one, so we‚Äôll just average over all hypotheses, weighted by how probable they are. Specifically, we‚Äôll compute:\n\\[\nP(y \\in C|X) = \\sum_h P(y \\in C | h) P(h | X)\n\\] The second term on the right is what we computed earlier using Bayes‚Äôs rule.\nWhat about the first term? This time, we‚Äôll assume weak sampling because there‚Äôs no reason to assume that this new value \\(y\\) was chosen to be a healthy value or not.\n\n\n3.1.3 Homework 2: Finish the details\n\n\n\n\n\nI haven‚Äôt provided all the details for this model because your assignment is to finish the implementation yourself, run some simulations, and collect a small amount of real data to compare the model to."
  },
  {
    "objectID": "03-generalization.html#the-number-game",
    "href": "03-generalization.html#the-number-game",
    "title": "3¬† Generalization",
    "section": "3.2 The number game üî¢",
    "text": "3.2 The number game üî¢\nIn most domains, requiring concepts to be restricted to contiguous intervals is not realistic. Numbers are one example. Consider the space of possible number concepts you could make up for integers between 1 to 100. In addition to concepts like ‚Äúnumbers between 20 and 50‚Äù, there are many other plausible concepts like ‚Äúmultiples of 10‚Äù, ‚Äúeven numbers‚Äù, or ‚Äúpowers of 3‚Äù.\nConsider the following problem: You are given one or more examples \\(X\\) of numbers that fit some rule and you want to know how probable it is that a new number \\(y\\) also fits the rule.\nThe model we discussed before can be naturally extended to this problem. For the likelihood, we can make the same strong sampling assumption as before.\nThe prior is where things get a little trickier. Intuitively some concepts like ‚Äúeven numbers‚Äù seem more probable even before seeing any examples than concepts like ‚Äúmultiples of 7‚Äù. This now becomes a psychological question: Which rules will people find to be more intuitively plausible? There is no single way to decide this, but we could run a survey to find out: Give people a long list of rules and ask them to judge how intuitively natural they seem. We could then construct a prior probability distribution using this data.\nAlternatively, we could come up with some definition of ‚Äúcomplexity‚Äù in hypotheses and assume that less complex hypotheses will receive higher prior probability.\nOnce we have chosen a prior probability distribution \\(P(h)\\), we can now proceed just as we did before."
  },
  {
    "objectID": "03-generalization.html#inductive-generalizations-about-animal-properties",
    "href": "03-generalization.html#inductive-generalizations-about-animal-properties",
    "title": "3¬† Generalization",
    "section": "3.3 Inductive generalizations about animal properties üê¥",
    "text": "3.3 Inductive generalizations about animal properties üê¥\nNow let‚Äôs consider an even more complex generalization problem, based on a 2002 paper by Neville Sanjana and Josh Tenenbaum2. This is the problem of generalizing properties from a set of example animals to other animals. The paper uses the following example:\nChimps have blicketitis\nSquirrels have blicketitis\n--------------------------\nHorses have blicketitis\nThe way to read this is as follows: The premises state that chimps and squirrels have blicketitis. The conclusion is that horses have blicketitis. The inductive generalization question is how probable is the conclusion given the premises? Intuitively, the conclusion in this example seems more plausible than the conclusion in the following example:\nChimps have blicketitis\nGorillas have blicketitis\n--------------------------\nHorses have blicketitis\nThe interesting psychological question is why is it that some generalizations seem more intuitively plausible than others?\n\n3.3.1 Hypothesis space\nThis problem is conceptually similar to the ones we‚Äôve already been discussing. You want to infer what animals have blicketitis after seeing some examples of animals that blicketitis. The first question to answer is what is the analogue of a consequential region for this problem. Animals don‚Äôt naturally fall on a one-dimensional interval so we‚Äôll need to define a different hypothesis space. One possibility is a hierarchy, which naturally captures the knowledge people have about animals.\n\n3.3.1.1 Clustering\nThe paper first creates a hierarchy of eight animals using similarity data collected from people. Specifically, they asked people to judge how similar all pairs of eight animals were and then calculated the average similarity judgment for each animal.\nThese similarity judgments can be used to construct a tree using a simple clustering algorithm. The algorithm works as follows:\n\nPut all animals in their own cluster.\nWhere there is more than one cluster that hasn‚Äôt been placed in a group, do the following:\n\nIdentify the pair of clusters with the greatest similarity between them.\nGroup those clusters into their own new cluster.\n\n\nThere are several approaches for computing the similarity between two clusters that contain multiple animals. For example, you might use the maximum similarity between any pair of individual animals in the two clusters.\nThe results of this algorithm can be represented as a tree, shown below. Each node in the tree represents a cluster. The hypotheses we will consider will be any combination of 1, 2, or 3 of the clusters determined using the clustering algorithm.\n\n\n\nThe tree of animal species. Image from Sanjana & Tenenbaum (2002).\n\n\n\n\n\n3.3.2 The model\nWe can now define the model. First, let‚Äôs define \\(P(h)\\) where \\(h\\) is a set of clusters. The authors make an assumption analogous to the following:\n\\[\nP(h) \\propto \\frac{1}{\\phi^k}\n\\] where \\(k\\) is the number of clusters in \\(h\\). \\(\\phi\\) is a parameter that we can choose. As long as \\(\\phi &gt; 1\\), \\(P(h)\\) will be smaller for hypotheses consisting of more clusters. This has the effect of assigning higher weight to ‚Äúsimpler‚Äù hypotheses.\nOnce again, we will make the strong sampling assumption for the likelihood. This time, \\(|h|\\) is the number of animal species in \\(h\\) and \\(n\\) is the number of examples in the premises.\nA consequence of both of these assumptions is something like Occam‚Äôs razor which says that the simplest explanation should be preferred. This model will assign 0 likelihood to any hypotheses that don‚Äôt include \\(X\\), thus narrowing the hypothesis space down to just hypotheses that are possible. Of those, it will favor hypotheses with fewer clusters and with fewer animals. In other words, it will favor the simplest hypotheses that are consistent with the examples we‚Äôve seen.\n\n\n3.3.3 Try out the model yourself\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import comb\nimport matplotlib.pyplot as plt\n\n\ndef animalGeneralization(premises, phi=20): \n  \n  # - premises is a list of animal names.\n  # - phi is a number that describes the strength of the simplicity\n  #   bias in the prior (default = 20). This function uses a slightly simpler\n  #   version of the prior than the one in the original paper.\n  # \n  # Plots the generalization values and returns them\n  \n  animals = [\"horse\", \"cow\", \"elephant\", \"rhino\", \"chimp\",\n  \"gorilla\", \"mouse\", \"squirrel\", \"dolphin\", \"seal\"]\n  \n  # We'll hard-code the clusters from the tree structure. Here, each column in the\n  # matrix. Each row is a cluster. A 1 means the animal is present in the cluster\n  animal_clusters = np.array([[1,0,0,0,0,0,0,0,0,0], # The singleton clusters\n                              [0,1,0,0,0,0,0,0,0,0],\n                              [0,0,1,0,0,0,0,0,0,0],\n                              [0,0,0,1,0,0,0,0,0,0],\n                              [0,0,0,0,1,0,0,0,0,0],\n                              [0,0,0,0,0,1,0,0,0,0],\n                              [0,0,0,0,0,0,1,0,0,0],\n                              [0,0,0,0,0,0,0,1,0,0],\n                              [0,0,0,0,0,0,0,0,1,0],\n                              [0,0,0,0,0,0,0,0,0,1],\n                              \n                              [1,1,0,0,0,0,0,0,0,0], # The pair clusters\n                              [0,0,1,1,0,0,0,0,0,0],\n                              [0,0,0,0,1,1,0,0,0,0],\n                              [0,0,0,0,0,0,1,1,0,0],\n                              [0,0,0,0,0,0,0,0,1,1],\n                              \n                              [1,1,1,1,0,0,0,0,0,0], # The bigger clusters\n                              [1,1,1,1,1,1,0,0,0,0],\n                              [1,1,1,1,1,1,1,1,0,0],\n                              [1,1,1,1,1,1,1,1,1,1],\n                              ],\n                              dtype = \"int\")\n  \n  animal_clusters_df = pd.DataFrame(animal_clusters, columns = animals)\n  \n  n_clusters = animal_clusters_df.shape[0]\n  n_animals = len(animals)\n  n_hypotheses = int(sum(comb(n_clusters,np.arange(1,4))))\n  \n  # Initialize a hypothesis space of all possible clusters\n  hypotheses = pd.DataFrame(np.zeros((n_hypotheses, n_animals)), \n  columns = animals)\n  priors = np.zeros(n_hypotheses)\n  \n  \n  # The first order hypotheses are just the 19 clusters defined by the tree\n  hypotheses[0:n_clusters] = animal_clusters_df\n  priors[0:n_clusters] = 1/phi\n  \n  # The second order hypotheses are the unique pairs of clusters in the tree\n  i = n_clusters\n  \n  for a in range(0, n_clusters-1):\n    for b in range(a+1, n_clusters):\n      \n      # Take the logical \"or\" of the two clusters from the tree\n      hypotheses[i:i+1] = (np.array(animal_clusters_df[a:a+1], dtype=\"int\") |\n      np.array(animal_clusters_df[b:b+1], dtype=\"int\"))\n      \n      # Update the prior\n      priors[i] = (1/phi)**2\n      \n      i += 1\n      \n  # The third order hypotheses are the unique triples of clusters in the tree\n  for a in range(0, n_clusters-2):\n    for b in range(a+1, n_clusters-1):\n      for c in range(b+1, n_clusters):\n        \n        # Take the logical \"or\" of the three clusters from the tree\n        hypotheses[i:i+1] = (np.array(animal_clusters_df[a:a+1], dtype=\"int\") |\n        np.array(animal_clusters_df[b:b+1], dtype=\"int\") |\n        np.array(animal_clusters_df[c:c+1], dtype=\"int\"))\n        \n        # Update the prior\n        priors[i] = (1/phi)**3\n        \n        i += 1\n            \n            \n  # Now we need to remove the duplicate hypotheses. For example, there's a\n  # {horse} cluster and a {cow} cluster, and there's also a {horse, cow}\n  # cluster. So in our second order hypotheses, there's no need to include\n  # {horse}+{cow}, because it's already included as a first order\n  # hypothesis.\n  \n  # To solve this problem, we'll remove the duplicate rows from our\n  # hypothesis matrix (data frame).\n  \n  # The pandas functions duplicated and drop_duplicates will handle this for us\n  duplicates = hypotheses.duplicated()\n  duplicate_indices = np.logical_not(duplicates)[np.logical_not(duplicates)].index\n  hypotheses = hypotheses.drop_duplicates(ignore_index=True)\n  \n  priors = priors[duplicate_indices]\n  priors = priors / sum(priors) # normalize the prior prob. distribution\n  n_hypotheses = len(priors) # update number of hypotheses to be more accurate\n  \n  # Create the likelihoods\n  likelihood = pd.DataFrame(np.zeros((n_hypotheses, n_animals)), \n  columns = animals)\n            \n  for i in range(0, n_hypotheses):\n    # Set the likelihood equal to 1 over the number of animal species\n    # in the hypothesis. If the animal isn't in the hypothesis, just ignore\n    # it and leave the likelihood at 0.\n    likelihood.loc[i, hypotheses.loc[i,:]==1] = 1/sum(hypotheses.loc[i,:])\n    \n  # Show the model the premises\n  for p in premises:\n    priors = priors * np.array(likelihood[p])\n      \n  priors = priors / sum(priors)\n  \n  # Now compute the generealization probilities by computing a \n  # matrix multiplication of the belief vector and the hypothesis\n  # matrix\n  generalizations = np.matmul(priors, np.array(hypotheses))\n  \n  \n  # Plot the results\n  fig, ax = plt.subplots()\n  y_pos = np.arange(len(animals))\n  \n  \n  ax.barh(animals, generalizations, align='center')\n  ax.set_xlabel(\"Generalization probability\")\n  ax.set_title(\"Premises: \" + \" \".join(premises))\n  \n  plt.show()\n  \n  return(generalizations)\n\nYou can try out a running version of the model by making a copy of the code here. Here, let‚Äôs look at how the model handles a few specific cases.\nLet‚Äôs start with a single example: horses can get blicketitis.\n\nanimalGeneralization([\"horse\"])\n\n\n\n\narray([1.        , 0.52984834, 0.30628906, 0.30628906, 0.19579428,\n       0.19579428, 0.12893614, 0.12893614, 0.0842413 , 0.0842413 ])\n\n\nHere, we see a standard generalization curve. Now, let‚Äôs add a few more animals.\n\nanimalGeneralization([\"horse\", \"cow\", \"mouse\"])\n\n\n\n\narray([1.        , 1.        , 0.57379051, 0.57379051, 0.47852518,\n       0.47852518, 1.        , 0.60980466, 0.1707609 , 0.1707609 ])\n\n\nNow the model has increased probability for most other animals. This makes sense because there is more reason to think blicketitis might affect lots of different animals.\n\nanimalGeneralization([\"horse\", \"cow\", \"mouse\", \"squirrel\"])\n\n\n\n\narray([1.        , 1.        , 0.64983602, 0.64983602, 0.58531333,\n       0.58531333, 1.        , 1.        , 0.18405475, 0.18405475])\n\n\nAdding a squirrel further supported this idea. But what if we add additional examples of animals we‚Äôve already seen?\n\nanimalGeneralization([\"horse\", \"cow\", \"mouse\", \"squirrel\", \"horse\", \"squirrel\"])\n\n\n\n\narray([1.        , 1.        , 0.32551484, 0.32551484, 0.26938358,\n       0.26938358, 1.        , 1.        , 0.06883892, 0.06883892])\n\n\nNow the probabilities for other animals drop, because it‚Äôs starting to look like maybe this disease only affects the four animals we‚Äôve seen so far.\nTo sum up, when we‚Äôve seen a small number of examples, like a single horse, the model will generally prefer simpler hypotheses. But once we‚Äôve seen more data, it will favor more complex hypotheses (like a group of animals from two separate evolutionary clusters) if the data support it.\nAs one final example, let‚Äôs look at the specific impact of multiple examples of a single animal.\n\nanimalGeneralization([\"gorilla\"])\nanimalGeneralization([\"gorilla\", \"gorilla\"])\nanimalGeneralization([\"gorilla\", \"gorilla\", \"gorilla\"])\n\n\n\n\n\n\n\n\n\n\narray([0.01730482, 0.01730482, 0.01730482, 0.01730482, 0.1259028 ,\n       1.        , 0.01270155, 0.01270155, 0.01111944, 0.01111944])\n\n\nHere, we see that the model becomes increasingly confident that the property is unique to gorillas. This makes intuitive sense and it‚Äôs something that people seem to exhibit in their judgments. But, as they point out in the paper, it‚Äôs not something that non-probabilistic models can easily explain.\n\n\n3.3.4 Results\nThe results in the paper show that this model predicts people‚Äôs judgments quite well, better than alternative models that do not rely on Bayesian inference. These results suggest that the assumptions of the model are very similar to the assumptions that people make when making inductive generalizations.\n\n\n\nComparison between model results and human judgments. Image from Sanjana & Tenenbaum (2002)."
  },
  {
    "objectID": "03-generalization.html#footnotes",
    "href": "03-generalization.html#footnotes",
    "title": "3¬† Generalization",
    "section": "",
    "text": "Tenenbaum, J. B. & Griffiths, T. L. (2001). Generalization, similarity, and Bayesian inference. Behavioral and Brain Sciences, 24, 629-640.‚Ü©Ô∏é\nSanjana, N. & Tenenbaum, J. (2002). Bayesian models of inductive generalization. Advances in Neural Information Processing Systems 15‚Ü©Ô∏é"
  },
  {
    "objectID": "02-bayes.html#footnotes",
    "href": "02-bayes.html#footnotes",
    "title": "2¬† Bayesian inference",
    "section": "",
    "text": "After the protracted and aptly named ‚Äúcolor wars‚Äù of 2113-2114, the previously dominant (and ruthless) orange cabs were wiped from existence. For the purposes of this problem, you may ignore them.‚Ü©Ô∏é"
  }
]