[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Computational Psychology",
    "section": "",
    "text": "Welcome"
  },
  {
    "objectID": "index.html#thanks",
    "href": "index.html#thanks",
    "title": "Introduction to Computational Psychology",
    "section": "Thanks",
    "text": "Thanks\nThis book borrows inspiration and content (especially Chapters 2 and 7) from Fausto Carcassi‚Äôs Introduction to Cognitive Modelling in R book and I am tremendously grateful to him for sharing his book publicly. Hopefully this resource will be equally helpful to others.\nA number of the modeling examples and homework assignments are adapted from code written by Danielle Navarro from previous iterations of her Computational Cognitive Science course with Andy Perfors. I am very grateful to both of them for making all of their materials public (and well documented).\nYour feedback is welcome and encouraged."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Introduction to Computational Psychology",
    "section": "License",
    "text": "License\n\n\n\n\n\nAnyone is free to reuse and adapt this book for their own non-commercial purposes, with attribution. If you do use this book in any way, please tell me about it."
  },
  {
    "objectID": "07-RSA.html",
    "href": "07-RSA.html",
    "title": "Language pragmatics",
    "section": "",
    "text": "When we talk to each other, we often rely on shared assumptions and context to get our points across more quickly. Linguists call this pragmatics. For example, consider the following exchange:\nPerson A didn‚Äôt explicitly ask a question, but Person B understood that A wouldn‚Äôt have said anything at all if they didn‚Äôt need something or weren‚Äôt explaining something they were about to do, so B responded with an answer to the implied question (‚ÄúWhere can I get gas?‚Äù).\nIn this chapter, we‚Äôll see how some aspects of pragmatics can be explained by combining Bayesian inference with two new ideas: information theory and decision theory (or expected utility)."
  },
  {
    "objectID": "07-RSA.html#surprisal",
    "href": "07-RSA.html#surprisal",
    "title": "7¬† Language pragmatics",
    "section": "7.1 Surprisal üòØ",
    "text": "7.1 Surprisal üòØ\nWe‚Äôll focus on one concept from information theory: surprisal. Surprisal captures how unexpected an observation is. Intuitively, the unexpectedness of an observation should depend how probable it is. Namely, the more probable it is, the less unexpected it is.\nWe want a function \\(f(p)\\) that takes a probability and gives us its unexpectedness. Here are some reasonable constraints on \\(f\\):\n\nWhen an observation has probability 0, it is infinitely unexpected: \\(f(0) = \\infty\\).\nWhen an observation has probability 1, it is not unexpected at all: \\(f(1) = 0\\).\nWhen we have two independent observations with probabilities \\(p_1\\) and \\(p_2\\), such that the probability of both occurring is \\(p_1 \\cdot p_2\\), we want the total unexpectedness of observing both to be the sum of the unexpectedness of each observation: \\(f(p_1 \\cdot p_2) = f(p_1) + f(p_2)\\).\n\nA function that satisfies these constraints is:\n\\[\nf(p) = -log(p)\n\\] Let‚Äôs verify.\n\n\\(-log(p) \\rightarrow \\infty\\) as \\(p \\rightarrow 0\\) ‚úÖ\n\\(-log(1) = 0\\) ‚úÖ\n\\(-log(p_1 \\cdot p_2) = -(log(p_1) + log(p_2)) = (-log(p_1))+(-log(p_2))\\) ‚úÖ\n\nHere‚Äôs a plot of this function:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\np = np.linspace(0.001,1,num=500)\nfp = -np.log(p)\n  \nfig, ax = plt.subplots()\nax.plot(p, fp)\nax.set_xlabel(\"p\")\nax.set_ylabel(\"surprisal\")\nplt.show()\n\n\n\n\nWhat does surprisal have to do with language?\nWhen you‚Äôre communicating with someone, surprisal can help you decide what information to give the other person. For example, imagine that you observe a number between 1 and 10 and you want to share it with someone else. You could either say:\n\nI saw an even number.\n\nOr you could say:\n\nI saw two.\n\nWhich would you say? The second one, duh. But why?\nIn formal terms, you‚Äôre trying to send a signal such that the number you observed is not surprising for the other person after receiving your signal. That is, you‚Äôre trying to minimize the surprisal of \\(P(2|\\text{signal})\\).\nExercise: Compute the surprisal of each of these two signals."
  },
  {
    "objectID": "07-RSA.html#rsa-model",
    "href": "07-RSA.html#rsa-model",
    "title": "7¬† Language pragmatics",
    "section": "7.2 The Rational Speech Act model üç™",
    "text": "7.2 The Rational Speech Act model üç™\nLet‚Äôs apply the concept of surprisal to the following example. There were initially two cookies in a cookie jar, you may have eaten some, and you‚Äôre telling your friend how many you had. You can say:\n\n‚ÄúI had some cookies.‚Äù (You ate one or two.)\n‚ÄúI had all the cookies.‚Äù (You ate two.)\n‚ÄúI had no cookies.‚Äù (You ate none.)\nNothing ‚Äì stay silent. (You could have eaten none or any number of cookies.)\n\nLet‚Äôs define this situation using a binary matrix. Each row is a signal (what you say) and each column is a state (how many cookies were eaten).\n\nstate_matrix = np.array([[0,1,1],\n                         [0,0,1],\n                         [1,0,0],\n                         [1,1,1]])\n                         \nsignals = [\"some\", \"all\", \"no\", \"silent\"]\nstates = [\"0\",\"1\",\"2\"]\n\nWe‚Äôll define a function that prints out this matrix as a heat map.\n\ndef print_state_matrix(m, x_labels=None, y_labels=None):\n  fig, ax = plt.subplots()\n  im = ax.imshow(m)\n  \n  if (x_labels and y_labels):\n    ax.set_xticks(np.arange(len(x_labels)), labels=x_labels)\n    ax.set_yticks(np.arange(len(y_labels)), labels=y_labels)\n  \n    for i in range(len(y_labels)):\n      for j in range(len(x_labels)):\n        text = ax.text(j, i, round(m[i, j],2),\n          ha=\"center\", va=\"center\", color=\"w\")\n  \n  fig.tight_layout()\n  plt.show()\n\n# Print out the matrix\nprint_state_matrix(state_matrix, states, signals)\n\n\n\n\nNow let‚Äôs calculate how a literal listener would interpret each of these statements. Each row in the matrix above (one for each signal) will define a distribution over the states. That is, for for state \\(h\\) and signal \\(d\\), we want to compute:\n\\[\nP_{\\text{listener}}(h|d) \\propto P(d|h) P(h)\n\\]\nThe literal listener assumes that, given a state, a signal consistent with that state is chosen at random by the speaker. This is already captured in the matrix above: Signals consistent with a state have a value of 1 in their cells and signals inconsistent have a 0. We will also assume that the listener has a uniform prior over states. Therefore, we can calculate the literal listener‚Äôs probabilities by just normalizing the rows of the matrix.\nWe‚Äôll do this more than once, so let‚Äôs write a function to do it.\n\ndef normalize_rows(m):\n  normalized_m = np.zeros((m.shape))\n  row_num = 0\n  for row in m:\n    normalized_m[row_num,:] = row / sum(row)\n    row_num += 1\n    \n  return(normalized_m)\n\n\nliteral_listener = normalize_rows(state_matrix)\nprint_state_matrix(literal_listener, states, signals)\n\n\n\n\nNow let‚Äôs calculate the pragmatic speaker. The pragmatic speaker chooses a signal given a state. Therefore, this time, the columns of the matrix will be probability distributions.\nUnlike the literal listener, the pragmatic speaker is trying to convey some information, so they will choose a signal with the greatest expected utility for the literal listener. Expected utility in this case is defined as the expected increase in understanding about the true state of the world. Remember, we already have a way of quantifying this: surprisal. So we‚Äôll define utility as negative surprisal of the state given the signal from the point of view of the literal listener: \\(-(-log(P_{\\text{listener}}(h|d)))\\).\n\n# First we apply a softmax decision function\nalpha = 1 # paramater controls how close to maximizing speaker is\npragmatic_speaker = np.exp(np.log(literal_listener)*alpha)\n\nfor col in range(pragmatic_speaker.shape[1]):\n  pragmatic_speaker[:,col] = (pragmatic_speaker[:,col] /\n    sum(pragmatic_speaker[:,col]))\n\nprint_state_matrix(pragmatic_speaker, states, signals)\n\nC:\\Users\\jern\\AppData\\Local\\Temp\\ipykernel_22432\\3850398469.py:3: RuntimeWarning:\n\ndivide by zero encountered in log\n\n\n\n\n\n\nIn the above code, I used something known as a ‚Äúsoftmax‚Äù rather than a true maximizing function. The exponential function captures the idea that the speaker will choose probabilistically, generally choosing options with more utility. This is a fairly common assumption in both psychology and economics because we often don‚Äôt have complete information when making decisions (or when modeling other people‚Äôs decisions), so assuming a pure maximizing function isn‚Äôt always the best choice.\nHowever, I also included a parameter alpha that controls how close to maximizing the speaker is. The larger alpha is, the closer the speaker will get to maximizing utility.\nNow we can calculate the probabilities for the pragmatic listener. This will be similar to the literal listener in that the pragmatic listener receives a signal and computes the probability of each state given a signal. The difference is that the pragmatic listener doesn‚Äôt assume the signals are chosen at random, but that the signals are chosen by the pragmatic speaker above.\nWe can compute the pragmatic listener by normalizing the matrix above along the rows.\n\npragmatic_listener = normalize_rows(pragmatic_speaker)\nprint_state_matrix(pragmatic_listener, states, signals)\n\n\n\n\nThe pragmatic listener correctly infers a scalar implicature, a concept from pragmatics in linguistics. Even though ‚ÄúI had some cookies‚Äù is consistent with eating all of the cookies, a pragmatic listener will infer that if a person says they ate some then they did not eat all (otherwise they would have said so)."
  },
  {
    "objectID": "07-RSA.html#does-this-model-match-human-behavior",
    "href": "07-RSA.html#does-this-model-match-human-behavior",
    "title": "7¬† Language pragmatics",
    "section": "7.3 Does this model match human behavior? üîµ",
    "text": "7.3 Does this model match human behavior? üîµ\nIn a word, yes. One of the first papers to look at this was by Michael Frank and Noah Goodman. They applied this model to the simple task of picking out a shape from a small set and tested the model‚Äôs predictions in a behavioral experiment.\n\n7.3.1 The task\n\nThe speaker: ‚ÄúImagine you are talking to someone and you want to refer to the middle object. What word would you use: blue or circle?‚Äù üü¶ üîµ üü©\nThe listener: ‚ÄúImagine someone is talking to you and uses the word blue to refer to one of these objects. Which object are they talking about?‚Äù üü¶ üîµ üü©\n\nIn the task and experiment, the researchers varied the actual set of objects in a systematic way.\n\n\n7.3.2 The model\nTheir model is virtually identical to the cookie jar model we just worked through. They assume that speakers try to choose words to maximize the listener‚Äôs utility, as measured by surprisal.\nA literal listener in this case would assume that any object that is consistent with the speaker‚Äôs chosen word could have been the one they were referring to.\nWith these assumptions, the researchers derive the probability of the speaker choosing each word (details in the paper) as:\n\\[\nP(w|r_s,C) = \\frac{|w|^{-1}}{\\sum_{w^\\prime \\in W} {|w^\\prime|}^{-1}}\n\\]\n\n\\(w\\): The speaker‚Äôs chosen word.\n\\(r_s\\): The object the speaker meant to refer to.\n\\(C\\): The set of objects.\n\\(|w|\\): The number of objects that \\(w\\) could apply to.\n\\(W\\): The set of words that apply to the object that the speaker meant to refer to.\n\nRegarding \\(W\\), imagine that the speaker wanted to refer to the blue circle. In that case, \\(W = \\{ \\text{blue}, \\text{circle} \\}\\), because either word could apply to the object.\nAccording to this model, speakers will tend to choose words that more uniquely identify an object in a set. For example, in the set above, because blue applies to two objects (blue square and blue circle), but circle only applies to one, circle will get higher probability.\n\n\n7.3.3 Homework 4: Implement the model\n\n\n\n\n\nI haven‚Äôt provided all the details for this model because your assignment is to finish the implementation yourself, run some simulations, and collect a small amount of real data to compare the model to."
  },
  {
    "objectID": "02-bayes.html",
    "href": "02-bayes.html",
    "title": "Bayesian inference",
    "section": "",
    "text": "This chapter reviews some basic probability and Bayesian inference. You might be asking yourself: What does this have to do with psychology? The answer becomes clear when you recognize that most of what we do when we‚Äôre making sense of the world is drawing inferences. When you see an ambiguous image, is it a rabbit or a duck? When someone mumbles something, did they say ‚Äúhello‚Äù or ‚Äúgo to hell‚Äù? When you take a pill and your headache goes away, did the pill eliminate your headache or did the headache go away on its own?\nIn all of these examples, there is more than one hypothesis about what we observed. Probability and Bayesian inference provide the tools for optimally determining how probable these different hypotheses are. One of the claims of this book is that when people are making inferences in situations like these, their inferences are often well predicted by the optimal inferences dictated by probability theory."
  },
  {
    "objectID": "02-bayes.html#basic-probability",
    "href": "02-bayes.html#basic-probability",
    "title": "2¬† Bayesian inference",
    "section": "2.1 Basic probability üé≤",
    "text": "2.1 Basic probability üé≤\n\nConditional probability: \\(P(b|a) = \\frac{P(a,b)}{P(a)}\\)\nChain rule: \\(P(a,b) = P(b|a)P(a)\\)\nMarginalization: \\(P(d) = \\sum_h P(d,h) = \\sum_h P(d|h) P(h)\\)\nBayes‚Äôs rule: \\(P(h|d) = \\frac{P(d|h) P(h)}{P(d)} = \\frac{P(d|h) P(h)}{\\sum_h P(d|h) P(h)}\\). \\(P(d|h)\\) is referred to as the likelihood, \\(P(h)\\) is the prior, and \\(P(h|d)\\) is the posterior."
  },
  {
    "objectID": "02-bayes.html#a-motivating-example-sampling-from-a-bag",
    "href": "02-bayes.html#a-motivating-example-sampling-from-a-bag",
    "title": "2¬† Bayesian inference",
    "section": "2.2 A motivating example: Sampling from a bag üëù",
    "text": "2.2 A motivating example: Sampling from a bag üëù\nSuppose you have a bag full of black and red balls. You can‚Äôt see inside the bag and you don‚Äôt know how many black and red balls are inside, but you know that there are nine total balls in the bag.\nYou want to know how many black balls and red balls there are. There are a finite number of hypotheses: {0 black balls, 1 black ball, 2 black balls, ‚Ä¶, 9 black balls}. Let‚Äôs call these hypotheses \\(B_0\\), \\(B_1\\), etc., respectively.\nYou don‚Äôt know which hypothesis is true, but you might have some idea which hypotheses are more likely than others. Therefore, it is natural to represent your uncertainty with a probability distribution over the possible unknown states that the world could be in ‚Äì in this case, the 10 hypotheses. Each hypothesis gets assigned a probability, and the probabilities sum to 1.\nFor simplicity, let‚Äôs assume that you don‚Äôt have any idea which hypotheses are more likely. In other words, you give every hypothesis the same probability: 1/10 = 0.1. This is also called a uniform distribution over hypotheses. This distribution is your prior.\nNow suppose you put your hand in the bag and pull out a ball at random. The possible observations are: {black, red}, Let‚Äôs call them \\(B\\) and \\(R\\), respectively. The probability of observing each color depends on which hypothesis is true, i.e., how many balls of each color are in the bag. For instance, if \\(B_0\\) is true (there are 0 black balls in the bag), then the probability of observing a red ball is 1 (\\(P(R|B_0)=1\\)), and the probability of observing a black ball is 0 (\\(P(B|B_0)=0\\)). These expressions that tell us how probable our observations are, given a specific hypothesis, are your likelihoods.\n\n2.2.1 Sampling from the generative model\nNow we have a distribution over hypotheses (a prior), \\(P(h)\\), and a distribution over observations given each hypothesis (a likelihood), \\(P(d|h)\\). These two things allow us to create a generative model, a model for sampling new data.\nHow do we sample from the generative model? Note that which hypothesis \\(h\\) is true does not depend on the data, while the data \\(d\\) depends on which hypothesis is true. Therefore, we can sample from the generative model using the following two-step process:\n\nSample a hypothesis from the prior.\nSample data given the hypothesis, using the likelihood.\n\nLet‚Äôs first create a vector with the probability of each hypothesis:\n\nimport numpy as np\nimport random\n\nrandom.seed(2022) # set random seed to get same results every time\n\nh_priors = np.repeat(0.1,10)\nprint(h_priors)\n\n[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n\n\nNow we‚Äôll do the first step: create a vector of 10000 hypotheses sampled from the prior:\n\nprior_samples = np.array(random.choices(np.arange(0,10,1),\n                   weights = h_priors, \n                   k = 10000))\n                   \nprint(prior_samples[0:9]) # printing out just a few\n\n[5 4 3 0 7 9 4 6 8]\n\n\nHere, each number corresponds to one hypothesis: 0 corresponds to \\(B_0\\), 1 to \\(B_1\\), and so on. Each sample represents one possible way (a hypothesis) the world could be. Since the prior was uniform (each hypothesis had the same probability), each hypothesis appears about equally often. We can plot all the samples to verify:\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nn, bins, patches = ax.hist(prior_samples, bins=10)\nax.set_xlabel('Prior sample')\nax.set_ylabel('Number of samples')\n\nText(0, 0.5, 'Number of samples')\n\n\n\n\n\nNow for the next step. For each sample in prior_samples, we want to sample an observation. To do that, let‚Äôs pause for a second and think about the probability of pulling a black ball given that hypothesis \\(B_3\\) is true, for example. This means that there are 3 black balls and 6 red balls in the bag. So the probability of pulling a black ball from the bag at random will be 3/9.\nGeneralizing this idea, we can get the probability of pulling a black ball from the bag by dividing the elements of prior_samples by 9:\n\np_black = prior_samples / 9\nprint(p_black[0:4]) # print out just a few\n\n[0.55555556 0.44444444 0.33333333 0.        ]\n\n\nNow, to complete our generative model, we just need to sample one value for each element of p_black. Each sample represents a draw from a bag.\n\nball_samples = np.random.binomial(n = np.repeat(1,len(p_black)),\n                                  p = p_black)\nprint(ball_samples[0:9])\n\n[1 0 1 0 1 1 0 1 1]\n\n\nball_samples is 1 for black and 0 for red. Once again, let‚Äôs plot all our samples.\n\nfig, ax = plt.subplots()\nn, bins, patches = ax.hist(ball_samples, bins=2)\nax.set_xticks([0,1])\nax.set_ylabel('Number of samples')\n\nText(0, 0.5, 'Number of samples')\n\n\n\n\n\nYou can think of this plot representing our overall beliefs about the number of red and black balls in the bag, averaged over all possible hypotheses.\nNot surprisingly, we got about equal numbers of red and black balls. This makes sense: We didn‚Äôt have any prior expectations about whether red or black balls were more likely in the bag.\nHow should our beliefs change after we pull a ball out of the bag? That is, how should we respond to evidence?"
  },
  {
    "objectID": "02-bayes.html#bayesian-updating-learning-from-evidence",
    "href": "02-bayes.html#bayesian-updating-learning-from-evidence",
    "title": "2¬† Bayesian inference",
    "section": "2.3 Bayesian updating: Learning from evidence ü§î",
    "text": "2.3 Bayesian updating: Learning from evidence ü§î\nLet‚Äôs apply Bayes‚Äôs rule to see how to optimally incorporate new data into your beliefs.\n\n2.3.1 Applying Bayes‚Äôs rule to the bag case\nSuppose you have a uniform prior distribution over the 10 hypotheses about balls in the bag. Now you pick a ball and it‚Äôs black. Given this observation \\(B\\), how should you change the probabilities you give to each hypothesis?\nIntuitively, you should now give a little bit more probability to those hypotheses that have more black balls than red balls, because those are the hypotheses that make your observations more likely. Moreover, you can safely exclude hypothesis \\(B_0\\), because your observation would be impossible if \\(B_0\\) were true. Let‚Äôs calculate this with Bayes‚Äôs rule.\nThe prior is the vector h_priors defined above. Given that we have observed \\(B\\), the likelihood should tell us, for each hypothesis, the probability of \\(B\\) given that hypothesis. For example, for \\(B_9\\), the likelihood \\(P(B|B_9) = 1\\). For \\(B_8\\), \\(P(B|B_8) = 8/9\\), because 8 of the 9 balls are black.\nGeneralizing this idea, \\(P(B|B_n) = n/9\\). We can therefore compute the likelihoods for all hypotheses in a vector:\n\nlikelihoods = np.arange(0,10,1) / 9\n\nprint(likelihoods)\n\n[0.         0.11111111 0.22222222 0.33333333 0.44444444 0.55555556\n 0.66666667 0.77777778 0.88888889 1.        ]\n\n\nNow suppose we want to find the probability of hypothesis \\(B_5\\) after observing one draw \\(B\\). Let‚Äôs apply Bayes‚Äôs rule:\n\\[P(B_5 | B) = \\frac{P(B|B_5) P(B_5)}{\\sum_h{p(B|h) P(h)}}\\]\nLet‚Äôs compute the parts we need to calculate \\(P(B_5 | B)\\).\n\n# Prior\np_B5 = h_priors[3]\n\n# Likelihood\nlikelihood_B5 = likelihoods[5]\n\n# Data\np_B = sum(likelihoods*h_priors)\n\n# Posterior\np_B5_given_B = p_B5 * likelihood_B5 / p_B\n\n# Print out results\nprint(\"P(B5) = \" + str(p_B5)) # Prior\nprint(\"P(B|B5) = \" + str(likelihood_B5)) # Likelihood\nprint(\"P(B) = \" + str(p_B)) # Data\nprint(\"P(B5|B) = \" + str(p_B5_given_B)) # Posterior\n\nP(B5) = 0.1\nP(B|B5) = 0.5555555555555556\nP(B) = 0.5\nP(B5|B) = 0.11111111111111112\n\n\nLet‚Äôs update the probabilities for all hypotheses in a more compact way.\n\nposteriors = (likelihoods * h_priors) / sum(likelihoods * h_priors)\n\nfor i in range(len(posteriors)):\n  print(\"P(B\" + str(i) + \"|B) = \" + str(posteriors[i]))\n\nP(B0|B) = 0.0\nP(B1|B) = 0.022222222222222223\nP(B2|B) = 0.044444444444444446\nP(B3|B) = 0.06666666666666667\nP(B4|B) = 0.08888888888888889\nP(B5|B) = 0.11111111111111112\nP(B6|B) = 0.13333333333333333\nP(B7|B) = 0.15555555555555556\nP(B8|B) = 0.17777777777777778\nP(B9|B) = 0.2\n\n\nAs expected, Bayes‚Äôs rule says we should increase the probability we assign to hypotheses with more black balls than red balls. Additionally, let‚Äôs double-check that the posterior probabilities sum to 1 (a requirement for a valid probability distribution).\n\nsum(posteriors)\n\n1.0\n\n\nFinally, let‚Äôs plot the posterior probabilities.\n\nfig, ax = plt.subplots()\nhypotheses = ('B0', 'B1', 'B2', 'B3', 'B4',\n              'B5', 'B6', 'B7', 'B8', 'B9')\ny_pos = np.arange(len(hypotheses))\n\nax.barh(y_pos, posteriors, align='center')\nax.set_yticks(y_pos)\nax.set_yticklabels(hypotheses)\nax.set_xlabel('Probability')\nax.set_ylabel('Hypothesis')\n\nText(0, 0.5, 'Hypothesis')\n\n\n\n\n\n\n\n2.3.2 How to avoid calculating P(d)\nIn practice, we generally do not need to calculate the \\(P(d)\\) (the denominator in Bayes‚Äôs rule) explicitly. I‚Äôll give you the general idea why in this section.\nFirst, we create a vector of prior probabilities, which has as many components as there are hypotheses. We‚Äôll just reuse h_priors. Note that the probabilities sum to 1, as they should because it‚Äôs a probability distribution.\n\nh_priors\n\narray([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\n\n\nNext, we create a likelihood array. When we did calculations above, we only had a vector with the likelihoods for a specific observation. However, we would like to have something that encodes the likelihood function for each possible observation given each possible hypothesis, rather than just for a specific observation.\nIn this example, there are two possible observations: \\(B\\) and \\(R\\). We can encode the likelihood as an \\(m \\times n\\) array where \\(m\\) is the number of hypotheses and \\(n\\) is the number of possible observations. In our case: \\(10 \\times 2\\).\n\nlikelihood_array = np.array((np.arange(0,10,1) / 9,\n                             1-(np.arange(0,10,1) / 9))).T\nprint(likelihood_array)\n\n[[0.         1.        ]\n [0.11111111 0.88888889]\n [0.22222222 0.77777778]\n [0.33333333 0.66666667]\n [0.44444444 0.55555556]\n [0.55555556 0.44444444]\n [0.66666667 0.33333333]\n [0.77777778 0.22222222]\n [0.88888889 0.11111111]\n [1.         0.        ]]\n\n\nNow we multiply the prior and likelihoods together (the numerator of Bayes‚Äôs rule) element-wise (first element gets multiplied with first element, second element by second element, etc.):\n\nprior_array = np.array((h_priors, h_priors)).T\nbayes_numerator = likelihood_array * prior_array\n\nprint(bayes_numerator)\n\n[[0.         0.1       ]\n [0.01111111 0.08888889]\n [0.02222222 0.07777778]\n [0.03333333 0.06666667]\n [0.04444444 0.05555556]\n [0.05555556 0.04444444]\n [0.06666667 0.03333333]\n [0.07777778 0.02222222]\n [0.08888889 0.01111111]\n [0.1        0.        ]]\n\n\nFinally, we want a distribution for each column, i.e., a distribution over hypotheses given each observation. Therefore, we sum each column and then divide each element by the sum of its column:\n\nposteriors = bayes_numerator / np.sum(bayes_numerator, axis = 0)\nprint(posteriors)\n\n[[0.         0.2       ]\n [0.02222222 0.17777778]\n [0.04444444 0.15555556]\n [0.06666667 0.13333333]\n [0.08888889 0.11111111]\n [0.11111111 0.08888889]\n [0.13333333 0.06666667]\n [0.15555556 0.04444444]\n [0.17777778 0.02222222]\n [0.2        0.        ]]\n\n\nAnd that gives us the posterior without us having to explicitly calculate the evidence for each observation!\n\n\n\n\n\n\nNote\n\n\n\nThe general idea is this. Because the denominator of Bayes‚Äôs rule, for a fixed observation, is a constant, you can usually get away with computing \\(P(d|h) P(h)\\) for every possible hypothesis \\(h\\) and then ‚Äúnormalize‚Äù the resulting values so that they sum to 1 (remember that they have to in order for it to be a valid probability distribution)."
  },
  {
    "objectID": "02-bayes.html#bayes-exercises",
    "href": "02-bayes.html#bayes-exercises",
    "title": "2¬† Bayesian inference",
    "section": "2.4 Exercises üìù",
    "text": "2.4 Exercises üìù\n\n2.4.1 Taxi cabs\n80% of the taxi cabs in Simpletown are green and 20% are yellow. An hit-and-run accident happened at night involving a taxi. A witness claimed that the taxi was yellow. After extensive testing, it is determined that the witness can correctly identify the color of a taxi only 75% of the time under conditions like the ones present during the accident. What is the probability that the taxi was yellow?\n\n\n2.4.2 Flipping coins\nYou observe a sequence of coin flips and want to determine if the coin is a trick coin (always comes up heads) or a normal coin. Let \\(P(\\text{heads}) = \\theta\\). Let \\(h_1\\) be the hypothesis that \\(\\theta = 0.5\\) (fair coin). Let \\(h_2\\) be the hypothesis that \\(\\theta = 1\\) (trick coin).\nFor this problem, we will define something called prior odds, which is the ratio of prior probabilities assigned to two hypotheses: \\(\\frac{P(h_1)}{P(h_2)}\\). Because most coins aren‚Äôt trick coins, we assume that \\(\\frac{P(h_1)}{P(h_2)} = 999\\), indicating a very strong (999 to 1) prior probability in favor of fair coins. We can now compute the posterior odds, the ratio of posterior probabilities for the two hypotheses after observing some data \\(d\\): \\(\\frac{P(h_1|d)}{P(h_2|d)}\\).\nCompute the posterior odds after observing the following sequences of coin flips:\n\nHHTHT\nHHHHH\nHHHHHHHHHH"
  },
  {
    "objectID": "02-bayes.html#solutions",
    "href": "02-bayes.html#solutions",
    "title": "2¬† Bayesian inference",
    "section": "2.5 Solutions",
    "text": "2.5 Solutions\n\n2.5.1 Taxi cabs\nLet \\(h_1\\) be the hypothesis that the taxi is yellow. Let \\(h_2\\) be the hypothesis that the taxi is green. Let data \\(d\\) be the witness report that the taxi was yellow. Given the problem statement, \\(P(h_1) = 0.2\\) and \\(P(h_2) = 0.8\\). The witness is only accurate 75% of the time, so \\(P(d|h1) = 0.75\\) (the witness saw a yellow taxi and correctly identified it) and \\(P(d|h2) = 0.25\\) (the witness saw a green taxi but identified it as yellow). Now we apply Bayes‚Äôs rule:\n\\[\\begin{align}\nP(h_1|d) &= \\frac{P(d|h_1) P(h_1)}{P(d)} \\\\\n&= \\frac{P(d|h_1) P(h_1)}{P(d|h_1) P(h_1) + P(d|h_2) P(h_2)} \\\\\n&= \\frac{(0.75) (0.2)}{(0.75)(0.2) + (0.25)(0.8)} \\approx 0.43\n\\end{align}\\]\nBecause yellow cabs are rare (have low prior probability), it is actually more probable that the cab was green, even though the witness is 75% accurate.\n\n\n2.5.2 Flipping coins\n\nHHTHT\n\\[\n\\begin{align}\n\\frac{P(h_1|d)}{P(h_2|d)} &= \\frac{P(d|h_1)}{P(d|h_2)} \\frac{P(h_1)}{P(h_2)} \\\\\n&= \\frac{(1/2)^5}{0} \\times 999 = \\inf\n\\end{align}\n\\] This sequence isn‚Äôt even possible under \\(h_2\\) so we have infinite evidence in favor of \\(h_1\\).\n\n\nHHHHH\n\\[\n\\begin{align}\n\\frac{P(h_1|d)}{P(h_2|d)} &= \\frac{P(d|h_1)}{P(d|h_2)} \\frac{P(h_1)}{P(h_2)} \\\\\n&= \\frac{(1/2)^5}{1^5} \\times 999 = 31.2\n\\end{align}\n\\]\nThis sequence favors \\(h_1\\) by a factor of about 31. Even five heads in a row can‚Äôt overcome our strong prior favoring \\(h_1\\).\n\n\nHHHHHHHHHH\n\\[\n\\begin{align}\n\\frac{P(h_1|d)}{P(h_2|d)} &= \\frac{P(d|h_1)}{P(d|h_2)} \\frac{P(h_1)}{P(h_2)} \\\\\n&= \\frac{(1/2)^{10}}{1^{10}} \\times 999 = 0.98\n\\end{align}\n\\]\nNow the evidence favors \\(h_2\\) (trick coin) just barely."
  },
  {
    "objectID": "10-causal-inference.html",
    "href": "10-causal-inference.html",
    "title": "Causal inference",
    "section": "",
    "text": "The world is full of weird and interesting relationships. Like this one.\nWhich of these weird relationships are causal relationships and how can we tell? (Does cutting back on margarine reduce your chances of divorce? Probably not.)\nCausal inference is, in fact, a major area of research in statistics and machine learning. But we‚Äôll just focus on the question of how people decide what causes what.\nWe‚Äôll use a computational framework for making optimal probabilistic causal judgments called Bayesian networks, or Bayes nets for short. Comparing people‚Äôs judgments to Bayes net predictions allows us to see how optimal (or not) people are. Additionally, as we‚Äôll see, Bayes nets are well suited for intervention, which is one way that people learn about causes."
  },
  {
    "objectID": "10-causal-inference.html#bayes-nets",
    "href": "10-causal-inference.html#bayes-nets",
    "title": "10¬† Causal inference",
    "section": "10.1 Bayes nets ‚û°Ô∏è",
    "text": "10.1 Bayes nets ‚û°Ô∏è\nA Bayes net is a graph that describes the dependencies between all the variables in a situation.\nFor example, let‚Äôs make a Bayes net for the problem in Chapter 5 of inferring the bias of a coin. In that problem, there were three key variables: the bias \\(\\theta\\), the total number of flips \\(n\\), and the number of heads \\(k\\). We can represent this as a Bayes net.\n\n\n\nBayes net representation of the coin bias problem.\n\n\nIn Bayes nets, shaded notes represent variables that are known or observed, and unshaded nodes represent variables that are unknown. We know how many times the coin was flipped and came up heads, but we don‚Äôt directly know the bias \\(\\theta\\).\nWe could extend this Bayes net to capture the generalization problem of predicting the outcome of the next coin flip \\(x\\).\n\n\n\nBayes net representation of the biased coin generalization problem.\n\n\nA complete Bayes net also specifies a probability distribution for each variable. In the example above, \\(k\\) is a function of \\(\\theta\\) and \\(n\\). As we learned in Chapter 5, this is a Binomial distribution. We also need to specify a prior probability over any unknown variables, like \\(\\theta\\). Previously we assumed it was distributed according to a Beta distribution. \\(x\\) is just a single coin flip ‚Äì it‚Äôs a special case of a Binomial distribution called a Bernoulli distribution in which \\(n = 1\\). To sum up:\n\n\\(\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\)\n\\(k \\sim \\text{Binomial}(n,\\theta)\\)\n\\(x \\sim \\text{Binomial}(n=1, \\theta)\\)"
  },
  {
    "objectID": "10-causal-inference.html#causal-intervention",
    "href": "10-causal-inference.html#causal-intervention",
    "title": "10¬† Causal inference",
    "section": "10.2 Causal intervention ü™ö",
    "text": "10.2 Causal intervention ü™ö\nConsider the following two Bayes nets.\n\n\n\nTwo Bayes nets with three variables.\n\n\nTo be concrete, let‚Äôs say that variables \\(s\\) and \\(x\\) represent levels of hormones sonin and xanthan, respectively. Variable \\(z\\) is an unknown variable.\nThe common cause network is so-called because sonin (\\(s\\)) and xanthan (\\(x\\)) are both causally dependent on \\(z\\). The chain network is so-called because all the variables form a causal chain from \\(s\\) to \\(x\\) to \\(z\\). (Note the directions of the arrows in the two Bayes nets.)\nLet‚Äôs see what kind of data these Bayes nets produce. Let‚Äôs assume that each root node of a network (\\(z\\) in the common cause, \\(x\\) in the chain) follows a normal distribution with mean 0 and SD 1. Each link in a network follows a normal distribution with mean equal to the value of its parent node and SD 1.\n\nimport numpy as np\n\nn_samples = 10000\n\n# Common cause\nz_mu = 0\nsd = 1\n\nz_samples_cc = np.zeros(n_samples)\ns_samples_cc = np.zeros(n_samples)\nx_samples_cc = np.zeros(n_samples)\n\nfor i in range(n_samples):\n  z_samples_cc[i] = np.random.normal(z_mu, sd)\n  s_samples_cc[i] = np.random.normal(z_samples_cc[i], sd)\n  x_samples_cc[i] = np.random.normal(s_samples_cc[i], sd)\n\n# Chain\nx_mu = 0\n\nz_samples_chain = np.zeros(n_samples)\ns_samples_chain = np.zeros(n_samples)\nx_samples_chain = np.zeros(n_samples)\n\nfor i in range(n_samples):\n  x_samples_chain[i] = np.random.normal(x_mu, sd)\n  z_samples_chain[i] = np.random.normal(x_samples_chain[i], sd)\n  s_samples_chain[i] = np.random.normal(z_samples_chain[i], sd)\n\nBecause \\(z\\) represents an unknown variable, let‚Äôs plot just \\(s\\) and \\(x\\).\n\nimport matplotlib.pyplot as plt\n\nfig, (ax1, ax2) = plt.subplots(1,2)\nax1.scatter(s_samples_cc, x_samples_cc, alpha = 0.1)\nax1.set_xlabel(\"s\")\nax1.set_ylabel(\"x\")\nax1.set_title(\"Common cause\")\n\nax2.scatter(s_samples_chain, x_samples_chain, alpha = 0.1)\nax2.set_xlabel(\"s\")\nax2.set_title(\"Chain\")\n\nplt.show()\n\n\n\n\nClearly, the data isn‚Äôt identical in the two cases, but data generated by both Bayes nets results in a strong positive correlation between \\(s\\) and \\(x\\).\nImagine you didn‚Äôt know how this data was generated and you just got one of these plots. Could you use it to tell whether it was produced by a common cause structure or a chain structure?\nSorry, but no. üòî Just knowing the data are positively correlated doesn‚Äôt give you enough information to figure out how \\(s\\) and \\(x\\) are causally related.\nBut what if you could manipulate the variables? That is, what if you could intervene on sonin levels and see what effect it had on xanthan levels?\n\nIf you increase the sonin levels üìà and the xanthan levels also increase üìà, then the causal structure must be a chain.\nIf you increase the sonin levels üìà and the xanthan levels don‚Äôt change ‚ùå, then the causal structure can‚Äôt be a chain (and therefore must be a common cause, because that‚Äôs the only other option we‚Äôre considering).\n\n\n10.2.1 Graph surgery\nThis intuition can be illustrated visually on the Bayes nets by performing ‚Äúsurgery‚Äù on the graphs. It works like this:\n\nRemove all incoming connections to the variable you‚Äôre intervening on.\nIf there‚Äôs still a path between the variable you intervened on and another variable, then you should still expect those variables to be related.\n\nLet‚Äôs apply this idea to our common cause and chain Bayes nets.\n\n\n\nGraph surgery applied to the common cause and chain Bayes nets. The invervention is indicated by the red arrow.\n\n\nAfter intervening on sonin levels (\\(s\\)), we remove the connection to \\(s\\) in the common cause network, but no connections in the chain network. The resulting Bayes nets show why we should expect to see a resulting change in xanthan for the chain, but not the common cause.\n\n\n10.2.2 Do people intuitively understand the logic of casual intervention?\nA study by Michael Waldmann and York Hagmayer presented people with either the common cause or the chain structure. They were told that sonin and xanthan were hormone levels in chimps and they got some example data that allowed them to learn that the hormone levels were positively correlated.\nThen they were either assigned to a doing or seeing condition. People in the doing condition were asked to imagine that 20 chimps had their sonin levels raised (or lowered). They then predicted how many of the chimps would have elevated xanthan levels. People in the seeing condition got essentially the same information but just learned that the chimps‚Äô sonin levels were high (not that it had been intentionally raised).\nAverage results and model predictions are below.\n\n\n\nResults from Waldmann & Hagmayer (2005), Experiment 2.\n\n\nPeople‚Äôs judgments mostly followed those of the Bayes net model predictions. In the common cause case, when the sonin levels are artificially raised, the relationship between sonin and xanthan is decoupled, so the model reverts to a base rate prediction about xanthan levels (and so did people, for the most part). But when just observing elevated sonin levels, the model should expect the positive relationship to hold.\nIn the chain case, the predictions are the same for seeing or doing, because intervening doesn‚Äôt change anything about the relationship between sonin and xanthan. People‚Äôs judgments indicate that they understood this."
  },
  {
    "objectID": "10-causal-inference.html#structure-strength",
    "href": "10-causal-inference.html#structure-strength",
    "title": "10¬† Causal inference",
    "section": "10.3 Causal structure and strength üèóüí™",
    "text": "10.3 Causal structure and strength üèóüí™\nBayes nets can also account for how people judge the strength of evidence for a causal relationship after seeing some data. This was the idea that Tom Griffiths and Josh Tenenbaum explored in a 2005 computational study.\nHere‚Äôs the basic problem they considered. Suppose researchers perform an experiment with rats to test whether a drug causes a gene to be expressed. A control group of rats doesn‚Äôt get the injection and the experimental group does. They record the number of rats in each group that express the gene.\nHere are some possible results from an experiment with 8 rats in each group. The table shows how many rats in each group expressed the gene.\n\n\n\nControl üêÄ\nExperimental üêÄ\n\n\n\n\n6/8 üß¨\n8/8 üß¨\n\n\n4/8 üß¨\n6/8 üß¨\n\n\n2/8 üß¨\n4/8 üß¨\n\n\n0/8 üß¨\n2/8 üß¨\n\n\n\nIn each of these hypothetical experiments, how strongly would you say that the drug causes the gene to be expressed?\nThese are a few of the cases that were included in an experiment conducted by Marc Buehner and Patricia Cheng. Here‚Äôs the full set of averaged human results.\n\n\n\nResults from Buehner & Cheng (1997), Experiment 1B, reproduced by Griffiths & Tenenbaum (2005). p(e+|c+) is the probability of the effect being present (e.g., the gene being expressed) given that the cause is present (e.g., the drug was administered). p(e+|c-) means the probability of the effect being present given that the cause is absent.\n\n\nFocusing just on the cases in the table, on average, people judged that the drug was less likely to have a causal effect on the gene as the total number of rats expressing the gene decreased, even when the difference in number of rats expressing the gene between conditions was held constant.\n\n10.3.1 The causal support model\nMaybe people reason about these problems by performing a kind of model selection between the two Bayes nets below.\n\n\n\nCausal inference as model selection. In one model, there is a connection from the potential cause to the effect; in the other, the two variables are unconnected. (Image from Griffiths & Tenenbaum (2005).)\n\n\nThese Bayes nets each have three variables: an effect \\(E\\), a cause \\(C\\), and a background cause \\(B\\). For our problem, the effect and cause refer to the gene and the drug. The inclusion of the background cause is to account for unknown factors that might cause the gene to be expressed without the drug.\nThe problem people are faced with is deciding which of these two models is best supported by the data \\(D\\) ‚Äì the number of times the effect occurred with and without the potential cause.\nThis can be done with Bayesian inference:\n\\[\nP(\\text{Graph } i) \\propto P(D|\\text{Graph } i) P(\\text{Graph } i)\n\\]\nBecause there are only two possible networks, we can compute the relative evidence for one Bayes net over the other as a ratio. We can then take the log of the expression to simplify it:\n\\[\n\\log \\frac{P(\\text{Graph } 1|D)}{P(\\text{Graph } 0|D)} = \\log \\frac{P(D | \\text{Graph } 1)}{P(D | \\text{Graph } 0)} + \\log \\frac{P(\\text{Graph } 1)}{P(\\text{Graph } 0)}\n\\]\nRegardless of what prior probabilities we assign to the two graphs, the relative evidence for one graph over the other is entirely determined by the log-likelihood ratio. This is defined as causal support: \\(\\log \\frac{P(D | \\text{Graph } 1)}{P(D | \\text{Graph } 0)}\\).\nComputing \\(P(D | \\text{Graph } 1)\\) requires fully specifying the Bayes net. We‚Äôll assume that \\(P(E|B) = w_0\\) and \\(P(E|C) = w_1\\). When both \\(B\\) and \\(C\\) are present, we‚Äôll assume they contribute independently to causing \\(E\\), and therefore operate like a probabilistic OR function:\n\\[\nP(e^+|b,c; w_o, w_1) = 1 - (1-w_0)^b (1-w_1)^c\n\\]\nHere, when the B or C are present, \\(b\\) and \\(c\\) are set to 1, and when they are absent, \\(b\\) and \\(c\\) are set to 0.\nThe likelihood for Graph 1 is therefore\n\\[\nP(D | w_0, w_1, \\text{Graph } 1) = \\prod_{e,c} P(e|b^+,c; w_o, w_1)^{N(e,c)}\n\\]\nwhere the product is over the possible settings of \\(e\\) and \\(c\\) (effect absent/cause absent, effect absent/cause present, ‚Ä¶) and the \\(N(e,c)\\) values are counts of times that these outcomes happened in the data \\(D\\).\nHere‚Äôs a function to compute this.\n\ndef compute_likelihood(data, w0, w1, graph):\n  '''Returns likelihood of data for a given graph\n     \n     Parameters:\n       data (list): observation counts in this order:\n         N(c-,e-), N(c-,e+), N(c+,e-), N(c+,e+)\n       w0 (float): probability of background cause producing effect\n       w1 (float): probability of cause of interest producing effect\n       graph (int): 0 (Graph 0) or 1 (Graph 1)\n\n     Returns:\n       (float): probability of data\n  '''\n  \n  if graph == 0:\n    # e-\n    p = (1-w0)**(data[0]+data[2])\n    # e+\n    p = p * w0**(data[1]+data[3])\n  elif graph == 1:\n    # c-, e-\n    p = (1-w0)**data[0]\n    # c-, e+\n    p = p * w0**data[1]\n    # c+, e-\n    p = p * (1 - (w0 + w1 - w0*w1))**data[2]\n    # c+, e+\n    p = p * (w0 + w1 - w0*w1)**data[3]\n  else:\n    # error!\n    return(0)\n  \n  return(p)\n\nCausal support doesn‚Äôt actually depend directly on the parameters \\(w_0\\) and \\(w_1\\). The reason is that we ultimately don‚Äôt care what the values of these parameters are because we just want to draw an inference at a higher level about the best-fitting Bayes net.\nMathematically speaking, \\(w_0\\) and \\(w_1\\) are averaged out of the model, an idea we first saw in Chapter 3. We can accomplish this using Monte Carlo approximation, introduced in Chapter 8.\n\ndef estimate_likelihood(graph_number, data): \n  '''Returns an estimate of the probability of observing the data\n     under the specified graph using Monte Carlo estimation.\n     \n     Parameters:\n       graph_number (int): either 0 (Graph 0) or 1 (Graph 1)\n       data (list): observation counts in this order:\n         N(c-,e-), N(c-,e+), N(c+,e-), N(c+,e+)\n\n     Returns:\n       (float): probability of observing data\n  '''\n  from numpy.random import default_rng\n  \n  n_samples = 5000\n  rng = default_rng(2022)\n  mc_samples = np.zeros(n_samples)\n  \n  for i in range(n_samples):\n    # Sample values for w0 and w1 from a uniform distribution\n    w0 = rng.random()\n    w1 = rng.random()\n    \n    mc_samples[i] = compute_likelihood(data, w0, w1, graph_number)\n  \n  return(1/n_samples * np.sum(mc_samples))\n\nFinally, let‚Äôs put it all together by writing a function that computes causal support.\n\ndef causal_support(data):\n  '''Returns a causal support value for a given data set.\n     Causal support is a measure of how strongly a data\n     set indicates that there is evidence for a causal\n     effect.\n     \n     Parameters:\n       data (list): observation counts in this order:\n         N(c-,e-), N(c-,e+), N(c+,e-), N(c+,e+)\n\n     Returns:\n       (float): strength of evidence for causal relationship\n  '''\n  import numpy as np\n  \n  return (np.log(estimate_likelihood(1,data=data) / \n                 estimate_likelihood(0,data=data)))\n\nNow let‚Äôs see what the model predicts for the four cases we considered earlier.\n\ncausal_support_predictions = [causal_support([2,6,0,8]),\n                              causal_support([4,4,2,6]),\n                              causal_support([6,2,4,4]),\n                              causal_support([8,0,6,2])]\n                              \nlabels = [\"6/8, 8/8\", \"4/8, 6/8\", \"2/8, 4/8\", \"0/8, 2/8\"]\nfig, ax = plt.subplots()\nax.bar(labels, causal_support_predictions)\nax.set_xlabel(\"Data\")\nax.set_ylabel(\"Model prediction\")\n\nplt.show()\n\n\n\n\nThe labels on the x-axis indicate the control condition counts, followed by the experimental condition counts.\nYou can see that the model‚Äôs predictions largely follow the pattern in the data from earlier. The model favors Graph 1 in the two leftmost cases, is essentially uncertain in the third case, and begins to think the evidence favors no causal relationship in the rightmost case.\nThe rest of the Griffiths & Tenenbaum paper shows how causal support is able to capture some subtle aspects of people‚Äôs causal judgments that other models that don‚Äôt incorporate both structure and strength fail to capture."
  },
  {
    "objectID": "06-sampling-assumptions.html",
    "href": "06-sampling-assumptions.html",
    "title": "Sampling assumptions",
    "section": "",
    "text": "So far, we‚Äôve seen where strong sampling was a logical assumption to make. But it isn‚Äôt always.\nFor example, consider the hormone problem from Chapter 3. If you were randomly testing people‚Äôs hormone levels and every person just happened to be healthy, clearly strong sampling wouldn‚Äôt be appropriate, because we were sampling from the full population of both healthy and unhealthy people.\nThis raises a question: Are people sensitive to the process by which data are generated?\nSpoiler alert: Yes.\n\n\nSuppose you see the following collection of objects on a table.\n\n\n\nFrom Xu & Tenenbaum (2007).\n\n\nNow consider the following situation:\n\nA teacher picks three blue circles from the pile and calls them ‚Äúwugs‚Äù.\n\nWhat is a wug? Are all circles wugs? Or just blue circles?\nConsider a different situation:\n\nA teacher picks one blue circle from the pile and calls it a ‚Äúwug‚Äù. The teacher then asks a child to choose two more (the child picks two blue circles). The teacher confirms that they are also wugs.\n\nWhat is a wug? Are all circles wugs? Or just blue circles?\nIn both situations, the observed data is the same: three blue circles labeled as wugs. What‚Äôs different is the process by which the data was generated. In one case, a knowledgeable teacher picked all the wugs; in the other, the teacher only picked one example and a (probably not knowledgeable) child picked the others.\nTo use some terms we‚Äôve seen before, the teacher is using strong sampling. The child is using something closer to weak sampling.\n\n\n\nThe hierarchy of objects. From Xu & Tenenbaum (2007).\n\n\nThis is based on an actual study by Fei Xu and Josh Tenenbaum, who also developed a model of the task. They asked whether children and adults would generalize the word ‚Äúwug‚Äù to the basic-level category (circles with lines in them) or the subordinate category (blue circles with lines in them). Here are their results:\n\n\n\nExperimental results from Xu & Tenenbaum (2007).\n\n\nPeople clearly distinguished between the teacher-driven situation and the child-driven (learner-driven) situation, and were much more likely to generalize ‚Äúwugs‚Äù to the subordinate category when the teacher picked the objects.\n\n\n\nThe teacher choosing wugs could be said to be using pedagogical sampling. The teacher deliberately used knowledge of the wug concept to select informative examples to help the child learn the concept as quickly as possible.\nThis is an idea explored by Patrick Shafto, Noah Goodman, and Thomas Griffiths (as well as Elizabeth Bonawitz and other researchers in related work). To put it in formal terms:\n\\[\n\\begin{equation}\nP_{\\text{teacher}}(d|h) \\propto (P_{\\text{learner}}(h|d))^\\alpha\n\\end{equation}\n\\tag{1}\\]\nHere, \\(\\alpha\\) is a parameter that controls how optimized the teacher‚Äôs choices are. As \\(\\alpha \\rightarrow \\infty\\), the teacher will choose examples \\(d\\) that maximize the learner‚Äôs posterior probability.\nThis means we can figure out how the learner will update their beliefs about a concept by directly applying Bayes rule:\n\\[\n\\begin{equation}\n  P_{\\text{learner}}(h|d) = \\frac{P_{\\text{teacher}}(d|h)P(h)}{\\sum_{h_i} P_{\\text{teacher}}(d|h_i)P(h_i)}\n\\end{equation}\n\\tag{2}\\]\nThis equation makes it clear that the teacher and learner are inextricably linked: The teacher chooses examples to maximize the learner‚Äôs understanding, and the learner updates beliefs based on expectations of how the teacher is choosing examples. The model is recursive.\nAs the researchers explain in their paper, the two equations above are a system of equations that can be rearranged to create the following equation defining how the teacher should choose examples:\n\\[\n\\begin{equation}\n  P_{\\text{teacher}}(d|h) \\propto \\left( \\frac{P_{\\text{teacher}}(d|h) P(h)}{\\sum_{h_i} P_{\\text{teacher}}(d|h_i) P(h_i)} \\right)^\\alpha\n\\end{equation}\n\\tag{3}\\]\nHow to solve this equation? In the paper, they use an iterative algorithm, that works like this:\n\nInitialize \\(P_{\\text{teacher}}(d|h)\\) using weak sampling.\nIterate through the following steps until \\(P_{\\text{teacher}}(d|h)\\) stabilizes (doesn‚Äôt change from one iteration to the next):\n\nFor each possible \\(h\\) and \\(d\\), compute \\(P_{\\text{learner}}(h|d)\\) using Equation Equation¬†2 and \\(P_{\\text{teacher}}(d|h)\\) values from the previous iteration.\nFor each possible \\(d\\) and \\(h\\), update \\(P_{\\text{teacher}}(d|h)\\) using \\(P_{\\text{learner}}(h|d)\\) values from the previous step, where \\(P_{\\text{teacher}}(d|h_i) = P_{\\text{learner}}(h_i|d) / \\sum_{d_j} P_{\\text{learner}}(h_i|d_j)\\) (Equation Equation¬†1).\n\n\n\n\nThe researchers tested out their model in an experiment using a simple task called the rectangle game.\nIn the game, concepts are rectangular boundaries in a two-dimensional space. The teacher knows the boundary and the learner has to figure it out from examples given by the teacher.\n\n\n\nThe rectangle game. From Shafto et al.¬†(2014).\n\n\nIn one version, the teacher can only provide positive examples (‚ÄúHere‚Äôs an example that‚Äôs inside the boundary‚Äù). In another version, the teacher can also provide negative examples (‚ÄúHere‚Äôs an example that‚Äôs not inside the boundary‚Äù).\nIn an experiment, they asked people to play the role of the learner in the rectangle game. There were three conditions:\n\nTeaching-pedagogical learning: People first acted as teachers, then as learners.\nPedagogical learning: People acted just as learners and were told the examples they saw were chosen by a teacher.\nNon-pedagogical learning: People acted just as learners and were told the examples they saw were not chosen by a teacher.\n\nAfter showing people examples, they asked them to draw a rectangle of their best guess about what the boundary was.\nOne key prediction of the model is that the most informative locations for examples under pedagogical sampling are at the corners of the rectangles. To test whether the learners understood this, they measured the proportion of examples that ended up in the corners of the rectangles people drew. The results are shown below.\n\n\n\nResults from Shafto et al.‚Äôs (2014) Experiment 1.\n\n\nAs you can see, when people thought a knowledgeable teacher was providing the examples (the teaching-pedagogical and pedagogical learning conditions), they were much more likely to draw tight boundaries around the examples, compared to when they didn‚Äôt think a teacher was providing the examples (the non-pedagogical learning condition). This suggests that people were indeed sensitive to the process by which the data were generated and incorporated that understanding into their inferences.\nIn the next chapter, we‚Äôll work through one more application of people‚Äôs sensitivity to different sampling assumptions: understanding the pragmatics of speech."
  },
  {
    "objectID": "06-sampling-assumptions.html#word-learning",
    "href": "06-sampling-assumptions.html#word-learning",
    "title": "6¬† Sampling assumptions",
    "section": "6.1 Word learning üí¨",
    "text": "6.1 Word learning üí¨\nSuppose you see the following collection of objects on a table.\n\n\n\nFrom Xu & Tenenbaum (2007).\n\n\nNow consider the following situation:\n\nA teacher picks three blue circles from the pile and calls them ‚Äúwugs‚Äù.\n\nWhat is a wug? Are all circles wugs? Or just blue circles?\nConsider a different situation:\n\nA teacher picks one blue circle from the pile and calls it a ‚Äúwug‚Äù. The teacher then asks a child to choose two more (the child picks two blue circles). The teacher confirms that they are also wugs.\n\nWhat is a wug? Are all circles wugs? Or just blue circles?\nIn both situations, the observed data is the same: three blue circles labeled as wugs. What‚Äôs different is the process by which the data was generated. In one case, a knowledgeable teacher picked all the wugs; in the other, the teacher only picked one example and a (probably not knowledgeable) child picked the others.\nTo use some terms we‚Äôve seen before, the teacher is using strong sampling. The child is using something closer to weak sampling.\n\n\n\nThe hierarchy of objects. From Xu & Tenenbaum (2007).\n\n\nThis is based on an actual study by Fei Xu and Josh Tenenbaum, who also developed a model of the task. They asked whether children and adults would generalize the word ‚Äúwug‚Äù to the basic-level category (circles with lines in them) or the subordinate category (blue circles with lines in them). Here are their results:\n\n\n\nExperimental results from Xu & Tenenbaum (2007).\n\n\nPeople clearly distinguished between the teacher-driven situation and the child-driven (learner-driven) situation, and were much more likely to generalize ‚Äúwugs‚Äù to the subordinate category when the teacher picked the objects."
  },
  {
    "objectID": "06-sampling-assumptions.html#pedagogical-sampling",
    "href": "06-sampling-assumptions.html#pedagogical-sampling",
    "title": "6¬† Sampling assumptions",
    "section": "6.2 Pedagogical sampling üßë‚Äçüè´",
    "text": "6.2 Pedagogical sampling üßë‚Äçüè´\nThe teacher choosing wugs could be said to be using pedagogical sampling. The teacher deliberately used knowledge of the wug concept to select informative examples to help the child learn the concept as quickly as possible.\nThis is an idea explored by Patrick Shafto, Noah Goodman, and Thomas Griffiths (as well as Elizabeth Bonawitz and other researchers in related work). To put it in formal terms:\n\\[\n\\begin{equation}\nP_{\\text{teacher}}(d|h) \\propto (P_{\\text{learner}}(h|d))^\\alpha\n\\end{equation}\n\\tag{6.1}\\]\nHere, \\(\\alpha\\) is a parameter that controls how optimized the teacher‚Äôs choices are. As \\(\\alpha \\rightarrow \\infty\\), the teacher will choose examples \\(d\\) that maximize the learner‚Äôs posterior probability.\nThis means we can figure out how the learner will update their beliefs about a concept by directly applying Bayes rule:\n\\[\n\\begin{equation}\n  P_{\\text{learner}}(h|d) = \\frac{P_{\\text{teacher}}(d|h)P(h)}{\\sum_{h_i} P_{\\text{teacher}}(d|h_i)P(h_i)}\n\\end{equation}\n\\tag{6.2}\\]\nThis equation makes it clear that the teacher and learner are inextricably linked: The teacher chooses examples to maximize the learner‚Äôs understanding, and the learner updates beliefs based on expectations of how the teacher is choosing examples. The model is recursive.\nAs the researchers explain in their paper, the two equations above are a system of equations that can be rearranged to create the following equation defining how the teacher should choose examples:\n\\[\n\\begin{equation}\n  P_{\\text{teacher}}(d|h) \\propto \\left( \\frac{P_{\\text{teacher}}(d|h) P(h)}{\\sum_{h_i} P_{\\text{teacher}}(d|h_i) P(h_i)} \\right)^\\alpha\n\\end{equation}\n\\tag{6.3}\\]\nHow to solve this equation? In the paper, they use an iterative algorithm, that works like this:\n\nInitialize \\(P_{\\text{teacher}}(d|h)\\) using weak sampling.\nIterate through the following steps until \\(P_{\\text{teacher}}(d|h)\\) stabilizes (doesn‚Äôt change from one iteration to the next):\n\nFor each possible \\(h\\) and \\(d\\), compute \\(P_{\\text{learner}}(h|d)\\) using Equation Equation¬†6.2 and \\(P_{\\text{teacher}}(d|h)\\) values from the previous iteration.\nFor each possible \\(d\\) and \\(h\\), update \\(P_{\\text{teacher}}(d|h)\\) using \\(P_{\\text{learner}}(h|d)\\) values from the previous step, where \\(P_{\\text{teacher}}(d|h_i) = P_{\\text{learner}}(h_i|d) / \\sum_{d_j} P_{\\text{learner}}(h_i|d_j)\\) (Equation Equation¬†6.1).\n\n\n\n6.2.1 The rectangle game\nThe researchers tested out their model in an experiment using a simple task called the rectangle game.\nIn the game, concepts are rectangular boundaries in a two-dimensional space. The teacher knows the boundary and the learner has to figure it out from examples given by the teacher.\n\n\n\nThe rectangle game. From Shafto et al.¬†(2014).\n\n\nIn one version, the teacher can only provide positive examples (‚ÄúHere‚Äôs an example that‚Äôs inside the boundary‚Äù). In another version, the teacher can also provide negative examples (‚ÄúHere‚Äôs an example that‚Äôs not inside the boundary‚Äù).\nIn an experiment, they asked people to play the role of the learner in the rectangle game. There were three conditions:\n\nTeaching-pedagogical learning: People first acted as teachers, then as learners.\nPedagogical learning: People acted just as learners and were told the examples they saw were chosen by a teacher.\nNon-pedagogical learning: People acted just as learners and were told the examples they saw were not chosen by a teacher.\n\nAfter showing people examples, they asked them to draw a rectangle of their best guess about what the boundary was.\nOne key prediction of the model is that the most informative locations for examples under pedagogical sampling are at the corners of the rectangles. To test whether the learners understood this, they measured the proportion of examples that ended up in the corners of the rectangles people drew. The results are shown below.\n\n\n\nResults from Shafto et al.‚Äôs (2014) Experiment 1.\n\n\nAs you can see, when people thought a knowledgeable teacher was providing the examples (the teaching-pedagogical and pedagogical learning conditions), they were much more likely to draw tight boundaries around the examples, compared to when they didn‚Äôt think a teacher was providing the examples (the non-pedagogical learning condition). This suggests that people were indeed sensitive to the process by which the data were generated and incorporated that understanding into their inferences.\nIn the next chapter, we‚Äôll work through one more application of people‚Äôs sensitivity to different sampling assumptions: understanding the pragmatics of speech."
  },
  {
    "objectID": "08-social-cognition.html",
    "href": "08-social-cognition.html",
    "title": "Social cognition",
    "section": "",
    "text": "Watch the video below. What do you see?\nIf you‚Äôre like most people, you see more than just lifeless shapes moving around. You see a whole drama play out involving characters with goals and emotions.\nThis animation, from a 1944 study by Heider and Simmel, is an excellent example of our capacity for social cognition: thinking about other people. We are constantly attributing goals, beliefs, desires, and feelings to others. The fact that we are willing to do it for simple shapes suggests we may not be able to help ourselves from doing it.\nWhen someone takes an action, makes a decision, or makes a facial expression, how do we infer what they‚Äôre thinking or feeling? In this chapter, we‚Äôll look at two computational approaches to answering this question."
  },
  {
    "objectID": "08-social-cognition.html#inverse-decision-making",
    "href": "08-social-cognition.html#inverse-decision-making",
    "title": "8¬† Social cognition",
    "section": "8.1 Inverse decision-making üçë",
    "text": "8.1 Inverse decision-making üçë\nAll social cognition problems have the same basic character: you observe something about other people, and you want to infer some underlying cause of the thing you observed. For example, you see someone take an action, and you want to infer the goal that motivated the action.\nThe basic idea behind many approaches to understanding how people reason about other people is that they have some kind of mental model of how other people act and they essentially run that model backward (or invert) the model to infer the information they don‚Äôt get to see.\nConsider a simple decision-making situation. Rue likes peaches more than oranges and oranges more than apples:\n\nüçë &gt; üçä &gt; üçé\n\nNow you give Rue a choice between these three fruits. Which one is she most likely to choose?\nProbably the peach right? I mean, she might choose the orange or the apple because she happens to be in the mood for one of those, but given the information you have, the peach is a good guess.\nNow imagine you don‚Äôt know Rue‚Äôs fruit preferences. She‚Äôs offered the following fruits to pick from:\n\nüçë üçä üçé\n\nShe chooses the peach üçë. If you had to guess her relative preferences for the fruits, what would you think?\nAt the very least, you might guess she likes peaches more than oranges and apples. But why?\nYou assume that people‚Äôs choices are guided by preferences and that people‚Äôs actions are basically rational. So seeing Rue make one choice from a set of options gives you information about what she likes.\nCan we formalize this intuition?1\n\n8.1.1 A preference learning model\nLet‚Äôs start with some assumptions:\n\nEach fruit provides some utility to the person who takes it (recall the notion of utility introduced in Chapter 7).\nUtilities are additive, meaning two peaches provide twice as much utility as one peach.\nPeople will choose options in proportion to their utility. This is the same ‚Äúsoftmax‚Äù assumption we used in the rational speech act model.\n\nThese assumptions combine to give us a choice model, a model of how people will make choices between different options. (We‚Äôve limited our focus to fruits, but the model could be applied to anything.)\n\\[\n\\begin{equation}\np(c = o_j|\\mathbf{u}, \\mathbf{A}) = \\frac{\\exp(U_j)}{\\sum^n_{k=1}\\exp(U_k)}\n\\end{equation}\n\\tag{8.1}\\]\nIn this model, \\(c\\) is the choice, \\(o_j\\) refers to option \\(j\\), \\(\\mathbf{A}\\) is a vector specifying each option and their attributes (or features), \\(\\mathbf{u}\\) is a vector of utilities that the decision-maker assigns to each attribute, and \\(U_j\\) is the summed total utility in each option.\n\n8.1.1.1 Inverting the model\nRemember that our goal is to infer other people‚Äôs preferences, not predict their choices. So, after we see someone make a choice between some fruits, how do we infer what fruits they like best? We can invert Equation Equation¬†8.1 using Bayes‚Äôs rule:\n\\[\n\\begin{equation}\np(\\mathbf{u}|c,\\mathbf{A}) = \\frac{p(c|\\mathbf{u},\\mathbf{A})p(\\mathbf{u})}{p(c|\\mathbf{A})}\n\\end{equation}\n\\tag{8.2}\\]\nSuppose Rue has a choice between the following options:\n\nOption 1: üçë üçé üçä\nOption 2: üçå\n\nJules has a choice between the following options:\n\nOption 1: üçë\nOption 2: üçé üçä üçå\n\nBoth Rue and Jules pick Option 1, which includes a peach üçë. Based on their choices alone, who do you think likes peaches more? (Or, at least, which person‚Äôs choice would make you more confident they like peaches?)\nLet‚Äôs apply the inverse decision-making model to these two choices.\nWe‚Äôll start by encoding these choices using a binary encoding scheme in which each element in a list represents a fruit and this element is set to 1 if that fruit is present in the option.\n\nimport numpy as np\n\n# Fruit order:\n# 1. peach\n# 2. apple\n# 3. orange\n# 4. banana\nchoice_rue = np.array([[1,1,1,0],\n                       [0,0,0,1]])\nchoice_jules = np.array([[1,0,0,0],\n                         [0,1,1,1]])\n\nLet‚Äôs define a function that calculates the choice probability in Equation Equation¬†8.1. This is our likelihood function.\n\ndef compute_choice_prob(j, u, options): \n  '''Returns the probability of choosing option j\n     from the set options.\n     \n     Parameters:\n       j (int): the chosen option\n       u (array): a vector of utilities assigned to the\n         attributes in the options\n       options (array): a matrix in which each row is an\n         option, and each option is a binary array of the \n         attributes in that option\n         \n     Returns:\n       p (float): probability of choosing option j\n  '''\n  \n  # Compute the total utility of each option\n  total_u = np.sum(u*options, axis=1)\n  \n  p = np.exp(total_u[j]) / np.sum(np.exp(total_u))\n  return(p)\n\nLet‚Äôs first assume they both like all fruits equally (they get a utility of 1 from any fruit). What is the probability of Rue and Jules making these choices?\n\nutilities = np.array([1,1,1,1])\n\nprint(\"Rue's choice probability:\" + \n  str(compute_choice_prob(0, utilities, choice_rue)))\nprint(\"Jules's choice probability:\" + \n  str(compute_choice_prob(0, utilities, choice_jules)))\n\nRue's choice probability:0.8807970779778824\nJules's choice probability:0.11920292202211755\n\n\nUnsurprisingly, Rue‚Äôs choice is much more probable. What if they like peaches four times as much as other fruits?\n\nutilities = np.array([4,1,1,1])\n\nprint(\"Rue's choice probability:\" + \n  str(compute_choice_prob(0, utilities, choice_rue)))\nprint(\"Jules's choice probability:\" + \n  str(compute_choice_prob(0, utilities, choice_jules)))\n\nRue's choice probability:0.9933071490757152\nJules's choice probability:0.7310585786300048\n\n\nThis increases Rue‚Äôs choice probability by just over 10%, but it increases Jules‚Äôs choice probability by over 60%.\nTo invert the model, we also need to specify the prior probability, \\(p(\\mathbf{u})\\). We‚Äôll assume that people generally like fruit, but they are likely to disagree about how much they like different fruits, and there‚Äôs a possibility that some people will dislike like certain fruits.\nWe can capture this idea by assuming that utilities for fruits are normally distributed with a positive mean. Let‚Äôs say that they have a mean of 2 and a standard deviation of 0.5:\n\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\nu_mean = 2\nu_sd = 0.5\n\nu = np.linspace(-1,5,1000)\npu = norm.pdf(u,u_mean,u_sd)\n\nfig, ax = plt.subplots(1,1)\nax.plot(u, pu)\nax.set_xlabel(\"u\")\nax.set_title(\"Prior probability of u\")\n\nplt.show()\n\n\n\n\nAdditionally, we‚Äôll make the simplifying assumption that utilities for different fruits are independent. This means that knowing someone likes pineapples doesn‚Äôt tell you anything about how much they‚Äôll like grapefruits.\nFinally, we need a way to compute \\(p(c|\\mathbf{A})\\). In other words, after seeing Rue make a choice, what was the probability of anyone with any preferences making that choice?\nPreviously, we got around computing denominators like this by normalizing. This works when you can enumerate the full hypothesis space. This time, the space of hypotheses (all possible assignments of utilities to fruits) is continuous and it‚Äôs not easy to integrate over. So we need to do something else.\n\n\n8.1.1.2 Monte Carlo estimation\nWe‚Äôll use an estimation method known as Monte Carlo that relies on random sampling.\nHere‚Äôs the basic idea as it applies to our problem. We want to know the overall probability of someone in the general population making a certain choice. We don‚Äôt know what that is, but we do know the distribution of utilities in the general population. That‚Äôs the prior \\(p(\\mathbf{u})\\) we just defined. So we can get a close approximation to what we want using the following algorithm:\n\nDraw a random sample \\(\\mathbf{u}\\) from \\(p(\\mathbf{u})\\).\nCompute \\(p(c|\\mathbf{u}, \\mathbf{A})\\).\nRepeat many times.\nCalculate the mean choice probability of all trials.\n\nLet‚Äôs write a function to do this.\n\ndef estimate_marginal_likelihood(j, options, n_samples): \n  '''Returns an estimate of the probability of choosing option j\n     from the set options using Monte Carlo estimation.\n     \n     Parameters:\n       j (int): the chosen option\n       options (array): a matrix in which each row is an\n         option, and each option is a binary array of the \n         attributes in that option\n       n_samples (int): number of Monte Carlo samples to\n         collect\n         \n     Returns:\n       (float): probability of choosing option j\n  '''\n  \n  n_attributes = len(options[0])\n  mc_samples = np.zeros(n_samples)\n  \n  for i in range(n_samples):\n    # draw random u sample\n    u = np.random.normal(u_mean,u_sd,n_attributes)\n    mc_samples[i] = compute_choice_prob(j, u, options)\n  \n  return(np.mean(mc_samples))\n\nLet‚Äôs compare the probability of someone making the choices that Rue and Jules made.\n\nprint(\"Rue's choice probability: \" + \n  str(estimate_marginal_likelihood(0, choice_rue, 2000)))\nprint(\"Jules's choice probability: \" +\n  str(estimate_marginal_likelihood(0, choice_jules, 2000)))\n\nRue's choice probability: 0.9719132424016815\nJules's choice probability: 0.028545335214233\n\n\nUnsurprisingly, Jules‚Äôs choice has a much higher baseline probability.\n\n\n8.1.1.3 Putting it all together\nNow we‚Äôll infer Rue‚Äôs and Jules‚Äôs preferences from their choices using Monte Carlo estimation again.\n\n# Set random state for reproducibility\nnp.random.RandomState(2022) \n\nn_samples = 2000 # number of Monte Carlo samples\nn_attributes = len(choice_rue[0])\npeach = 0\nbanana = 3\n\nmarginal_likelihood_rue = estimate_marginal_likelihood(0, choice_rue, n_samples)\nmarginal_likelihood_jules = estimate_marginal_likelihood(0, choice_jules, n_samples)\n\nmc_samples_rue = np.zeros(n_samples)\nmc_samples_jules = np.zeros(n_samples)\n\nfor i in range(n_samples):\n  # Draw u samples for Rue and Jules\n  u_rue = np.random.normal(u_mean,u_sd,n_attributes)\n  u_jules = np.random.normal(u_mean,u_sd,n_attributes)\n  \n  # Compute posterior for peach utility for each person\n  pu_rue = ((compute_choice_prob(0, u_rue, choice_rue) * u_rue) / \n    marginal_likelihood_rue)\n  pu_jules = ((compute_choice_prob(0, u_jules, choice_jules) * u_jules) /\n    marginal_likelihood_jules)\n\n  mc_samples_rue[i] = pu_rue[peach]\n  mc_samples_jules[i] = pu_jules[peach]\n\nLet‚Äôs look at the results.\n\nimport pandas as pd\nimport seaborn as sb\n\nsamples = pd.DataFrame(data = {'Rue': mc_samples_rue,\n                               'Jules': mc_samples_jules})\n# reorganize the data\nsamples = samples.melt(value_vars=[\"Jules\",\"Rue\"], \n  var_name=\"person\", value_name=\"u\")\n# filter out extreme samples\nsamples_filtered = samples[samples[\"u\"] &lt; 50]\n\nfig, ax = plt.subplots(1,1)\ng = sb.displot(data=samples_filtered, x=\"u\", hue=\"person\", kind=\"kde\")\n\nplt.show()\n\nC:\\Users\\jern\\AppData\\Local\\anaconda3\\Lib\\site-packages\\seaborn\\axisgrid.py:118: UserWarning:\n\nThe figure layout has changed to tight\n\n\n\n\n\n\n\n\n\nThe model‚Äôs inferences here require some interpretation. The model is more certain about Rue having a preference for peach, but the distribution on the Jules‚Äôs preference for peach has a much longer tail.\nThis makes some sense. Because of our mostly positive prior distribution, it‚Äôs reasonable to assume by default that Rue likes peaches.\nJules‚Äôs choice is a bit odd, so it‚Äôs harder to make sense of. One way to explain it is that she simply made a mistake (after all, as we saw above, it‚Äôs an improbable choice). Another way to explain it is by assuming she has a very strong preference for peaches. (Of course, the model doesn‚Äôt take into account the countless other reasons Jules might prefer one fruit over three.)\n\n\n\n8.1.2 Homework 5: Your turn\n\n\n\n\n\nIn your next assignment, you‚Äôll fully implement a version of this model and compare it to data I collected (along with Chris Lucas and Charles Kemp).\n\n\n\nCards used in the preference learning task from Jern, Lucas, & Kemp (2017)."
  },
  {
    "objectID": "08-social-cognition.html#na√Øve-utility-calculus",
    "href": "08-social-cognition.html#na√Øve-utility-calculus",
    "title": "8¬† Social cognition",
    "section": "8.2 Na√Øve utility calculus üßÆ",
    "text": "8.2 Na√Øve utility calculus üßÆ\nSuppose Rue and Jules both make a choice between the same two fruits:\n\nOption 1: üçë\nOption 2: üçå\n\nThey both choose the peach üçë. The difference is that, for Rue, both fruits were placed in a bowl in front of her. For Jules, the peach was in a bowl in front her, and the banana was on a high shelf in another room.\nIn both cases, you‚Äôd probably assume they like peaches, but you might conclude there‚Äôs weaker evidence that Jules likes them. She may have chosen the peach because it was just convenient.\nThis (pretty contrived) example shows how people‚Äôs choices are a function of both rewards (or preferences) and costs. And when we‚Äôre thinking about other people‚Äôs behavior, we take their potential rewards and costs into account to understand why they‚Äôre doing things.\nThis idea was best articulated and formalized by Julian Jara-Ettinger and his collaborators. And they incorporated it into a computational model.\nSpecifically, suppose a person has a plan \\(p\\) to achieve an outcome \\(o\\). We can define their utility \\(U\\) as:\n\\[\nU(o,p) = R(o) - C(p)\n\\]\nwhere \\(R(\\cdot)\\) is their reward from the outcome and \\(C(\\cdot)\\) is the cost they incur from carrying out the plan.\n\n8.2.1 Markov decision processes\nEarlier, we considered single decisions that happened in isolation. But lots of behavior involves a series of small decisions that happen in a sequence: the order in which someone prepares and cooks a dish, the route someone takes to get from their workplace to their home, which sections of a book chapter they write first.\nA series of decisions is a plan \\(p\\). And a useful computational framework for choosing optimal plans is a Markov decision process.\nTo be concrete, let‚Äôs focus on the specific environment in Jara-Ettinger et al (2020): a 7x7 grid in which agents can move in any direction, one step at a time.\n\n\n\nExample of a grid and an agent path from Jara-Ettinger et al (2020).\n\n\nThe locations in the grid are states \\(S\\). Whenever an agent is in a state \\(s\\), they can take an action \\(a \\in A\\), moving to any adjacent state. The agent has some goal state in mind (in the figure above, it‚Äôs the green square). What we want is an optimal policy that takes a state and returns an optimal action for getting toward the goal state. This can be computed as follows.\nFirst, we compute each state‚Äôs optimal value \\(V^*(s)\\), using this recursive equation:\n\\[\nV^*(s) = \\text{max}_a\\gamma \\sum_{s^\\prime} P_{s,a}(s^\\prime) V^*(s^\\prime) + R(a,s) - C(a,s)\n\\]\nwhere \\(P_{s,a}(s^\\prime)\\) is ‚Äúthe probability that the agent will be in state \\(s^\\prime\\) when she takes action \\(a\\) in state \\(s\\) and \\(\\gamma \\in {0,1}\\).‚Äù\nEssentially, this equation requires going through every state in the grid, summing up its rewards and costs, plus the expected downstream rewards and costs of that state.\nSolving this recursive function is outside the scope of this book, but there are standard methods for doing it and even Python packages for solving MDPs.\nThe action policy is then defined as:\n\\[\n\\begin{equation}\np(a|s) \\propto \\exp \\left( \\sum_{s^\\prime} P_{s,a}(s^\\prime) V^*(s^\\prime) \\right)\n\\end{equation}\n\\tag{8.3}\\] In their model, they once again introduce a notion of a ‚Äúsoftmax‚Äù choice rule, rather than assume agents will be perfectly optimal. This makes sense because the goal is to model how people reason about others, and people often have incomplete information about other people.\n\n\n8.2.2 Inverting the MDP\nAfter seeing an agent take a series of actions \\(\\mathbf{a}\\), we can infer their costs and rewards pretty much the same way we did before, by applying Bayes‚Äôs rule.\n\\[\np(C,R|\\mathbf{a}) \\propto p(\\mathbf{a}|C,R) p(C,R)\n\\]\nIn the model, they use uniform priors over costs and rewards. Computing \\(p(\\mathbf{a}|C,R)\\) means just applying Equation Equation¬†8.3 repeatedly for each action the agent took in their sequence. (The researchers also modeled a notion of sub-goals that I‚Äôve omitted.)\n\n\n8.2.3 Results\nThe researchers tested their model in a series of experiments in which people saw paths that agents took across the grid and then had to rate the costs and rewards the agent assigned to different states.\nResults from one experiment are below.\n\n\n\nPartial results from Experiment 1b of Jara-Ettinger et al (2020).\n\n\nThe cost and reward ratings were separately normalized so that they had a mean of zero. Then they were combined in the plots.\nCompared to simpler alternative models, the full na√Øve utility calculus model provided the best fit to people‚Äôs judgments."
  },
  {
    "objectID": "08-social-cognition.html#footnotes",
    "href": "08-social-cognition.html#footnotes",
    "title": "8¬† Social cognition",
    "section": "",
    "text": "The following is based on a model originally developed by Chris Lucas and his collaborators.‚Ü©Ô∏é"
  },
  {
    "objectID": "05-hierarchical-bayes.html",
    "href": "05-hierarchical-bayes.html",
    "title": "Hierarchical generalization",
    "section": "",
    "text": "In previous examples, there were always a finite number of hypotheses that we were making inferences about (number of black balls, fair or trick coin, yellow or green taxi). Sometimes, we want to consider an infinite set of hypotheses. For example, after flipping a coin, what is the probability of that coin coming up heads? The answer to this question could be any number in the interval [0,1]."
  },
  {
    "objectID": "05-hierarchical-bayes.html#beta-binomial",
    "href": "05-hierarchical-bayes.html#beta-binomial",
    "title": "5¬† Hierarchical generalization",
    "section": "5.1 The Beta-Binomial model ü™ô",
    "text": "5.1 The Beta-Binomial model ü™ô\n\n\n\nPhoto by ZSun Fu on Unsplash.\n\n\nWe can answer this question with a model called the Beta-Binomial model, named for the probability distributions it uses. First, let‚Äôs set up the basic assumptions of the model.\nLet \\(P(\\text{heads}) = \\theta\\). We don‚Äôt know what \\(\\theta\\) is. After observing a sequence of coin flips \\(D\\), we want to estimate \\(\\theta\\). This can be accomplished by directly applying Bayes‚Äôs rule:\n\\[\nP(\\theta|D) = \\frac{P(D|\\theta) P(\\theta)}{P(D)}\n\\]\nThe data \\(D\\) in this case corresponds to the number of \\(k\\) heads out of \\(n\\) total flips. This follows a Binomial distribution, which describes the probability of getting \\(k\\) successes out of \\(n\\) trials, when the probability of success on each trial is \\(\\theta\\). We will define heads as a ‚Äúsuccess‚Äù.\n\\[\n\\begin{align}\nP(D|\\theta) = P(k|\\theta,n) &= \\text{Bin}(k; n, \\theta) \\\\\n&= \\binom{n}{k} \\theta^{k} (1-\\theta)^{n-k}\n\\end{align}\n\\]\nThe notation for the \\(\\text{Bin}(\\cdot)\\) function indicates that this is a distribution over \\(k\\) (number of successes) and the distribution has the parameters \\(n\\) (the total number of trials) and \\(\\theta\\) (the probability of a success on each trial).\nWe can define the prior, \\(P(\\theta)\\), however we like. Because \\(\\theta\\) is a random variable that can take on any value from 0 to 1, we cannot just say \\(P(\\theta) = 0.5\\) like we could in earlier examples. Instead, \\(P(\\theta)\\) must be a probability distribution that assigns probabilities to any value from 0 to 1. If we know nothing about \\(\\theta\\), we could use a Uniform(\\([0,1]\\)) or non-informative prior that assigns equal probability to all values of \\(\\theta\\).\nAlternatively, a convenient choice (for reasons explained below) for \\(P(\\theta)\\) is the Beta distribution:\n\\[\nP(\\theta) = \\text{Beta}(\\theta;\\alpha,\\beta)\n\\]\nThe Beta distribution has two parameters: \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\). Let‚Äôs create a function that will allow us to visualize the Beta distribution.\n\nfrom scipy import stats\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_beta(a, b):\n  x = np.linspace(0,1,num=500)\n  px = stats.beta.pdf(x, a, b)\n  \n  fig, ax = plt.subplots()\n  ax.plot(x, px)\n  plt.show()\n\nplot_beta takes two arguments: a (\\(\\alpha\\)), and b(\\(\\beta\\)) and plots a Beta distribution with those parameter values.\nLet‚Äôs see what it looks like with a few different values.\n\nplot_beta(1,1)\n\n\n\n\nWhen \\(\\alpha = \\beta = 1\\), the Beta distribution is identical to a Uniform(\\([0,1]\\)) distribution.\n\nplot_beta(3,3)\n\n\n\n\nWhen \\(\\alpha\\) and \\(\\beta\\) are greater than 1 and equal, we get a distribution with a peak around 0.5. If we had strong prior expectations that the coin was unbiased, we could increase the parameters even more:\n\nplot_beta(50,50)\n\n\n\n\nWhat about when \\(\\alpha\\) and \\(\\beta\\) are not equal?\n\nplot_beta(4,2)\n\n\n\n\nThis allows us to capture skewed priors, perhaps capturing a belief that the coin has a specific bias.\nNow, what if \\(\\alpha\\) and \\(\\beta\\) are less than 1?\n\nplot_beta(0.5,0.5)\n\n\n\n\nThis might capture the belief that the coin is strongly biased, but we aren‚Äôt sure in which direction.\n\n5.1.1 Conjugate distributions\nThe Beta distribution is the conjugate distribution for the Binomial distribution. This means that when the likelihood is a Binomial distribution and the prior is a Beta distribution, then the posterior is also a Beta distribution. Specifically, after making these assumptions,\n\\[\nP(\\theta|D) = \\text{Beta}(\\theta; \\alpha + k, \\beta + n-k)\n\\]\nThe parameters of the posterior distribution are (1) the sum of \\(\\alpha\\) from the prior and the number of observed heads and (2) the sum of \\(\\beta\\) from the prior and the number of observed tails. This means that the parameters \\(\\alpha\\) and \\(\\beta\\) of the Beta prior have a natural interpretation as ‚Äúvirtual flips‚Äù. For example, the larger \\(\\alpha\\) is compared to \\(\\beta\\), the more biased toward heads we expect \\(\\theta\\) to be. Additionally, the larger \\(\\alpha\\) and \\(\\beta\\) are, the more certain (less diffuse) the prior is.\n\n\n5.1.2 Parameter estimation\nBecause we used a conjugate distribution, we can use our same plot_beta function to generate posterior probability distributions after some coin flips.\nSuppose we start with a fairly strong belief that a coin is fair, represented by this distribution:\n\nplot_beta(30,30)\n\n\n\n\nNow, suppose you flip a coin 20 times and it comes up heads every time. What should you think about the bias of the coin now? According to our model:\n\nplot_beta(30+20,30)\n\n\n\n\nAs you can see, this should cause you to shift your beliefs somewhat.\nThis wasn‚Äôt totally realistic, though. If you picked a coin off the ground, your prior beliefs about it being biased would probably look more like this:\n\nplot_beta(2000,2000)\n\n\n\n\nWhat happens if we now flipped this coin 20 times and it came up heads every time?\n\nplot_beta(2000+20,2000)\n\n\n\n\nYou might be mildly surprised, but those 20 flips wouldn‚Äôt be enough to budge your estimate about the bias of the coin by much.\nFinally, let‚Äôs imagine a situation in which you had a weak prior belief that a coin was biased:\n\nplot_beta(5,1)\n\n\n\n\nNow you flip the coin 100 times and it comes up heads 48 times. What should your updated beliefs be?\n\nplot_beta(5+48,1+52)\n\n\n\n\nAs you can see, the posterior distribution shows that you should think this coin is probably fair now. This illustrates how sufficient evidence can override prior beliefs.\n\n\n5.1.3 Hypothesis averaging\nIn Chapter 3, we solved the generalization problem by summing over all hypotheses, weighted by their posterior probabilities. Here, we can do something similar.\nSuppose we want to know the probability of the next flip coming up heads. In other words, we want to know \\(P(\\text{heads}|D)\\). We can do that by averaging over all possible values of \\(\\theta\\):\n\\[\nP(\\text{heads}|D) = \\int_\\theta P(\\text{heads}|\\theta) \\cdot P(\\theta|D) d\\theta = \\int_\\theta \\theta \\cdot P(\\theta|D) d\\theta\n\\]"
  },
  {
    "objectID": "05-hierarchical-bayes.html#overhypotheses",
    "href": "05-hierarchical-bayes.html#overhypotheses",
    "title": "5¬† Hierarchical generalization",
    "section": "5.2 Overhypotheses üôÜ",
    "text": "5.2 Overhypotheses üôÜ\nNow consider a slightly different situation. You flip 19 different coins in a row, each one time, and they all come up heads. Now you pick up a 20th coin from the same bag as the previous 19 coins. What do you think is the probability of that 20th coin coming up heads? Is it higher than 0.5?\nIf you answered yes, it‚Äôs probably because you formed an overhypothesis about the bias of the coins. After flipping all those coins, you may have concluded that this particular set of coins is more likely than usual to be biased. As a result, your estimate about the probability of the 20th coin coming up heads was higher than it otherwise would be.\n\n5.2.1 The shape bias\nThis coins example is pretty artificial, but the notion of overhypotheses is one that you find in language learning. A phenomenon known as the shape bias refers to the fact that even young children are more likely to generalize a new word based on its shape rather than other properties like color or texture.\n\n\n\nA common task used to test for the shape bias.\n\n\nThis makes sense because objects tend to have common shapes and are less likely to have common colors or textures.\n\n\n5.2.2 Modeling the learning of overhypotheses through hierarchical Bayesian learning\nCharles Kemp, Andy Perfors, and Josh Tenenbaum developed a model of this kind of learning. They focused on bags of black and white marbles rather than flipping coins. They imagine a problem in which you have many bags of marbles that you draw from. After drawing from many bags, you draw a single marble from a new bag and make a prediction about the proportion of black and white marbles in that bag.\nThe details of the model are outside the scope of this book. But the basic idea is that the model learns at two levels simultaneously. At the higher level, the model learns the parameters \\(\\alpha\\) and \\(\\beta\\) of a Beta distribution that characterizes the proportion of black and white marbles in each bag. As we saw above, a Beta distribution can have a peak around a particular proportion, or it can be peaked around both 0 and 1, meaning that each bag is likely to be nearly all black or all white.\nAt the lower level, the model learns the specific distribution of marbles within a bag. If you draw 20 marbles and 5 of them are black, you may have some uncertainty about the overall proportion in the bag, but your best estimate will be around 5/20 or 1/4.\nWhere the model excels is being able to draw inferences across bags. If you see many bags that are full of only black or only white marbles, and then you draw a single black marble out of a new bag, you are likely to be very confident that the rest of the marbles in that bag are black.\nBut if you see many bags that have mixed proportions of black and white marbles, and then you draw a single black marble out of a new bag, you will be far less confident about the proportion of black marbles in that bag. A model that doesn‚Äôt make inferences at multiple levels would struggle to draw this distinction."
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "Why computational modeling?",
    "section": "",
    "text": "The field of psychology is full of theories. But this book is focused on a specific type of theory: the type that can be formalized as a computational model.\nWhat advantage does a computational (or mathematical) theory have over one that can‚Äôt be expressed in computational terms? The biggest advantage is precision. For example, suppose you say that attention is like a spotlight üî¶: you can attend to things that are currently within the light, and you can control where the light is shining, but things outside of the light are outside of our awareness. This is a kind of theory ‚Äì an analogy-based one ‚Äì and it‚Äôs a good start for making some general qualitative predictions about how attention works.\nBut you‚Äôll quickly run into problems if you want to make precise quantitative predictions about attention. How big is the spotlight? Does the size expand and contract? How quickly does it move around? Is attention completely absent outside of the spotlight or does it ramp down as you get near the edge of the light? In other words, if you wanted to build a computer model of this theory, a simple analogy doesn‚Äôt cut it.\nComputational models, if nothing else, force us to be explicit about all of our assumptions.\n\n\nWe don‚Äôt perceive the world as it truly is. As one example, the visible spectrum that our eyes can detect is just a fraction of the full electromagnetic spectrum. Similarly, we can only hear a narrow range of audible sound frequencies. In other words, we perceive an incomplete picture of the surrounding world.\n\n\n\nSource: Abstruse Goose.\n\n\nSimilarly, we are constantly making assumptions about the things we see and hear and using those assumptions to fill in gaps.\nWhat we have in our heads is a kind of model of the world around us ‚Äì what cognitive scientists call a mental representation. These representations help us to reach rapid conclusions about things involving language, causes and effects, concepts, mental states, and many other aspects of cognition.\nSome of the key questions for cognitive scientists who use computational models are:\n\nWhat mental representations do we rely on?\nHow do our minds use these representations to learn when we get new information?\nWhat kind of information do we get and how do our expectations about the kind of information we‚Äôre getting to affect how we use it?\n\nThis book will elaborate, with examples, on each of these questions.\n\n\n\n\n\n\n\n\nTo get some initial experience with computational modeling, you‚Äôll build and experiment with a simple model of classical conditioning developed by Robert Rescorla and Allan Wagner ‚Äì now called the Rescorla-Wagner model.\nAll homework assignments for this book will be done in Google Colab. Click the button above at the top of this section view Homework 1.\nIf you‚Äôre unfamiliar with Colab (or Jupyter Notebooks), watch this brief introduction video.\n\n\n\n\n\n\n\nNote\n\n\n\nYou‚Äôll have to make a copy of the notebook saved to your own Drive in order to edit it."
  },
  {
    "objectID": "01-intro.html#representations",
    "href": "01-intro.html#representations",
    "title": "1¬† Why computational modeling?",
    "section": "1.1 Representations üî∏",
    "text": "1.1 Representations üî∏\nWe don‚Äôt perceive the world as it truly is. As one example, the visible spectrum that our eyes can detect is just a fraction of the full electromagnetic spectrum. Similarly, we can only hear a narrow range of audible sound frequencies. In other words, we perceive an incomplete picture of the surrounding world.\n\n\n\nSource: Abstruse Goose.\n\n\nSimilarly, we are constantly making assumptions about the things we see and hear and using those assumptions to fill in gaps.\nWhat we have in our heads is a kind of model of the world around us ‚Äì what cognitive scientists call a mental representation. These representations help us to reach rapid conclusions about things involving language, causes and effects, concepts, mental states, and many other aspects of cognition.\nSome of the key questions for cognitive scientists who use computational models are:\n\nWhat mental representations do we rely on?\nHow do our minds use these representations to learn when we get new information?\nWhat kind of information do we get and how do our expectations about the kind of information we‚Äôre getting to affect how we use it?\n\nThis book will elaborate, with examples, on each of these questions."
  },
  {
    "objectID": "01-intro.html#homework-1-build-your-first-computational-model",
    "href": "01-intro.html#homework-1-build-your-first-computational-model",
    "title": "1¬† Why computational modeling?",
    "section": "1.2 Homework 1: Build your first computational model üíª",
    "text": "1.2 Homework 1: Build your first computational model üíª\n\n\n\n\n\nTo get some initial experience with computational modeling, you‚Äôll build and experiment with a simple model of classical conditioning developed by Robert Rescorla and Allan Wagner ‚Äì now called the Rescorla-Wagner model.\nAll homework assignments for this book will be done in Google Colab. Click the button above at the top of this section view Homework 1.\nIf you‚Äôre unfamiliar with Colab (or Jupyter Notebooks), watch this brief introduction video.\n\n\n\n\n\n\n\nNote\n\n\n\nYou‚Äôll have to make a copy of the notebook saved to your own Drive in order to edit it."
  },
  {
    "objectID": "03-generalization.html",
    "href": "03-generalization.html",
    "title": "Generalization",
    "section": "",
    "text": "In the last chapter, you learned that much of cognition is about making inferences. A common inference we‚Äôre faced with involves generalizing examples of things to new cases.\nHow can we apply Bayesian inference to these kinds of problems?"
  },
  {
    "objectID": "03-generalization.html#hormones",
    "href": "03-generalization.html#hormones",
    "title": "3¬† Generalization",
    "section": "3.1 Healthy hormone levels üíâ",
    "text": "3.1 Healthy hormone levels üíâ\nThis example comes from a 2001 paper by Josh Tenenbaum and Thomas Griffiths1\nThe basic problem: You learn the value of a healthy hormone level (say, 60) that varies on a scale from 1 to 100 (integers only). What is the probability that another value (say, 70) is also healthy?\n\n3.1.1 Setting up a model\n\n3.1.1.1 The hypothesis space\nTo start with, we‚Äôll assume that healthy values lie in a contiguous interval. Using the term from the paper, this interval is the consequential region \\(C\\).\nThe hypothesis space consists of all possible consequential regions. For example, [0,100], [10,19], and [44,45], are all valid hypotheses. The full hypothesis space is every valid interval between 0 and 100.\n\n\n3.1.1.2 Prior\nHow much weight should we assign to each hypothesis? We might have reason to favor shorter intervals over longer ones, for example. In the paper, they use an Erlang prior. Alternatively, for simplicity of calculation, we could again assume a uniform prior distribution, placing equal weight on all hypotheses, like we did in the previous chapter. This is tantamount to making no prior assumptions about which intervals are most probable.\n\n\n3.1.1.3 Likelihood\nSuppose you learn that a healthy patient has a hormone level of 60. What was the likelihood of observing this value, assuming we know which hypothesis is correct? That is, what is \\(P(x = 60 | h)\\). It depends on how we assume the patient was chosen.\n\n3.1.1.3.1 Weak vs.¬†strong sampling\nUnder weak sampling, we assume that each observation was sampled from the full range of possibilities, and it was just a coincidence that we happened to get one from the consequential region (a healthy patient). If that‚Äôs true, then the probability of getting any particular value doesn‚Äôt depend on which hypothesis is true:\n\\[\nP(x|h) = \\frac{1}{L}\n\\]\nwhere \\(L\\) is the length of the range of possible values (100 in our case).\nUnder strong sampling, we assume that each observation was specifically chosen as an example of the consequential region \\(C\\). In other words, someone chose a healthy person and tested their hormone levels as an example for you. In this case, the probability of seeing a particular value depends on the size of the region:\n\\[\nP(x|h) = \\begin{cases}\n  \\frac{1}{|h|} & \\text{if } x \\in h \\\\\n  0 & \\text{otherwise}\n  \\end{cases}\n\\] where \\(|h|\\) is the size of \\(h\\), i.e., the number of values contained in \\(h\\). If you have multiple observations \\(X = \\{x_1, x_2, \\ldots, x_n \\}\\), then \\(P(X|h) = (1/|h|)^n\\). This is because we will assume each sample is independent like a coin flip.\nThe result of the strong sampling assumption is the size principle: among hypotheses that include all of the observed examples, those that are smallest will receive higher posterior probability because they will have higher likelihoods.\n\n\n\nSample hypotheses. The thickness of the lines indicates their likelihood, depicting the size principle. Image from Griffiths & Tenenbaum (2001).\n\n\n\n\n\n3.1.1.4 Posterior\nWe can now simply apply Bayes‚Äôs rule to compute the probability of each hypothesis, given an observation (or set of observations).\n\\[\nP(h|X) = \\frac{P(X|h) P(h)}{\\sum_{h_i} P(X|h_i) P(h_i)}\n\\]\n\n\n\n3.1.2 Generalizing\nWe aren‚Äôt quite finished. Remember that we really want to know the probability of some new value \\(y\\) also being a healthy hormone level. But at this point all we have done is assigned a probability to each interval being the consequential region.\nWhat we want to do is essentially a two-step process:\n\nFor each hypothesis \\(h\\), check to see if \\(y\\) is in it.\nIf it is, check how probable it is that \\(h\\) is the consequential region \\(C\\), given our observations \\(X\\).\n\nThis basic idea is sometimes known as hypothesis averaging because we don‚Äôt actually care which hypothesis is the right one, so we‚Äôll just average over all hypotheses, weighted by how probable they are. Specifically, we‚Äôll compute:\n\\[\nP(y \\in C|X) = \\sum_h P(y \\in C | h) P(h | X)\n\\] The second term on the right is what we computed earlier using Bayes‚Äôs rule.\nWhat about the first term? This time, we‚Äôll assume weak sampling because there‚Äôs no reason to assume that this new value \\(y\\) was chosen to be a healthy value or not.\n\n\n3.1.3 Homework 2: Finish the details\n\n\n\n\n\nI haven‚Äôt provided all the details for this model because your assignment is to finish the implementation yourself, run some simulations, and collect a small amount of real data to compare the model to."
  },
  {
    "objectID": "03-generalization.html#the-number-game",
    "href": "03-generalization.html#the-number-game",
    "title": "3¬† Generalization",
    "section": "3.2 The number game üî¢",
    "text": "3.2 The number game üî¢\nIn most domains, requiring concepts to be restricted to contiguous intervals is not realistic. Numbers are one example. Consider the space of possible number concepts you could make up for integers between 1 to 100. In addition to concepts like ‚Äúnumbers between 20 and 50‚Äù, there are many other plausible concepts like ‚Äúmultiples of 10‚Äù, ‚Äúeven numbers‚Äù, or ‚Äúpowers of 3‚Äù.\nConsider the following problem: You are given one or more examples \\(X\\) of numbers that fit some rule and you want to know how probable it is that a new number \\(y\\) also fits the rule.\nThe model we discussed before can be naturally extended to this problem. For the likelihood, we can make the same strong sampling assumption as before.\nThe prior is where things get a little trickier. Intuitively some concepts like ‚Äúeven numbers‚Äù seem more probable even before seeing any examples than concepts like ‚Äúmultiples of 7‚Äù. This now becomes a psychological question: Which rules will people find to be more intuitively plausible? There is no single way to decide this, but we could run a survey to find out: Give people a long list of rules and ask them to judge how intuitively natural they seem. We could then construct a prior probability distribution using this data.\nAlternatively, we could come up with some definition of ‚Äúcomplexity‚Äù in hypotheses and assume that less complex hypotheses will receive higher prior probability.\nOnce we have chosen a prior probability distribution \\(P(h)\\), we can now proceed just as we did before."
  },
  {
    "objectID": "03-generalization.html#inductive-generalizations-about-animal-properties",
    "href": "03-generalization.html#inductive-generalizations-about-animal-properties",
    "title": "3¬† Generalization",
    "section": "3.3 Inductive generalizations about animal properties üê¥",
    "text": "3.3 Inductive generalizations about animal properties üê¥\nNow let‚Äôs consider an even more complex generalization problem, based on a 2002 paper by Neville Sanjana and Josh Tenenbaum2. This is the problem of generalizing properties from a set of example animals to other animals. The paper uses the following example:\nChimps have blicketitis\nSquirrels have blicketitis\n--------------------------\nHorses have blicketitis\nThe way to read this is as follows: The premises state that chimps and squirrels have blicketitis. The conclusion is that horses have blicketitis. The inductive generalization question is how probable is the conclusion given the premises? Intuitively, the conclusion in this example seems more plausible than the conclusion in the following example:\nChimps have blicketitis\nGorillas have blicketitis\n--------------------------\nHorses have blicketitis\nThe interesting psychological question is why is it that some generalizations seem more intuitively plausible than others?\n\n3.3.1 Hypothesis space\nThis problem is conceptually similar to the ones we‚Äôve already been discussing. You want to infer what animals have blicketitis after seeing some examples of animals that blicketitis. The first question to answer is what is the analogue of a consequential region for this problem. Animals don‚Äôt naturally fall on a one-dimensional interval so we‚Äôll need to define a different hypothesis space. One possibility is a hierarchy, which naturally captures the knowledge people have about animals.\n\n3.3.1.1 Clustering\nThe paper first creates a hierarchy of eight animals using similarity data collected from people. Specifically, they asked people to judge how similar all pairs of eight animals were and then calculated the average similarity judgment for each animal.\nThese similarity judgments can be used to construct a tree using a simple clustering algorithm. The algorithm works as follows:\n\nPut all animals in their own cluster.\nWhere there is more than one cluster that hasn‚Äôt been placed in a group, do the following:\n\nIdentify the pair of clusters with the greatest similarity between them.\nGroup those clusters into their own new cluster.\n\n\nThere are several approaches for computing the similarity between two clusters that contain multiple animals. For example, you might use the maximum similarity between any pair of individual animals in the two clusters.\nThe results of this algorithm can be represented as a tree, shown below. Each node in the tree represents a cluster. The hypotheses we will consider will be any combination of 1, 2, or 3 of the clusters determined using the clustering algorithm.\n\n\n\nThe tree of animal species. Image from Sanjana & Tenenbaum (2002).\n\n\n\n\n\n3.3.2 The model\nWe can now define the model. First, let‚Äôs define \\(P(h)\\) where \\(h\\) is a set of clusters. The authors make an assumption analogous to the following:\n\\[\nP(h) \\propto \\frac{1}{\\phi^k}\n\\] where \\(k\\) is the number of clusters in \\(h\\). \\(\\phi\\) is a parameter that we can choose. As long as \\(\\phi &gt; 1\\), \\(P(h)\\) will be smaller for hypotheses consisting of more clusters. This has the effect of assigning higher weight to ‚Äúsimpler‚Äù hypotheses.\nOnce again, we will make the strong sampling assumption for the likelihood. This time, \\(|h|\\) is the number of animal species in \\(h\\) and \\(n\\) is the number of examples in the premises.\nA consequence of both of these assumptions is something like Occam‚Äôs razor which says that the simplest explanation should be preferred. This model will assign 0 likelihood to any hypotheses that don‚Äôt include \\(X\\), thus narrowing the hypothesis space down to just hypotheses that are possible. Of those, it will favor hypotheses with fewer clusters and with fewer animals. In other words, it will favor the simplest hypotheses that are consistent with the examples we‚Äôve seen.\n\n\n3.3.3 Try out the model yourself\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import comb\nimport matplotlib.pyplot as plt\n\n\ndef animalGeneralization(premises, phi=20): \n  \n  # - premises is a list of animal names.\n  # - phi is a number that describes the strength of the simplicity\n  #   bias in the prior (default = 20). This function uses a slightly simpler\n  #   version of the prior than the one in the original paper.\n  # \n  # Plots the generalization values and returns them\n  \n  animals = [\"horse\", \"cow\", \"elephant\", \"rhino\", \"chimp\",\n  \"gorilla\", \"mouse\", \"squirrel\", \"dolphin\", \"seal\"]\n  \n  # We'll hard-code the clusters from the tree structure. Here, each column in the\n  # matrix. Each row is a cluster. A 1 means the animal is present in the cluster\n  animal_clusters = np.array([[1,0,0,0,0,0,0,0,0,0], # The singleton clusters\n                              [0,1,0,0,0,0,0,0,0,0],\n                              [0,0,1,0,0,0,0,0,0,0],\n                              [0,0,0,1,0,0,0,0,0,0],\n                              [0,0,0,0,1,0,0,0,0,0],\n                              [0,0,0,0,0,1,0,0,0,0],\n                              [0,0,0,0,0,0,1,0,0,0],\n                              [0,0,0,0,0,0,0,1,0,0],\n                              [0,0,0,0,0,0,0,0,1,0],\n                              [0,0,0,0,0,0,0,0,0,1],\n                              \n                              [1,1,0,0,0,0,0,0,0,0], # The pair clusters\n                              [0,0,1,1,0,0,0,0,0,0],\n                              [0,0,0,0,1,1,0,0,0,0],\n                              [0,0,0,0,0,0,1,1,0,0],\n                              [0,0,0,0,0,0,0,0,1,1],\n                              \n                              [1,1,1,1,0,0,0,0,0,0], # The bigger clusters\n                              [1,1,1,1,1,1,0,0,0,0],\n                              [1,1,1,1,1,1,1,1,0,0],\n                              [1,1,1,1,1,1,1,1,1,1],\n                              ],\n                              dtype = \"int\")\n  \n  animal_clusters_df = pd.DataFrame(animal_clusters, columns = animals)\n  \n  n_clusters = animal_clusters_df.shape[0]\n  n_animals = len(animals)\n  n_hypotheses = int(sum(comb(n_clusters,np.arange(1,4))))\n  \n  # Initialize a hypothesis space of all possible clusters\n  hypotheses = pd.DataFrame(np.zeros((n_hypotheses, n_animals)), \n  columns = animals)\n  priors = np.zeros(n_hypotheses)\n  \n  \n  # The first order hypotheses are just the 19 clusters defined by the tree\n  hypotheses[0:n_clusters] = animal_clusters_df\n  priors[0:n_clusters] = 1/phi\n  \n  # The second order hypotheses are the unique pairs of clusters in the tree\n  i = n_clusters\n  \n  for a in range(0, n_clusters-1):\n    for b in range(a+1, n_clusters):\n      \n      # Take the logical \"or\" of the two clusters from the tree\n      hypotheses[i:i+1] = (np.array(animal_clusters_df[a:a+1], dtype=\"int\") |\n      np.array(animal_clusters_df[b:b+1], dtype=\"int\"))\n      \n      # Update the prior\n      priors[i] = (1/phi)**2\n      \n      i += 1\n      \n  # The third order hypotheses are the unique triples of clusters in the tree\n  for a in range(0, n_clusters-2):\n    for b in range(a+1, n_clusters-1):\n      for c in range(b+1, n_clusters):\n        \n        # Take the logical \"or\" of the three clusters from the tree\n        hypotheses[i:i+1] = (np.array(animal_clusters_df[a:a+1], dtype=\"int\") |\n        np.array(animal_clusters_df[b:b+1], dtype=\"int\") |\n        np.array(animal_clusters_df[c:c+1], dtype=\"int\"))\n        \n        # Update the prior\n        priors[i] = (1/phi)**3\n        \n        i += 1\n            \n            \n  # Now we need to remove the duplicate hypotheses. For example, there's a\n  # {horse} cluster and a {cow} cluster, and there's also a {horse, cow}\n  # cluster. So in our second order hypotheses, there's no need to include\n  # {horse}+{cow}, because it's already included as a first order\n  # hypothesis.\n  \n  # To solve this problem, we'll remove the duplicate rows from our\n  # hypothesis matrix (data frame).\n  \n  # The pandas functions duplicated and drop_duplicates will handle this for us\n  duplicates = hypotheses.duplicated()\n  duplicate_indices = np.logical_not(duplicates)[np.logical_not(duplicates)].index\n  hypotheses = hypotheses.drop_duplicates(ignore_index=True)\n  \n  priors = priors[duplicate_indices]\n  priors = priors / sum(priors) # normalize the prior prob. distribution\n  n_hypotheses = len(priors) # update number of hypotheses to be more accurate\n  \n  # Create the likelihoods\n  likelihood = pd.DataFrame(np.zeros((n_hypotheses, n_animals)), \n  columns = animals)\n            \n  for i in range(0, n_hypotheses):\n    # Set the likelihood equal to 1 over the number of animal species\n    # in the hypothesis. If the animal isn't in the hypothesis, just ignore\n    # it and leave the likelihood at 0.\n    likelihood.loc[i, hypotheses.loc[i,:]==1] = 1/sum(hypotheses.loc[i,:])\n    \n  # Show the model the premises\n  for p in premises:\n    priors = priors * np.array(likelihood[p])\n      \n  priors = priors / sum(priors)\n  \n  # Now compute the generealization probilities by computing a \n  # matrix multiplication of the belief vector and the hypothesis\n  # matrix\n  generalizations = np.matmul(priors, np.array(hypotheses))\n  \n  \n  # Plot the results\n  fig, ax = plt.subplots()\n  y_pos = np.arange(len(animals))\n  \n  \n  ax.barh(animals, generalizations, align='center')\n  ax.set_xlabel(\"Generalization probability\")\n  ax.set_title(\"Premises: \" + \" \".join(premises))\n  \n  plt.show()\n  \n  return(generalizations)\n\nYou can try out a running version of the model by making a copy of the code here. Here, let‚Äôs look at how the model handles a few specific cases.\nLet‚Äôs start with a single example: horses can get blicketitis.\n\nanimalGeneralization([\"horse\"])\n\n\n\n\narray([1.        , 0.52984834, 0.30628906, 0.30628906, 0.19579428,\n       0.19579428, 0.12893614, 0.12893614, 0.0842413 , 0.0842413 ])\n\n\nHere, we see a standard generalization curve. Now, let‚Äôs add a few more animals.\n\nanimalGeneralization([\"horse\", \"cow\", \"mouse\"])\n\n\n\n\narray([1.        , 1.        , 0.57379051, 0.57379051, 0.47852518,\n       0.47852518, 1.        , 0.60980466, 0.1707609 , 0.1707609 ])\n\n\nNow the model has increased probability for most other animals. This makes sense because there is more reason to think blicketitis might affect lots of different animals.\n\nanimalGeneralization([\"horse\", \"cow\", \"mouse\", \"squirrel\"])\n\n\n\n\narray([1.        , 1.        , 0.64983602, 0.64983602, 0.58531333,\n       0.58531333, 1.        , 1.        , 0.18405475, 0.18405475])\n\n\nAdding a squirrel further supported this idea. But what if we add additional examples of animals we‚Äôve already seen?\n\nanimalGeneralization([\"horse\", \"cow\", \"mouse\", \"squirrel\", \"horse\", \"squirrel\"])\n\n\n\n\narray([1.        , 1.        , 0.32551484, 0.32551484, 0.26938358,\n       0.26938358, 1.        , 1.        , 0.06883892, 0.06883892])\n\n\nNow the probabilities for other animals drop, because it‚Äôs starting to look like maybe this disease only affects the four animals we‚Äôve seen so far.\nTo sum up, when we‚Äôve seen a small number of examples, like a single horse, the model will generally prefer simpler hypotheses. But once we‚Äôve seen more data, it will favor more complex hypotheses (like a group of animals from two separate evolutionary clusters) if the data support it.\nAs one final example, let‚Äôs look at the specific impact of multiple examples of a single animal.\n\nanimalGeneralization([\"gorilla\"])\nanimalGeneralization([\"gorilla\", \"gorilla\"])\nanimalGeneralization([\"gorilla\", \"gorilla\", \"gorilla\"])\n\n\n\n\n\n\n\n\n\n\narray([0.01730482, 0.01730482, 0.01730482, 0.01730482, 0.1259028 ,\n       1.        , 0.01270155, 0.01270155, 0.01111944, 0.01111944])\n\n\nHere, we see that the model becomes increasingly confident that the property is unique to gorillas. This makes intuitive sense and it‚Äôs something that people seem to exhibit in their judgments. But, as they point out in the paper, it‚Äôs not something that non-probabilistic models can easily explain.\n\n\n3.3.4 Results\nThe results in the paper show that this model predicts people‚Äôs judgments quite well, better than alternative models that do not rely on Bayesian inference. These results suggest that the assumptions of the model are very similar to the assumptions that people make when making inductive generalizations.\n\n\n\nComparison between model results and human judgments. Image from Sanjana & Tenenbaum (2002)."
  },
  {
    "objectID": "03-generalization.html#footnotes",
    "href": "03-generalization.html#footnotes",
    "title": "3¬† Generalization",
    "section": "",
    "text": "Tenenbaum, J. B. & Griffiths, T. L. (2001). Generalization, similarity, and Bayesian inference. Behavioral and Brain Sciences, 24, 629-640.‚Ü©Ô∏é\nSanjana, N. & Tenenbaum, J. (2002). Bayesian models of inductive generalization. Advances in Neural Information Processing Systems 15‚Ü©Ô∏é"
  },
  {
    "objectID": "09-iterated-learning.html",
    "href": "09-iterated-learning.html",
    "title": "Iterated learning",
    "section": "",
    "text": "If someone told you they defecated their water before drinking it, you probably wouldn‚Äôt be too impressed. Unless you lived in the 1600s, when ‚Äúdefecate‚Äù meant ‚Äúto purify something‚Äù.\nLanguage changes. Words change. Concepts change. Is there any way to predict the conceptual drift that is always happening in our culture?\nYeah, maybe, kinda! But first, some math.\n\n\nWords and concepts change over time. Markov chains provide a useful modeling framework for time-dependent data.\nAs the name suggests, a Markov chain is a chain, specifically of states. At each time step, it moves to a new state. A Markov chain meets the following conditions:\n\nA system can be in a finite number of states.\nThe state at each step of the chain depends only on the previous state. (This dependence can be probabilistic.)\n\nFor example, consider the game Telephone, where one person whispers a word to the next person, they whisper what they hear to the next person, and so on. The set of possible words are the states (finite but admittedly large).\nThe word that Person \\(n+1\\) hears clearly only depends on the word that Person \\(n\\) whispered. But there might be some miscommunication: the probability that Person \\(n+1\\) accurately hears the word is probably less than 1, with similar-sounding words getting higher probability than dissimilar words.\n\n\nConsider a very simple chain of colors that are either red or blue. Here, the state space \\(S = \\{ \\text{red, blue} \\}\\). To specify this Markov chain, we need to define the transition probabilities: the probabilities of moving from each state to every other state.\nLet \\(t_{ij}\\) be the probability of transitioning from state \\(i\\) to state \\(j\\). For example, if \\(t_{RB} = 0.3\\) then the probability of transitioning from the red state to the blue state is 0.3. We can then write the transition probabilities in a matrix.\n\n\nHere‚Äôs our transition probability matrix.\n\n\n\n\nR\nB\n\n\n\n\nR\n0\n1\n\n\nB\n1\n0\n\n\n\nIn this example, \\(t_{RB} = t_{BR} = 1\\). This means that the chain will deterministically alternate between red and blue. Note that the rows of the transition matrix must sum to 1 because they represent all the possible states that the chain might transition to given a current state. Although the columns also sum to 1 in this example, this is not a requirement.\nExample sequence: R B R B R B ‚Ä¶\n\n\n\n\n\n\n\nR\nB\n\n\n\n\nR\n0.25\n0.75\n\n\nB\n0.75\n0.25\n\n\n\nIn this example, we‚Äôd expect to the sequence to flip back and forth between red and blue, but not deterministically. At every step, the sequence is more likely to flip than to stay in the current color.\nExample sequence: R B R R B R R B R B R ‚Ä¶\n\n\n\n\n\n\n\nR\nB\n\n\n\n\nR\n0.75\n0.25\n\n\nB\n0.75\n0.25\n\n\n\nIn this example, no matter what state we‚Äôre in, there is a 0.75 probability of moving to red and a 0.25 probability of moving to blue. Therefore, in this example, we‚Äôd expect the chain to spend about 3/4 of its time in the red state and 1/4 of its time in the blue state. This example makes clear that the columns do not need to sum to 1.\nExample sequence: R R B B R R B B R R R ‚Ä¶\n\n\n\n\nA common question for a given Markov chain is what is its so-called stationary distribution, the proportion of time the chain will spend in each state if it runs for a very long time. For Markov chains a small number of states, computing the stationary distribution is easy to do with some algebra. Consider the following example.\nLet‚Äôs define a transition matrix for Markov chain with two states, \\(s_1\\) and \\(s_2\\):\n\\[\n\\begin{equation}\nT = \\left(\n\\begin{matrix}\nt_{11} & t_{12} \\\\\nt_{21} & t_{22}\n\\end{matrix} \\right)\n\\end{equation}\n\\]\nThe stationary distribution corresponds to the probability of the chain being in state \\(s_1\\) versus \\(s_2\\). We will define \\(\\theta_1\\) as the probability of the chain being in state \\(s_1\\) and \\(\\theta_2\\) as the probability of the chain being in state \\(s_2\\). We can define \\(\\theta_1\\) and \\(\\theta_2\\) recursively as follows:\n\\[\n\\begin{eqnarray}\n\\theta_1 = t_{11} \\theta_1 + t_{21} \\theta_2 \\\\\n\\theta_2 = t_{22} \\theta_2 + t_{12} \\theta_1 \\\\\n\\end{eqnarray}\n\\]\nThese equations essentially say that the probability of being in a state is the probability of being in that state and staying in that state plus the probability of being in the other state and switching states. We will then use the following facts to solve for the \\(\\theta\\)s:\n\\[\n\\begin{eqnarray}\n\\theta_1 + \\theta_2 = 1 \\\\\nt_{11} + t_{12} = 1 \\\\\nt_{21} + t_{22} = 1\n\\end{eqnarray}\n\\]\nUsing some algebra, we get the following expressions:\n\\[\n\\begin{eqnarray}\n\\theta_1 = \\frac{t_{21}}{t_{12}+t_{21}} \\\\\n\\theta_2 = \\frac{t_{12}}{t_{21}+t_{12}}\n\\end{eqnarray}\n\\]\nWe can see that these expressions give sensible results by applying them to the red-blue examples above.\nIn Example 1, unsurprisingly, \\(\\theta_R = \\theta_B = 0.5\\). Because the sequence alternates, it will be red half the time and blue half the time. In Example 2, again \\(\\theta_R = \\theta_B = 0.5\\). Even though the alternating is not deterministic, no matter what state the sequence is in, there is an equal probability that it will switch to the other state. In Example 3, \\(\\theta_R = 0.75\\) and \\(\\theta_B = 0.25\\).\n\n\n\n\nNow let‚Äôs consider a more complicated situation in which each step in a Markov chain generates an observation. A learner gets to see the observations, but not the states themselves. In this situation, we say that the states are ‚Äúhidden‚Äù, hence the name hidden Markov model.\nHidden Markov models are sometimes used to model language production, where the hidden states are the parts of speech (like noun, verb, adjective, or more complex things like noun phrases, objects, and subjects). What we actually get to observe as readers and listeners, however, are words (like ‚Äúdog‚Äù, ‚Äúeats‚Äù, and ‚Äúplay‚Äù). In English, many words are ambiguous and can potentially belong to different parts of speech. ‚ÄúPlay‚Äù, for example could be a noun (‚ÄúThe actors performed in a play‚Äù) or a verb (‚ÄúThe children play in the sand‚Äù). We are constantly decoding the language we observe to infer the parts of speech of the words.\n\n\nA hidden Markov model includes the following pieces:\n\nA set of states \\(S = \\{s_1, \\ldots, s_N\\}\\)\nA set of observations \\(O = \\{o_1, \\ldots, o_M\\}\\)\nA set of initial state probabilities \\(\\Pi = \\{\\pi_1, \\ldots, \\pi_N\\}\\), where \\(\\pi_i\\) is the probability of state \\(i\\) begin the first state in the sequence\nA matrix of state transition probabilities \\(T\\), where \\(t_{ij}\\) in the matrix is the probability of transitioning from state \\(s_i\\) to state \\(s_j\\)\nA matrix of ‚Äúemission‚Äù probabilities \\(B\\), where \\(b_{ik}\\) in the matrix is the probability of producing observation \\(o_k\\) while in state \\(s_i\\)\nAn observation sequence \\(Y = (y_1, \\ldots, y_T)\\), where \\(T\\) is the length of the observed sequence of data\nAn state sequence that is not observed \\(X = (x_1, \\ldots, x_T)\\)\n\n\n\n\nA hidden Markov model. Source: Wikipedia (public domain).\n\n\n\n\n\nUsually the purpose of using an HMM is to infer \\(X\\) from \\(Y\\). There is a standard algorithm for doing this called the Viterbi algorithm, the details of which are beyond the scope of this book.\nThere are many existing programming packages available for working with HMMs. On Python package is hmmlearn, which includes a predict() function, which will predict the most probable sequence of hidden states \\(X\\), given a sequence of observations \\(Y\\) and a fully specified Hidden Markov model. Note that this function will not always be correct. If the model is probabilistic and there are some observations that can be produced in more than one state (for example, the word ‚Äúplay‚Äù could be produced by either the noun or verb state), there will be some cases where it is impossible to be certain which state an observation came from.\n\n\n\n\nSo what does all this have to do with ‚Äúdefecate‚Äù changing meaning over time?\nWell you can think of communication over time as one giant long-term game of Telephone. And an important question you can then ask is: What is the stationary distribution of these chains?\nThis is the subject of a line research in computational cognitive science. The basic model combines some assumptions we‚Äôve seen before with some assumptions of hidden Markov models you just learned about in this chapter.\nFirst, assume that people are trying to learn some concept by seeing an example \\(x_i\\) and getting a label for it \\(y_i\\) and then trying to generalize that label to other instances by forming a hypothesis \\(h\\) about what other things \\(y_i\\) applies to. This is the problem we encountered in the generalization chapter, and we can assume that each person forms their beliefs about \\(h\\) using Bayesian inference.\nNow let‚Äôs add in the communication factor. Each person \\(n+1\\) gets a label \\(y_n\\) from someone else Person \\(n\\), forms their own hypothesis \\(h_{n+1}\\) then labels a new object \\(x_{n+1}\\) for the next person. This process repeats, creating a Markov chain.\nTo simplify this chain, what we end up with is a sequence of states where each state is a hypothesis about the concept.\nWhat will this chain converge to?\nIt turns out that the stationary distribution of this chain is the prior \\(P(h)\\). Because people make mistakes and communication is noisy, over time, the labels \\(y_n\\) become virtually useless and people will converge to their shared prior beliefs.\n\n\nFor this reason, we can use a kind of Telephone game as a way to find out what people‚Äôs shared prior beliefs are.\nA study lead by Michael Kalish tested this idea for function learning: learning to predict \\(y\\) values from \\(x\\) values.\nIn the task, people were given \\(x\\) values and had to predict the corresponding \\(y\\) values using a slider. They got feedback on 50 training examples. Then they did an additional 25 trials without feedback.\nThose 25 test trials were doubled and used as the 50 training examples for the next person, and so on. What functions would people eventually learn?\nThe figure below shows several representative chains. The researchers found that regardless of what the initial training data was, the chains usually converged to a linear increasing function, suggesting that this is what most people were expecting.\n\n\n\nIterated function learning results from Kalish et al.¬†(2007).\n\n\nThis same method has been applied in other domains as well. For example, in a study lead by Jordan Suchow, people had to reproduce as accurately as possible the positions of characters in a word. The initial positions were randomly positioned, and what each person reproduced would be passed to the next person in the chain.\n\n\n\nIterated typesetting results from Suchow et al.¬†(2016).\n\n\nThe researchers found that, over time, people drifted much closer to equal spaced letters, which was much more legible. Moreover, if they averaged the responses across all chains (DZP2 in the figure), they found that people‚Äôs responses were even better than equal spacing, getting closer to Linotype, recommended by designers."
  },
  {
    "objectID": "09-iterated-learning.html#markov-chains",
    "href": "09-iterated-learning.html#markov-chains",
    "title": "9¬† Iterated learning",
    "section": "9.1 Markov chains üîó",
    "text": "9.1 Markov chains üîó\nWords and concepts change over time. Markov chains provide a useful modeling framework for time-dependent data.\nAs the name suggests, a Markov chain is a chain, specifically of states. At each time step, it moves to a new state. A Markov chain meets the following conditions:\n\nA system can be in a finite number of states.\nThe state at each step of the chain depends only on the previous state. (This dependence can be probabilistic.)\n\nFor example, consider the game Telephone, where one person whispers a word to the next person, they whisper what they hear to the next person, and so on. The set of possible words are the states (finite but admittedly large).\nThe word that Person \\(n+1\\) hears clearly only depends on the word that Person \\(n\\) whispered. But there might be some miscommunication: the probability that Person \\(n+1\\) accurately hears the word is probably less than 1, with similar-sounding words getting higher probability than dissimilar words.\n\n9.1.1 Examples\nConsider a very simple chain of colors that are either red or blue. Here, the state space \\(S = \\{ \\text{red, blue} \\}\\). To specify this Markov chain, we need to define the transition probabilities: the probabilities of moving from each state to every other state.\nLet \\(t_{ij}\\) be the probability of transitioning from state \\(i\\) to state \\(j\\). For example, if \\(t_{RB} = 0.3\\) then the probability of transitioning from the red state to the blue state is 0.3. We can then write the transition probabilities in a matrix.\n\n9.1.1.1 Example 1\nHere‚Äôs our transition probability matrix.\n\n\n\n\nR\nB\n\n\n\n\nR\n0\n1\n\n\nB\n1\n0\n\n\n\nIn this example, \\(t_{RB} = t_{BR} = 1\\). This means that the chain will deterministically alternate between red and blue. Note that the rows of the transition matrix must sum to 1 because they represent all the possible states that the chain might transition to given a current state. Although the columns also sum to 1 in this example, this is not a requirement.\nExample sequence: R B R B R B ‚Ä¶\n\n\n9.1.1.2 Example 2\n\n\n\n\nR\nB\n\n\n\n\nR\n0.25\n0.75\n\n\nB\n0.75\n0.25\n\n\n\nIn this example, we‚Äôd expect to the sequence to flip back and forth between red and blue, but not deterministically. At every step, the sequence is more likely to flip than to stay in the current color.\nExample sequence: R B R R B R R B R B R ‚Ä¶\n\n\n9.1.1.3 Example 3\n\n\n\n\nR\nB\n\n\n\n\nR\n0.75\n0.25\n\n\nB\n0.75\n0.25\n\n\n\nIn this example, no matter what state we‚Äôre in, there is a 0.75 probability of moving to red and a 0.25 probability of moving to blue. Therefore, in this example, we‚Äôd expect the chain to spend about 3/4 of its time in the red state and 1/4 of its time in the blue state. This example makes clear that the columns do not need to sum to 1.\nExample sequence: R R B B R R B B R R R ‚Ä¶\n\n\n\n9.1.2 Stationary distributions\nA common question for a given Markov chain is what is its so-called stationary distribution, the proportion of time the chain will spend in each state if it runs for a very long time. For Markov chains a small number of states, computing the stationary distribution is easy to do with some algebra. Consider the following example.\nLet‚Äôs define a transition matrix for Markov chain with two states, \\(s_1\\) and \\(s_2\\):\n\\[\n\\begin{equation}\nT = \\left(\n\\begin{matrix}\nt_{11} & t_{12} \\\\\nt_{21} & t_{22}\n\\end{matrix} \\right)\n\\end{equation}\n\\]\nThe stationary distribution corresponds to the probability of the chain being in state \\(s_1\\) versus \\(s_2\\). We will define \\(\\theta_1\\) as the probability of the chain being in state \\(s_1\\) and \\(\\theta_2\\) as the probability of the chain being in state \\(s_2\\). We can define \\(\\theta_1\\) and \\(\\theta_2\\) recursively as follows:\n\\[\n\\begin{eqnarray}\n\\theta_1 = t_{11} \\theta_1 + t_{21} \\theta_2 \\\\\n\\theta_2 = t_{22} \\theta_2 + t_{12} \\theta_1 \\\\\n\\end{eqnarray}\n\\]\nThese equations essentially say that the probability of being in a state is the probability of being in that state and staying in that state plus the probability of being in the other state and switching states. We will then use the following facts to solve for the \\(\\theta\\)s:\n\\[\n\\begin{eqnarray}\n\\theta_1 + \\theta_2 = 1 \\\\\nt_{11} + t_{12} = 1 \\\\\nt_{21} + t_{22} = 1\n\\end{eqnarray}\n\\]\nUsing some algebra, we get the following expressions:\n\\[\n\\begin{eqnarray}\n\\theta_1 = \\frac{t_{21}}{t_{12}+t_{21}} \\\\\n\\theta_2 = \\frac{t_{12}}{t_{21}+t_{12}}\n\\end{eqnarray}\n\\]\nWe can see that these expressions give sensible results by applying them to the red-blue examples above.\nIn Example 1, unsurprisingly, \\(\\theta_R = \\theta_B = 0.5\\). Because the sequence alternates, it will be red half the time and blue half the time. In Example 2, again \\(\\theta_R = \\theta_B = 0.5\\). Even though the alternating is not deterministic, no matter what state the sequence is in, there is an equal probability that it will switch to the other state. In Example 3, \\(\\theta_R = 0.75\\) and \\(\\theta_B = 0.25\\)."
  },
  {
    "objectID": "09-iterated-learning.html#hmms",
    "href": "09-iterated-learning.html#hmms",
    "title": "9¬† Iterated learning",
    "section": "9.2 Hidden Markov models üôà",
    "text": "9.2 Hidden Markov models üôà\nNow let‚Äôs consider a more complicated situation in which each step in a Markov chain generates an observation. A learner gets to see the observations, but not the states themselves. In this situation, we say that the states are ‚Äúhidden‚Äù, hence the name hidden Markov model.\nHidden Markov models are sometimes used to model language production, where the hidden states are the parts of speech (like noun, verb, adjective, or more complex things like noun phrases, objects, and subjects). What we actually get to observe as readers and listeners, however, are words (like ‚Äúdog‚Äù, ‚Äúeats‚Äù, and ‚Äúplay‚Äù). In English, many words are ambiguous and can potentially belong to different parts of speech. ‚ÄúPlay‚Äù, for example could be a noun (‚ÄúThe actors performed in a play‚Äù) or a verb (‚ÄúThe children play in the sand‚Äù). We are constantly decoding the language we observe to infer the parts of speech of the words.\n\n9.2.1 Formal definition\nA hidden Markov model includes the following pieces:\n\nA set of states \\(S = \\{s_1, \\ldots, s_N\\}\\)\nA set of observations \\(O = \\{o_1, \\ldots, o_M\\}\\)\nA set of initial state probabilities \\(\\Pi = \\{\\pi_1, \\ldots, \\pi_N\\}\\), where \\(\\pi_i\\) is the probability of state \\(i\\) begin the first state in the sequence\nA matrix of state transition probabilities \\(T\\), where \\(t_{ij}\\) in the matrix is the probability of transitioning from state \\(s_i\\) to state \\(s_j\\)\nA matrix of ‚Äúemission‚Äù probabilities \\(B\\), where \\(b_{ik}\\) in the matrix is the probability of producing observation \\(o_k\\) while in state \\(s_i\\)\nAn observation sequence \\(Y = (y_1, \\ldots, y_T)\\), where \\(T\\) is the length of the observed sequence of data\nAn state sequence that is not observed \\(X = (x_1, \\ldots, x_T)\\)\n\n\n\n\nA hidden Markov model. Source: Wikipedia (public domain).\n\n\n\n\n9.2.2 Inference\nUsually the purpose of using an HMM is to infer \\(X\\) from \\(Y\\). There is a standard algorithm for doing this called the Viterbi algorithm, the details of which are beyond the scope of this book.\nThere are many existing programming packages available for working with HMMs. On Python package is hmmlearn, which includes a predict() function, which will predict the most probable sequence of hidden states \\(X\\), given a sequence of observations \\(Y\\) and a fully specified Hidden Markov model. Note that this function will not always be correct. If the model is probabilistic and there are some observations that can be produced in more than one state (for example, the word ‚Äúplay‚Äù could be produced by either the noun or verb state), there will be some cases where it is impossible to be certain which state an observation came from."
  },
  {
    "objectID": "09-iterated-learning.html#cultural-transmission",
    "href": "09-iterated-learning.html#cultural-transmission",
    "title": "9¬† Iterated learning",
    "section": "9.3 Cultural transmission üó£",
    "text": "9.3 Cultural transmission üó£\nSo what does all this have to do with ‚Äúdefecate‚Äù changing meaning over time?\nWell you can think of communication over time as one giant long-term game of Telephone. And an important question you can then ask is: What is the stationary distribution of these chains?\nThis is the subject of a line research in computational cognitive science. The basic model combines some assumptions we‚Äôve seen before with some assumptions of hidden Markov models you just learned about in this chapter.\nFirst, assume that people are trying to learn some concept by seeing an example \\(x_i\\) and getting a label for it \\(y_i\\) and then trying to generalize that label to other instances by forming a hypothesis \\(h\\) about what other things \\(y_i\\) applies to. This is the problem we encountered in the generalization chapter, and we can assume that each person forms their beliefs about \\(h\\) using Bayesian inference.\nNow let‚Äôs add in the communication factor. Each person \\(n+1\\) gets a label \\(y_n\\) from someone else Person \\(n\\), forms their own hypothesis \\(h_{n+1}\\) then labels a new object \\(x_{n+1}\\) for the next person. This process repeats, creating a Markov chain.\nTo simplify this chain, what we end up with is a sequence of states where each state is a hypothesis about the concept.\nWhat will this chain converge to?\nIt turns out that the stationary distribution of this chain is the prior \\(P(h)\\). Because people make mistakes and communication is noisy, over time, the labels \\(y_n\\) become virtually useless and people will converge to their shared prior beliefs.\n\n9.3.1 Using iterated learning to identify people‚Äôs priors\nFor this reason, we can use a kind of Telephone game as a way to find out what people‚Äôs shared prior beliefs are.\nA study lead by Michael Kalish tested this idea for function learning: learning to predict \\(y\\) values from \\(x\\) values.\nIn the task, people were given \\(x\\) values and had to predict the corresponding \\(y\\) values using a slider. They got feedback on 50 training examples. Then they did an additional 25 trials without feedback.\nThose 25 test trials were doubled and used as the 50 training examples for the next person, and so on. What functions would people eventually learn?\nThe figure below shows several representative chains. The researchers found that regardless of what the initial training data was, the chains usually converged to a linear increasing function, suggesting that this is what most people were expecting.\n\n\n\nIterated function learning results from Kalish et al.¬†(2007).\n\n\nThis same method has been applied in other domains as well. For example, in a study lead by Jordan Suchow, people had to reproduce as accurately as possible the positions of characters in a word. The initial positions were randomly positioned, and what each person reproduced would be passed to the next person in the chain.\n\n\n\nIterated typesetting results from Suchow et al.¬†(2016).\n\n\nThe researchers found that, over time, people drifted much closer to equal spaced letters, which was much more legible. Moreover, if they averaged the responses across all chains (DZP2 in the figure), they found that people‚Äôs responses were even better than equal spacing, getting closer to Linotype, recommended by designers."
  },
  {
    "objectID": "04-categorization.html",
    "href": "04-categorization.html",
    "title": "Categorization",
    "section": "",
    "text": "The last chapter introduced the problem of inductive generalization. This chapter will focus on a specific case of generalization that is particular interest to psychologists, cognitive scientists, and people who use AI and machine learning: classification or categorization. Basically, assigning labels to things.\nPeople constantly classify things in the world into categories (chairs, cats, friends, enemies, edible things, and so on). Doing this helps us to communicate and function in novel situations.\nIn machine learning, it‚Äôs often useful to classify inputs into different categories like whether a social media post violates community standards or not, whether an image contains a human face or not, or whether an MRI contains a tumor.\nPsychologists study how people form categories because it provides a window into how we organize our knowledge. But the basic problem is the same as the one studied in machine learning.\nI‚Äôm going to intrdouce a fairly simple psychological model of categorization introduced by Robert Nosofsky called the Generalized Context Model. It assumes that people make classification judgments using the following general algorithm:\nThis model makes some outlandish assumptions (like the idea that people would remember every example they had seen before). But, to a first approximation, it does a decent job of predicting people‚Äôs classification judgments in a lot of situations. And for learning purposes, it has the advantage of being pretty easy to understand and implement."
  },
  {
    "objectID": "04-categorization.html#a-typical-category-learning-experiment",
    "href": "04-categorization.html#a-typical-category-learning-experiment",
    "title": "4¬† Categorization",
    "section": "4.1 A typical category learning experiment üü¢üü®",
    "text": "4.1 A typical category learning experiment üü¢üü®\nIn psychology, category learning experiments have a pretty similar structure. People see some unfamiliar stimuli that differ on several dimensions and can be sorted into different categories. The task is to learn what distinguishes one category from another. The experiments usually consist of two phases: a training phase, and a testing phase.\n\nTraining phase: People see many examples of each category and have to classify them, often simply by guessing at first. They get feedback and gradually learn to tell the different categories apart.\nTesting phase: People see more examples, usually some brand new ones, and have to classify them again. This time they don‚Äôt get feedback. The point of this phase is to test what people actually learned about the meaning of the categories.\n\nThe stimuli could be abstract shapes, cartoon insects, race cars with different features, or anything else. What matters is that they have clearly distinguishable features."
  },
  {
    "objectID": "04-categorization.html#representing-stimuli-in-a-model",
    "href": "04-categorization.html#representing-stimuli-in-a-model",
    "title": "4¬† Categorization",
    "section": "4.2 Representing stimuli in a model üß©",
    "text": "4.2 Representing stimuli in a model üß©\nFor modeling purposes, all of these types of stimuli (with discrete-valued features) can be represented in a matrix, with the following properties:\n\nEach dimension of the matrix represents a different feature of the stimuli.\nThe length of each dimension represents how many different values that feature can have.\nA single item can then be represented by a binary matrix with 1s in the cells indicating which feature values it has and 0s everywhere else.\n\nFor example, in Robert Nosofsky‚Äôs (1986) test of the GCM1, the stimuli were semicircles with lines through them. The two feature dimensions were (1) circle size and (2) line orientation. Each feature had four possible values.\nUsing our binary matrix representation, here is one possible stimulus from the Nosofsky experiment:\n\nimport numpy as np\n\nstimulus = np.array([[0, 0, 0, 0],\n                     [0, 0, 0, 0],\n                     [0, 0, 1, 0],\n                     [0, 0, 0, 0]], dtype=int)\nprint(stimulus)\n\n[[0 0 0 0]\n [0 0 0 0]\n [0 0 1 0]\n [0 0 0 0]]\n\n\nThe Nosofksy stimuli are pretty simple: They can have exactly one value on each dimension, so this representation will always only have a single 1 in the matrix.\nIf we wanted to know how many of each stimulus to present our model, it would be pretty cumbersome to maintain a long list of arrays like this one. So we could instead maintain a single matrix that stores counts of stimuli. That is, the values at each element will represent the number of stimuli observed with those feature values.\nFor example, let‚Äôs consider the ‚Äúdimensional‚Äù condition of Nosofsky‚Äôs experiment, in which subjects saw only the stimuli on the ‚Äúdiagonals‚Äù of the matrix. Let‚Äôs assume they saw each one 100 times (actually, they did a mind-numbing 1200 trials after 2600 practice trials ü•¥). We could represent that like so:\n\ntraining_set = np.array([[100, 0, 0, 100],\n                         [0, 100, 100, 0],\n                         [0, 100, 100, 0],\n                         [100, 0, 0, 100]], dtype=int)\nprint(training_set)\n\n[[100   0   0 100]\n [  0 100 100   0]\n [  0 100 100   0]\n [100   0   0 100]]\n\n\nThis still isn‚Äôt ideal though. Because it doesn‚Äôt represent the category labels that subjects got during their training. We don‚Äôt know which examples belonged to which categories.\nThinking ahead to how the GCM works, let‚Äôs instead represent the training examples more like a list, so that we can iterate through them:\n\ntDimensional = np.zeros((8,3), dtype=int)\ntDimensional[0] = [1,1,1]\ntDimensional[1] = [1,2,2]\ntDimensional[2] = [1,3,2]\ntDimensional[3] = [1,4,1]\ntDimensional[4] = [2,1,4]\ntDimensional[5] = [2,2,3]\ntDimensional[6] = [2,3,3]\ntDimensional[7] = [2,4,4]\n\nprint(tDimensional)\n\n[[1 1 1]\n [1 2 2]\n [1 3 2]\n [1 4 1]\n [2 1 4]\n [2 2 3]\n [2 3 3]\n [2 4 4]]\n\n\nIn this format, each row of the matrix is an array with three elements. The first element is the category label: 1 or 2. The second two elements are the indices into the stimulus matrix. Note: Here I‚Äôve deviated from Python norms to be consistent with the numbering in the table in the original Nosofsky paper, shown below. When working with the model, remember that indexing in Python starts with 0.\n\n\n\nTraining examples from the dimensional condition. Image from Nosofsky (1986)."
  },
  {
    "objectID": "04-categorization.html#the-generalized-context-model-gcm",
    "href": "04-categorization.html#the-generalized-context-model-gcm",
    "title": "4¬† Categorization",
    "section": "4.3 The Generalized Context Model (GCM)",
    "text": "4.3 The Generalized Context Model (GCM)\nHere, I will describe a special case of the model here that applies to tasks (like the one in the paper) with only two categories. However, this model can easily be extended to any number of categories. The model is defined by two key equations.\nThe first equation defines the probability of classifying Stimulus \\(S_i\\) into Category \\(C_1\\) (i.e., Category 1):\n\\[\n\\begin{equation}\nP(C_1|S_i) = \\frac{b_1 \\sum_{j \\in C_1} \\eta_{ij}}{b_1 \\sum_{j \\in C_1} \\eta_{ij} + (1-b_1) \\sum_{k \\in C_2} \\eta_{ik}}\n\\end{equation}\n\\tag{4.1}\\]\nThe sum \\(\\sum_{j \\in C_1}\\) is a sum over all stimuli \\(S_j\\) that belong to Category 1. The sum \\(\\sum_{j \\in C_1}\\) is a sum over all stimuli \\(S_k\\) that belong to Category 2.\nEquation Equation¬†4.1 has a parameter \\(b_1\\) which is defined as the response bias for Category 1. \\(b_1\\) can range from 0 to 1 and captures the possibility that a subject is biased to respond with one category or another. This is similar but not identical to a prior distribution. When \\(b_1\\) is small, the model is biased toward Category 2; when \\(b_1\\) is large, the model is biased toward Category 1. Note that when \\(b = 0.5\\), it cancels out of the equation.\nWe could also include a response bias for Category 2, but the model assumes that \\(\\sum_i b_i = 1\\). Therefore, when there are only two categories, \\(b_2 = 1 - b_1\\) which is what you see in the second term of the denominator of Equation Equation¬†4.1.\n\\(\\eta_{ij}\\) is a function that defines how similar to \\(S_i\\) and \\(S_j\\) are. The GCM assumes that stimuli can be represented as points in a multi-dimensional space (in this case, a two-dimensional space) and the similarity is defined as a function of the distance between those two points:\n\\[\n\\eta_{ij} = e^{-c^2 \\left[w_1 (f_{i1} - f_{j1})^2 + (1-w_1)(f_{i2} - f_{j2})^2 \\right]}\n\\]\nwhere \\(f_{i1}\\) and \\(f_{i2}\\) refer to the feature values on dimensions 1 and 2 of \\(S_i\\). This equation has two parameters: \\(c\\) and \\(w_1\\). \\(c\\) is a scaling parameter that affects how steeply the exponential curve is. This will allow us to account for how different people might have different ideas of how close two stimuli have must be to be called similar. \\(w_1\\) is called the attentional weight for dimension 1. This parameter captures how much weight is placed on dimension 1 over dimension 2. Just like \\(b_1\\), \\(w_1\\) can range from 0 to 1, and the larger it is, the more weight is placed on dimension 1. Similarly, we could add a \\(w_2\\), but the attention weights are constrained to sum to 1.\n\n4.3.1 Generating model predictions\nIn order to generate predictions from this model, we need two things. First, we need to define what the training stimuli are, and what their feature values are. This is what will allow us to compute the sums in Equation Equation¬†4.1.\nNext, we need to specify the values of the 3 parameters \\(b_1\\), \\(c\\), and \\(w_1\\). Nosofsky does this by first collecting data from people in the experiment. That is, for every stimulus \\(S_i\\) we can record the proportion of times people classified it as Category 1. In other words, we can collect empirical estimates of \\(P(C_1|S_i)\\) for all values of \\(S_i\\).\nThen Nosofsky uses a maximum likelihood procedure for fitting the model to the data. Conceptually, the idea is to find the set of values of the three parameters that produce model predictions that are as close as possible to the empirical data. (This is exactly how linear regression works, where you have some data \\((\\bar{x},\\bar{y})\\) and want to find the best-fitting values of \\(a\\) and \\(b\\) for the function \\(y = ax + b\\) that describes the relationship between \\(x\\) and \\(y\\).)\nWe can define model fit by mean squared error (MSE). MSE is defined as\n\\[\n\\frac{1}{n} \\sum_i (y_i - x_i)^2\n\\]\nwhere \\(y\\) are the data and \\(x\\) are the model predictions. A lower MSE means a better model fit. Finding the best values of the parameters can be achieved through an exhaustive search of all values. For example, we might consider all possible values of each parameter in increments of 0.1. We could then write a program that performs the following basic algorithm:\n\nSet minimum MSE to \\(\\inf\\).\nFor all possible values of \\(b_1\\), \\(c\\), and \\(w_1\\):\n\nGenerate model predictions \\(P(R_1|S_i)\\) for all \\(S_i\\).\nCompute MSE between the empirical data and model predictions generated in the previous step.\nIf this MSE is smaller than minimum MSE, set minimum MSE to the current value and store the current values of \\(b_1\\), \\(c\\), and \\(w_1\\).\n\nReturn the stored values of \\(b_1\\), \\(c\\), and \\(w_1\\).\n\n\n\n4.3.2 Homework 3\n\n\n\n\n\nIn your next homework you will implement the GCM described above. In the homework, you will have to encode training and test stimuli and implement the equations and model-fitting algorithm above. Additionally, to get some experience with the behavioral side of cognitive science, you will create a working version of a category learning experiment in order to collect some data that you can use to compare with the GCM‚Äôs predictions."
  },
  {
    "objectID": "04-categorization.html#footnotes",
    "href": "04-categorization.html#footnotes",
    "title": "4¬† Categorization",
    "section": "",
    "text": "Nosofsky, R. M. (1986). Attention, similarity, and the identification-categorization relationship. Journal of Experimental Psychology: General, 115(1), 39-57.‚Ü©Ô∏é"
  }
]