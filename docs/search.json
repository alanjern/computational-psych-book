[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Computational Psychology",
    "section": "",
    "text": "Welcome"
  },
  {
    "objectID": "index.html#thanks",
    "href": "index.html#thanks",
    "title": "Introduction to Computational Psychology",
    "section": "Thanks",
    "text": "Thanks\nThis book borrows inspiration and content (especially Chapters 2 and 7) from Fausto Carcassi‚Äôs Introduction to Cognitive Modelling in R book and I am tremendously grateful to him for sharing his book publicly. Hopefully this resource will be equally helpful to others.\nA number of the modeling examples and homework assignments are adapted from code written by Danielle Navarro from previous iterations of her Computational Cognitive Science course with Andy Perfors. I am very grateful to both of them for making all of their materials public (and well documented).\nYour feedback is welcome and encouraged."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Introduction to Computational Psychology",
    "section": "License",
    "text": "License\n\n\n\n\n\nAnyone is free to reuse and adapt this book for their own non-commercial purposes, with attribution. If you do use this book in any way, please tell me about it."
  },
  {
    "objectID": "01-intro.html#representations",
    "href": "01-intro.html#representations",
    "title": "1¬† Why computational modeling?",
    "section": "1.1 Representations üî∏",
    "text": "1.1 Representations üî∏\nWe don‚Äôt perceive the world as it truly is. As one example, the visible spectrum that our eyes can detect is just a fraction of the full electromagnetic spectrum. Similarly, we can only hear a narrow range of sound frequencies. In other words, we perceive an incomplete picture of the world.\n\n\n\nSource: Abstruse Goose.\n\n\nEven for the parts of the world that we can perceive, the information we get through our senses is often incomplete, so we are constantly making assumptions to fill in gaps.\nAs a result, what we have in our heads is a kind of model of the world around us ‚Äì what cognitive scientists call a mental representation. These representations help us to reach rapid conclusions about things involving language, causes and effects, concepts, mental states, and many other aspects of cognition.\nSome of the key questions for cognitive scientists who use computational models are:\n\nWhat mental representations do we rely on?\nHow do our minds use these representations to learn when we get new information?\nWhat kind of information do we get and how do our expectations about the kind of information we‚Äôre getting to affect how we use it?\n\nThis book focuses on each of these questions."
  },
  {
    "objectID": "01-intro.html#homework-1-build-your-first-computational-model",
    "href": "01-intro.html#homework-1-build-your-first-computational-model",
    "title": "1¬† Why computational modeling?",
    "section": "1.2 Homework 1: Build your first computational model üíª",
    "text": "1.2 Homework 1: Build your first computational model üíª\nTo get some initial experience with computational modeling, you‚Äôll build and experiment with a simple model of classical conditioning developed by Robert Rescorla and Allan Wagner ‚Äì now called the Rescorla-Wagner model.\nAll homework assignments for this book will be done in Google Colab. Click the button below to view Homework 1.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou‚Äôll have to make a copy of the notebook saved to your own Drive in order to edit it.\n\n\nIf you‚Äôre unfamiliar with Colab (or Jupyter Notebooks), watch this brief introduction video.\n\nThis video gives some extra tips for working with Colab."
  },
  {
    "objectID": "10-causal-inference.html#bayes-nets",
    "href": "10-causal-inference.html#bayes-nets",
    "title": "10¬† Causal inference",
    "section": "10.1 Bayes nets ‚û°Ô∏è",
    "text": "10.1 Bayes nets ‚û°Ô∏è\nA Bayes net is a graph that describes the dependencies between all the variables in a situation.\nFor example, let‚Äôs make a Bayes net for the problem in Chapter 5 of inferring the bias of a coin. In that problem, there were three key variables: the bias \\(\\theta\\), the total number of flips \\(n\\), and the number of heads \\(k\\). We can represent this as a Bayes net.\n\n\n\nBayes net representation of the coin bias problem.\n\n\nIn Bayes nets, shaded notes represent variables that are known or observed, and unshaded nodes represent variables that are unknown. We know how many times the coin was flipped and came up heads, but we don‚Äôt directly know the bias \\(\\theta\\).\nWe could extend this Bayes net to capture the generalization problem of predicting the outcome of the next coin flip \\(x\\).\n\n\n\nBayes net representation of the biased coin generalization problem.\n\n\nA complete Bayes net also specifies a probability distribution for each variable. In the example above, \\(k\\) is a function of \\(\\theta\\) and \\(n\\). As we learned in Chapter 5, this is a Binomial distribution. We also need to specify a prior probability over any unknown variables, like \\(\\theta\\). Previously we assumed it was distributed according to a Beta distribution. \\(x\\) is just a single coin flip ‚Äì it‚Äôs a special case of a Binomial distribution called a Bernoulli distribution in which \\(n = 1\\). To sum up:\n\n\\(\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\)\n\\(k \\sim \\text{Binomial}(n,\\theta)\\)\n\\(x \\sim \\text{Binomial}(n=1, \\theta)\\)"
  },
  {
    "objectID": "10-causal-inference.html#causal-intervention",
    "href": "10-causal-inference.html#causal-intervention",
    "title": "10¬† Causal inference",
    "section": "10.2 Causal intervention ü™ö",
    "text": "10.2 Causal intervention ü™ö\nConsider the following two Bayes nets.\n\n\n\nTwo Bayes nets with three variables.\n\n\nTo be concrete, let‚Äôs say that variables \\(s\\) and \\(x\\) represent levels of hormones sonin and xanthan, respectively. Variable \\(z\\) is an unknown variable.\nThe common cause network is so-called because sonin (\\(s\\)) and xanthan (\\(x\\)) are both causally dependent on \\(z\\). The chain network is so-called because all the variables form a causal chain from \\(s\\) to \\(x\\) to \\(z\\). (Note the directions of the arrows in the two Bayes nets.)\nLet‚Äôs see what kind of data these Bayes nets produce. Let‚Äôs assume that each root node of a network (\\(z\\) in the common cause, \\(x\\) in the chain) follows a normal distribution with mean 0 and SD 1. Each link in a network follows a normal distribution with mean equal to the value of its parent node and SD 1.\n\nimport numpy as np\n\nn_samples = 10000\n\n# Common cause\nz_mu = 0\nsd = 1\n\nz_samples_cc = np.zeros(n_samples)\ns_samples_cc = np.zeros(n_samples)\nx_samples_cc = np.zeros(n_samples)\n\nfor i in range(n_samples):\n  z_samples_cc[i] = np.random.normal(z_mu, sd)\n  s_samples_cc[i] = np.random.normal(z_samples_cc[i], sd)\n  x_samples_cc[i] = np.random.normal(s_samples_cc[i], sd)\n\n# Chain\nx_mu = 0\n\nz_samples_chain = np.zeros(n_samples)\ns_samples_chain = np.zeros(n_samples)\nx_samples_chain = np.zeros(n_samples)\n\nfor i in range(n_samples):\n  x_samples_chain[i] = np.random.normal(x_mu, sd)\n  z_samples_chain[i] = np.random.normal(x_samples_chain[i], sd)\n  s_samples_chain[i] = np.random.normal(z_samples_chain[i], sd)\n\nBecause \\(z\\) represents an unknown variable, let‚Äôs plot just \\(s\\) and \\(x\\).\n\nimport matplotlib.pyplot as plt\n\nfig, (ax1, ax2) = plt.subplots(1,2)\nax1.scatter(s_samples_cc, x_samples_cc, alpha = 0.1)\nax1.set_xlabel(\"s\")\nax1.set_ylabel(\"x\")\nax1.set_title(\"Common cause\")\n\nax2.scatter(s_samples_chain, x_samples_chain, alpha = 0.1)\nax2.set_xlabel(\"s\")\nax2.set_title(\"Chain\")\n\nplt.show()\n\n\n\n\nClearly, the data isn‚Äôt identical in the two cases, but data generated by both Bayes nets results in a strong positive correlation between \\(s\\) and \\(x\\).\nImagine you didn‚Äôt know how this data was generated and you just got one of these plots. Could you use it to tell whether it was produced by a common cause structure or a chain structure?\nSorry, but no. üòî Just knowing the data are positively correlated doesn‚Äôt give you enough information to figure out how \\(s\\) and \\(x\\) are causally related.\nBut what if you could manipulate the variables? That is, what if you could intervene on sonin levels and see what effect it had on xanthan levels?\n\nIf you increase the sonin levels üìà and the xanthan levels also increase üìà, then the causal structure must be a chain.\nIf you increase the sonin levels üìà and the xanthan levels don‚Äôt change ‚ùå, then the causal structure can‚Äôt be a chain (and therefore must be a common cause, because that‚Äôs the only other option we‚Äôre considering).\n\n\n10.2.1 Graph surgery\nThis intuition can be illustrated visually on the Bayes nets by performing ‚Äúsurgery‚Äù on the graphs. It works like this:\n\nRemove all incoming connections to the variable you‚Äôre intervening on.\nIf there‚Äôs still a path between the variable you intervened on and another variable, then you should still expect those variables to be related.\n\nLet‚Äôs apply this idea to our common cause and chain Bayes nets.\n\n\n\nGraph surgery applied to the common cause and chain Bayes nets. The invervention is indicated by the red arrow.\n\n\nAfter intervening on sonin levels (\\(s\\)), we remove the connection to \\(s\\) in the common cause network, but no connections in the chain network. The resulting Bayes nets show why we should expect to see a resulting change in xanthan for the chain, but not the common cause.\n\n\n10.2.2 Do people intuitively understand the logic of casual intervention?\nA study by Michael Waldmann and York Hagmayer presented people with either the common cause or the chain structure. They were told that sonin and xanthan were hormone levels in chimps and they got some example data that allowed them to learn that the hormone levels were positively correlated.\nThen they were either assigned to a doing or seeing condition. People in the doing condition were asked to imagine that 20 chimps had their sonin levels raised (or lowered). They then predicted how many of the chimps would have elevated xanthan levels. People in the seeing condition got essentially the same information but just learned that the chimps‚Äô sonin levels were high (not that it had been intentionally raised).\nAverage results and model predictions are below.\n\n\n\nResults from Waldmann & Hagmayer (2005), Experiment 2.\n\n\nPeople‚Äôs judgments mostly followed those of the Bayes net model predictions. In the common cause case, when the sonin levels are artificially raised, the relationship between sonin and xanthan is decoupled, so the model reverts to a base rate prediction about xanthan levels (and so did people, for the most part). But when just observing elevated sonin levels, the model should expect the positive relationship to hold.\nIn the chain case, the predictions are the same for seeing or doing, because intervening doesn‚Äôt change anything about the relationship between sonin and xanthan. People‚Äôs judgments indicate that they understood this."
  },
  {
    "objectID": "10-causal-inference.html#structure-strength",
    "href": "10-causal-inference.html#structure-strength",
    "title": "10¬† Causal inference",
    "section": "10.3 Causal structure and strength üèóüí™",
    "text": "10.3 Causal structure and strength üèóüí™\nBayes nets can also account for how people judge the strength of evidence for a causal relationship after seeing some data. This was the idea that Tom Griffiths and Josh Tenenbaum explored in a 2005 computational study.\nHere‚Äôs the basic problem they considered. Suppose researchers perform an experiment with rats to test whether a drug causes a gene to be expressed. A control group of rats doesn‚Äôt get the injection and the experimental group does. They record the number of rats in each group that express the gene.\nHere are some possible results from an experiment with 8 rats in each group. The table shows how many rats in each group expressed the gene.\n\n\n\nControl üêÄ\nExperimental üêÄ\n\n\n\n\n6/8 üß¨\n8/8 üß¨\n\n\n4/8 üß¨\n6/8 üß¨\n\n\n2/8 üß¨\n4/8 üß¨\n\n\n0/8 üß¨\n2/8 üß¨\n\n\n\nIn each of these hypothetical experiments, how strongly would you say that the drug causes the gene to be expressed?\nThese are a few of the cases that were included in an experiment conducted by Marc Buehner and Patricia Cheng. Here‚Äôs the full set of averaged human results.\n\n\n\nResults from Buehner & Cheng (1997), Experiment 1B, reproduced by Griffiths & Tenenbaum (2005). p(e+|c+) is the probability of the effect being present (e.g., the gene being expressed) given that the cause is present (e.g., the drug was administered). p(e+|c-) means the probability of the effect being present given that the cause is absent.\n\n\nFocusing just on the cases in the table, on average, people judged that the drug was less likely to have a causal effect on the gene as the total number of rats expressing the gene decreased, even when the difference in number of rats expressing the gene between conditions was held constant.\n\n10.3.1 The causal support model\nMaybe people reason about these problems by performing a kind of model selection between the two Bayes nets below.\n\n\n\nCausal inference as model selection. In one model, there is a connection from the potential cause to the effect; in the other, the two variables are unconnected. (Image from Griffiths & Tenenbaum (2005).)\n\n\nThese Bayes nets each have three variables: an effect \\(E\\), a cause \\(C\\), and a background cause \\(B\\). For our problem, the effect and cause refer to the gene and the drug. The inclusion of the background cause is to account for unknown factors that might cause the gene to be expressed without the drug.\nThe problem people are faced with is deciding which of these two models is best supported by the data \\(D\\) ‚Äì the number of times the effect occurred with and without the potential cause.\nThis can be done with Bayesian inference:\n\\[\nP(\\text{Graph } i) \\propto P(D|\\text{Graph } i) P(\\text{Graph } i)\n\\]\nBecause there are only two possible networks, we can compute the relative evidence for one Bayes net over the other as a ratio. We can then take the log of the expression to simplify it:\n\\[\n\\log \\frac{P(\\text{Graph } 1|D)}{P(\\text{Graph } 0|D)} = \\log \\frac{P(D | \\text{Graph } 1)}{P(D | \\text{Graph } 0)} + \\log \\frac{P(\\text{Graph } 1)}{P(\\text{Graph } 0)}\n\\]\nRegardless of what prior probabilities we assign to the two graphs, the relative evidence for one graph over the other is entirely determined by the log-likelihood ratio. This is defined as causal support: \\(\\log \\frac{P(D | \\text{Graph } 1)}{P(D | \\text{Graph } 0)}\\).\nComputing \\(P(D | \\text{Graph } 1)\\) requires fully specifying the Bayes net. We‚Äôll assume that \\(P(E|B) = w_0\\) and \\(P(E|C) = w_1\\). When both \\(B\\) and \\(C\\) are present, we‚Äôll assume they contribute independently to causing \\(E\\), and therefore operate like a probabilistic OR function:\n\\[\nP(e^+|b,c; w_o, w_1) = 1 - (1-w_0)^b (1-w_1)^c\n\\]\nHere, when the B or C are present, \\(b\\) and \\(c\\) are set to 1, and when they are absent, \\(b\\) and \\(c\\) are set to 0.\nThe likelihood for Graph 1 is therefore\n\\[\nP(D | w_0, w_1, \\text{Graph } 1) = \\prod_{e,c} P(e|b^+,c; w_o, w_1)^{N(e,c)}\n\\]\nwhere the product is over the possible settings of \\(e\\) and \\(c\\) (effect absent/cause absent, effect absent/cause present, ‚Ä¶) and the \\(N(e,c)\\) values are counts of times that these outcomes happened in the data \\(D\\).\nHere‚Äôs a function to compute this.\n\ndef compute_likelihood(data, w0, w1, graph):\n  '''Returns likelihood of data for a given graph\n     \n     Parameters:\n       data (list): observation counts in this order:\n         N(c-,e-), N(c-,e+), N(c+,e-), N(c+,e+)\n       w0 (float): probability of background cause producing effect\n       w1 (float): probability of cause of interest producing effect\n       graph (int): 0 (Graph 0) or 1 (Graph 1)\n\n     Returns:\n       (float): probability of data\n  '''\n  \n  if graph == 0:\n    # e-\n    p = (1-w0)**(data[0]+data[2])\n    # e+\n    p = p * w0**(data[1]+data[3])\n  elif graph == 1:\n    # c-, e-\n    p = (1-w0)**data[0]\n    # c-, e+\n    p = p * w0**data[1]\n    # c+, e-\n    p = p * (1 - (w0 + w1 - w0*w1))**data[2]\n    # c+, e+\n    p = p * (w0 + w1 - w0*w1)**data[3]\n  else:\n    # error!\n    return(0)\n  \n  return(p)\n\nCausal support doesn‚Äôt actually depend directly on the parameters \\(w_0\\) and \\(w_1\\). The reason is that we ultimately don‚Äôt care what the values of these parameters are because we just want to draw an inference at a higher level about the best-fitting Bayes net.\nMathematically speaking, \\(w_0\\) and \\(w_1\\) are averaged out of the model, an idea we first saw in Chapter 3. We can accomplish this using Monte Carlo approximation, introduced in Chapter 8.\n\ndef estimate_likelihood(graph_number, data): \n  '''Returns an estimate of the probability of observing the data\n     under the specified graph using Monte Carlo estimation.\n     \n     Parameters:\n       graph_number (int): either 0 (Graph 0) or 1 (Graph 1)\n       data (list): observation counts in this order:\n         N(c-,e-), N(c-,e+), N(c+,e-), N(c+,e+)\n\n     Returns:\n       (float): probability of observing data\n  '''\n  from numpy.random import default_rng\n  \n  n_samples = 5000\n  rng = default_rng(2022)\n  mc_samples = np.zeros(n_samples)\n  \n  for i in range(n_samples):\n    # Sample values for w0 and w1 from a uniform distribution\n    w0 = rng.random()\n    w1 = rng.random()\n    \n    mc_samples[i] = compute_likelihood(data, w0, w1, graph_number)\n  \n  return(1/n_samples * np.sum(mc_samples))\n\nFinally, let‚Äôs put it all together by writing a function that computes causal support.\n\ndef causal_support(data):\n  '''Returns a causal support value for a given data set.\n     Causal support is a measure of how strongly a data\n     set indicates that there is evidence for a causal\n     effect.\n     \n     Parameters:\n       data (list): observation counts in this order:\n         N(c-,e-), N(c-,e+), N(c+,e-), N(c+,e+)\n\n     Returns:\n       (float): strength of evidence for causal relationship\n  '''\n  import numpy as np\n  \n  return (np.log(estimate_likelihood(1,data=data) / \n                 estimate_likelihood(0,data=data)))\n\nNow let‚Äôs see what the model predicts for the four cases we considered earlier.\n\ncausal_support_predictions = [causal_support([2,6,0,8]),\n                              causal_support([4,4,2,6]),\n                              causal_support([6,2,4,4]),\n                              causal_support([8,0,6,2])]\n                              \nlabels = [\"6/8, 8/8\", \"4/8, 6/8\", \"2/8, 4/8\", \"0/8, 2/8\"]\nfig, ax = plt.subplots()\nax.bar(labels, causal_support_predictions)\nax.set_xlabel(\"Data\")\nax.set_ylabel(\"Model prediction\")\n\nplt.show()\n\n\n\n\nThe labels on the x-axis indicate the control condition counts, followed by the experimental condition counts.\nYou can see that the model‚Äôs predictions largely follow the pattern in the data from earlier. The model favors Graph 1 in the two leftmost cases, is essentially uncertain in the third case, and begins to think the evidence favors no causal relationship in the rightmost case.\nThe rest of the Griffiths & Tenenbaum paper shows how causal support is able to capture some subtle aspects of people‚Äôs causal judgments that other models that don‚Äôt incorporate both structure and strength fail to capture."
  },
  {
    "objectID": "02-bayes.html#basic-probability",
    "href": "02-bayes.html#basic-probability",
    "title": "2¬† Bayesian inference",
    "section": "2.1 Basic probability üé≤",
    "text": "2.1 Basic probability üé≤\n\nConditional probability: \\(P(b|a) = \\frac{P(a,b)}{P(a)}\\)\nChain rule: \\(P(a,b) = P(b|a)P(a)\\)\nMarginalization: \\(P(d) = \\sum_h P(d,h) = \\sum_h P(d|h) P(h)\\)\nBayes‚Äôs rule: \\(P(h|d) = \\frac{P(d|h) P(h)}{P(d)} = \\frac{P(d|h) P(h)}{\\sum_h P(d|h) P(h)}\\). \\(P(d|h)\\) is referred to as the likelihood, \\(P(h)\\) is the prior, and \\(P(h|d)\\) is the posterior."
  },
  {
    "objectID": "02-bayes.html#a-motivating-example-sampling-from-a-bag",
    "href": "02-bayes.html#a-motivating-example-sampling-from-a-bag",
    "title": "2¬† Bayesian inference",
    "section": "2.2 A motivating example: Sampling from a bag üëù",
    "text": "2.2 A motivating example: Sampling from a bag üëù\nSuppose you have a bag full of black and red balls. You can‚Äôt see inside the bag and you don‚Äôt know how many black and red balls are inside, but you know that there are nine total balls in the bag.\nYou want to know how many black balls and red balls there are. There are a finite number of hypotheses: {0 black balls, 1 black ball, 2 black balls, ‚Ä¶, 9 black balls}. Let‚Äôs call these hypotheses \\(B_0\\), \\(B_1\\), etc., respectively.\nYou don‚Äôt know which hypothesis is true, but you might have some idea that some hypotheses are more likely than others. It‚Äôs natural to represent your uncertainty with a probability distribution over the possible unknown states that the world could be in ‚Äì in this case, the ten hypotheses. Each hypothesis gets assigned a probability and the probabilities sum to 1.\nFor starters, let‚Äôs assume that you don‚Äôt have any idea which hypotheses are more likely. In other words, you give every hypothesis the same probability: 1/10 = 0.1. This is called a uniform distribution over hypotheses. This distribution is your prior.\nNow suppose you put your hand in the bag and pull out a ball at random. The possible observations are: {black, red}, Let‚Äôs call them \\(B\\) and \\(R\\), respectively. The probability of observing each color depends on which hypothesis is true, i.e., how many balls of each color are in the bag. For instance, if \\(B_0\\) is true (there are 0 black balls in the bag), then the probability of observing a red ball is 1 (\\(P(R|B_0)=1\\)), and the probability of observing a black ball is 0 (\\(P(B|B_0)=0\\)). These expressions that tell us how probable our observations are, given a specific hypothesis, are your likelihoods.\n\n2.2.1 Sampling from the generative model\nNow we have a distribution over hypotheses (a prior), \\(P(h)\\), and a distribution over observations given each hypothesis (a likelihood), \\(P(d|h)\\). These two things allow us to create a generative model, a model for sampling new data.\nHow do we sample from the generative model? Note that which hypothesis \\(h\\) is true does not depend on the data, while the data \\(d\\) depends on which hypothesis is true. Therefore, we can sample from the generative model using the following two-step process:\n\nSample a hypothesis from the prior.\nSample data given the hypothesis, using the likelihood.\n\nLet‚Äôs first create an array with the probability of each hypothesis:\n\nimport numpy as np\nimport random\n\nrandom.seed(2022) # set random seed to get same results every time\n\nh_priors = np.repeat(0.1,10)\nprint(h_priors)\n\n[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n\n\nNow we‚Äôll do the first step: create an array of 10000 hypotheses sampled from the prior:\n\nprior_samples = np.array(random.choices(np.arange(0,10,1),\n                   weights = h_priors, \n                   k = 10000))\n                   \nprint(prior_samples[0:9]) # printing out just a few\n\n[5 4 3 0 7 9 4 6 8]\n\n\nHere, each number corresponds to one hypothesis: 0 corresponds to \\(B_0\\), 1 to \\(B_1\\), and so on. Each sample represents one possible way (a hypothesis) the world could be. Since the prior was uniform (each hypothesis had the same probability), each hypothesis appears about equally often. We can plot all the samples to verify:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme()\nfig, ax = plt.subplots()\nsns.histplot(prior_samples, bins=10)\nax.set_xlabel('Prior sample')\nax.set_ylabel('Number of samples')\n\nText(0, 0.5, 'Number of samples')\n\n\n\n\n\nNow for the next step. For each sample in prior_samples, we want to sample an observation. To do that, let‚Äôs pause for a second and think about the probability of pulling a black ball given that hypothesis \\(B_3\\) is true, for example. This means that there are 3 black balls and 6 red balls in the bag. So the probability of pulling a black ball from the bag at random will be 3/9.\nGeneralizing this idea, we can get the probability of pulling a black ball from the bag by dividing the elements of prior_samples by 9:\n\np_black = prior_samples / 9\nprint(p_black[0:4]) # print out just a few\n\n[0.55555556 0.44444444 0.33333333 0.        ]\n\n\nNow, to complete our generative model, we just need to sample one value for each element of p_black. Each sample represents a draw from a bag.\n\nball_samples = np.random.binomial(n = np.repeat(1,len(p_black)),\n                                  p = p_black)\nprint(ball_samples[0:9])\n\n[0 1 0 0 1 1 1 1 1]\n\n\nball_samples is 1 for black and 0 for red. Once again, let‚Äôs plot all our samples.\n\nfig, ax = plt.subplots()\nsns.histplot(ball_samples, bins=2)\nax.set_xticks(ticks=[0,1])\nax.set_ylabel(\"Number of samples\")\n\nText(0, 0.5, 'Number of samples')\n\n\n\n\n\nYou can think of this plot representing your overall beliefs about the number of red and black balls in the bag, averaged over all possible hypotheses.\nNot surprisingly, we got about equal numbers of red and black balls. This makes sense: We didn‚Äôt have any prior expectations about whether red or black balls were more likely in the bag.\nHow should your beliefs change after you pull a ball out of the bag? That is, how should you respond to evidence?"
  },
  {
    "objectID": "02-bayes.html#bayesian-updating-learning-from-evidence",
    "href": "02-bayes.html#bayesian-updating-learning-from-evidence",
    "title": "2¬† Bayesian inference",
    "section": "2.3 Bayesian updating: Learning from evidence ü§î",
    "text": "2.3 Bayesian updating: Learning from evidence ü§î\nLet‚Äôs apply Bayes‚Äôs rule to see how to optimally incorporate new data into your beliefs.\n\n2.3.1 Applying Bayes‚Äôs rule to the bag case\nSuppose you have a uniform prior distribution over the 10 hypotheses about balls in the bag. Now you pick a ball and it‚Äôs black. Given this observation \\(B\\), how should you change the probabilities you give to each hypothesis?\nIntuitively, you should now give a little bit more probability to those hypotheses that have more black balls than red balls, because those are the hypotheses that make your observations more likely. Moreover, you can definitively exclude hypothesis \\(B_0\\), because your observation would be impossible if \\(B_0\\) were true. Let‚Äôs calculate this with Bayes‚Äôs rule.\nThe prior is the array h_priors defined above. Given that we have observed \\(B\\), the likelihood should tell us, for each hypothesis, the probability of \\(B\\) given that hypothesis. For example, for \\(B_9\\), the likelihood \\(P(B|B_9) = 1\\). For \\(B_8\\), \\(P(B|B_8) = 8/9\\), because 8 of the 9 balls are black.\nGeneralizing this idea, \\(P(B|B_n) = n/9\\). We can therefore compute the likelihoods for all hypotheses in a vector:\n\nlikelihoods = np.arange(0,10,1) / 9\n\nprint(likelihoods)\n\n[0.         0.11111111 0.22222222 0.33333333 0.44444444 0.55555556\n 0.66666667 0.77777778 0.88888889 1.        ]\n\n\nNow suppose we want to find the probability of hypothesis \\(B_5\\) after observing one draw \\(B\\). Let‚Äôs apply Bayes‚Äôs rule:\n\\[P(B_5 | B) = \\frac{P(B|B_5) P(B_5)}{\\sum_h{p(B|h) P(h)}}\\]\nLet‚Äôs compute the parts we need to calculate \\(P(B_5 | B)\\).\n\n# Prior\np_B5 = h_priors[3]\n\n# Likelihood\nlikelihood_B5 = likelihoods[5]\n\n# Data\np_B = sum(likelihoods*h_priors)\n\n# Posterior\np_B5_given_B = p_B5 * likelihood_B5 / p_B\n\n# Print out results\nprint(\"P(B5) = \" + str(p_B5)) # Prior\nprint(\"P(B|B5) = \" + str(likelihood_B5)) # Likelihood\nprint(\"P(B) = \" + str(p_B)) # Data\nprint(\"P(B5|B) = \" + str(p_B5_given_B)) # Posterior\n\nP(B5) = 0.1\nP(B|B5) = 0.5555555555555556\nP(B) = 0.5\nP(B5|B) = 0.11111111111111112\n\n\nLet‚Äôs update the probabilities for all hypotheses in a more compact way.\n\nposteriors = (likelihoods * h_priors) / sum(likelihoods * h_priors)\n\nfor i in range(len(posteriors)):\n  print(\"P(B\" + str(i) + \"|B) = \" + str(posteriors[i]))\n\nP(B0|B) = 0.0\nP(B1|B) = 0.022222222222222223\nP(B2|B) = 0.044444444444444446\nP(B3|B) = 0.06666666666666667\nP(B4|B) = 0.08888888888888889\nP(B5|B) = 0.11111111111111112\nP(B6|B) = 0.13333333333333333\nP(B7|B) = 0.15555555555555556\nP(B8|B) = 0.17777777777777778\nP(B9|B) = 0.2\n\n\nAs expected, Bayes‚Äôs rule says we should increase the probability we assign to hypotheses with more black balls than red balls. Additionally, let‚Äôs double-check that the posterior probabilities sum to 1 (a requirement for a valid probability distribution).\n\nsum(posteriors)\n\n1.0\n\n\nFinally, let‚Äôs plot the posterior probabilities.\n\nhypotheses = ('B0', 'B1', 'B2', 'B3', 'B4',\n              'B5', 'B6', 'B7', 'B8', 'B9')\ny_pos = np.arange(len(hypotheses))\n\nfig, ax = plt.subplots()\nsns.barplot(posteriors, orient=\"y\")\nax.set_yticks(ticks = y_pos, labels=hypotheses)\nax.set_ylabel('Hypothesis')\nax.set_xlabel('Probability')\n\nText(0.5, 0, 'Probability')\n\n\n\n\n\n\n\n2.3.2 How to avoid calculating P(d)\nIn practice, we generally do not need to calculate the \\(P(d)\\) (the denominator in Bayes‚Äôs rule) explicitly. I‚Äôll give you the general idea why in this section.\nFirst, we create an array of prior probabilities, which has as many components as there are hypotheses. We‚Äôll just reuse h_priors. Note that the probabilities sum to 1, as they should because it‚Äôs a probability distribution.\n\nh_priors\n\narray([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\n\n\nNext, we create a likelihood array. When we did calculations above, we only had an array with the likelihoods for a specific observation. However, we would like to have something that encodes the likelihood function for each possible observation given each possible hypothesis, rather than just for a specific observation.\nIn this example, there are two possible observations: \\(B\\) and \\(R\\). We can encode the likelihood as an \\(m \\times n\\) array where \\(m\\) is the number of hypotheses and \\(n\\) is the number of possible observations. In our case: \\(10 \\times 2\\).\n\nlikelihood_array = np.array((np.arange(0,10,1) / 9,\n                             1-(np.arange(0,10,1) / 9))).T\nprint(likelihood_array)\n\n[[0.         1.        ]\n [0.11111111 0.88888889]\n [0.22222222 0.77777778]\n [0.33333333 0.66666667]\n [0.44444444 0.55555556]\n [0.55555556 0.44444444]\n [0.66666667 0.33333333]\n [0.77777778 0.22222222]\n [0.88888889 0.11111111]\n [1.         0.        ]]\n\n\nNow we multiply the prior and likelihoods together (the numerator of Bayes‚Äôs rule) element-wise (first element gets multiplied with first element, second element by second element, etc.):\n\nprior_array = np.array((h_priors, h_priors)).T\nbayes_numerator = likelihood_array * prior_array\n\nprint(bayes_numerator)\n\n[[0.         0.1       ]\n [0.01111111 0.08888889]\n [0.02222222 0.07777778]\n [0.03333333 0.06666667]\n [0.04444444 0.05555556]\n [0.05555556 0.04444444]\n [0.06666667 0.03333333]\n [0.07777778 0.02222222]\n [0.08888889 0.01111111]\n [0.1        0.        ]]\n\n\nFinally, we want a distribution for each column, i.e., a distribution over hypotheses given each observation. Therefore, we sum each column and then divide each element by the sum of its column:\n\nposteriors = bayes_numerator / np.sum(bayes_numerator, axis = 0)\nprint(posteriors)\n\n[[0.         0.2       ]\n [0.02222222 0.17777778]\n [0.04444444 0.15555556]\n [0.06666667 0.13333333]\n [0.08888889 0.11111111]\n [0.11111111 0.08888889]\n [0.13333333 0.06666667]\n [0.15555556 0.04444444]\n [0.17777778 0.02222222]\n [0.2        0.        ]]\n\n\nAnd that gives us the posterior without us having to explicitly calculate the evidence for each observation!\n\n\n\n\n\n\nNote\n\n\n\nThe general idea is this. Because the denominator of Bayes‚Äôs rule, for a fixed observation, is a constant, you can usually get away with computing \\(P(d|h) P(h)\\) for every possible hypothesis \\(h\\) and then ‚Äúnormalize‚Äù the resulting values so that they sum to 1 (remember that they have to in order for it to be a valid probability distribution)."
  },
  {
    "objectID": "02-bayes.html#bayes-exercises",
    "href": "02-bayes.html#bayes-exercises",
    "title": "2¬† Bayesian inference",
    "section": "2.4 Exercises üìù",
    "text": "2.4 Exercises üìù\n\n2.4.1 Taxi cabs\n80% of the taxi cabs in Simpletown are green and 20% are yellow1. A hit-and-run accident happened at night involving a taxi. A witness claimed that the taxi was yellow. After extensive testing, it is determined that the witness can correctly identify the color of a taxi only 75% of the time under conditions like the ones present during the accident. What is the probability that the taxi was yellow?\n\n\n2.4.2 Flipping coins\nYou observe a sequence of coin flips and want to determine if the coin is a trick coin (always comes up heads) or a normal coin. Let \\(P(\\text{heads}) = \\theta\\). Let \\(h_1\\) be the hypothesis that \\(\\theta = 0.5\\) (fair coin). Let \\(h_2\\) be the hypothesis that \\(\\theta = 1\\) (trick coin).\nFor this problem, we will define something called prior odds, which is the ratio of prior probabilities assigned to two hypotheses: \\(\\frac{P(h_1)}{P(h_2)}\\). Because most coins aren‚Äôt trick coins, we assume that \\(\\frac{P(h_1)}{P(h_2)} = 999\\), indicating a very strong (999 to 1) prior probability in favor of fair coins. We can now compute the posterior odds, the ratio of posterior probabilities for the two hypotheses after observing some data \\(d\\): \\(\\frac{P(h_1|d)}{P(h_2|d)}\\).\nCompute the posterior odds after observing the following sequences of coin flips:\n\nHHTHT\nHHHHH\nHHHHHHHHHH"
  },
  {
    "objectID": "02-bayes.html#solutions",
    "href": "02-bayes.html#solutions",
    "title": "2¬† Bayesian inference",
    "section": "2.5 Solutions",
    "text": "2.5 Solutions\n\n2.5.1 Taxi cabs\nLet \\(h_1\\) be the hypothesis that the taxi is yellow. Let \\(h_2\\) be the hypothesis that the taxi is green. Let data \\(d\\) be the witness report that the taxi was yellow. Given the problem statement, \\(P(h_1) = 0.2\\) and \\(P(h_2) = 0.8\\). The witness is only accurate 75% of the time, so \\(P(d|h1) = 0.75\\) (the witness saw a yellow taxi and correctly identified it) and \\(P(d|h2) = 0.25\\) (the witness saw a green taxi but identified it as yellow). Now we apply Bayes‚Äôs rule:\n\\[\\begin{align}\nP(h_1|d) &= \\frac{P(d|h_1) P(h_1)}{P(d)} \\\\\n&= \\frac{P(d|h_1) P(h_1)}{P(d|h_1) P(h_1) + P(d|h_2) P(h_2)} \\\\\n&= \\frac{(0.75) (0.2)}{(0.75)(0.2) + (0.25)(0.8)} \\approx 0.43\n\\end{align}\\]\nBecause yellow cabs are rare (have low prior probability), it is actually more probable that the cab was green, even though the witness is 75% accurate.\n\n\n2.5.2 Flipping coins\n\nHHTHT\n\\[\n\\begin{align}\n\\frac{P(h_1|d)}{P(h_2|d)} &= \\frac{P(d|h_1)}{P(d|h_2)} \\frac{P(h_1)}{P(h_2)} \\\\\n&= \\frac{(1/2)^5}{0} \\times 999 = \\inf\n\\end{align}\n\\] This sequence isn‚Äôt even possible under \\(h_2\\) so we have infinite evidence in favor of \\(h_1\\).\n\n\nHHHHH\n\\[\n\\begin{align}\n\\frac{P(h_1|d)}{P(h_2|d)} &= \\frac{P(d|h_1)}{P(d|h_2)} \\frac{P(h_1)}{P(h_2)} \\\\\n&= \\frac{(1/2)^5}{1^5} \\times 999 = 31.2\n\\end{align}\n\\]\nThis sequence favors \\(h_1\\) by a factor of about 31. Even five heads in a row can‚Äôt overcome our strong prior favoring \\(h_1\\).\n\n\nHHHHHHHHHH\n\\[\n\\begin{align}\n\\frac{P(h_1|d)}{P(h_2|d)} &= \\frac{P(d|h_1)}{P(d|h_2)} \\frac{P(h_1)}{P(h_2)} \\\\\n&= \\frac{(1/2)^{10}}{1^{10}} \\times 999 = 0.98\n\\end{align}\n\\]\nNow the evidence favors \\(h_2\\) (trick coin) just barely."
  },
  {
    "objectID": "03-generalization.html#hormones",
    "href": "03-generalization.html#hormones",
    "title": "3¬† Generalization",
    "section": "3.1 Healthy hormone levels üíâ",
    "text": "3.1 Healthy hormone levels üíâ\nThe following example comes from a 2001 paper by Josh Tenenbaum and Thomas Griffiths1\nThe basic problem: You learn the value of a healthy hormone level (say, 60) that varies on a scale from 1 to 100 (integers only). What is the probability that another value (say, 70) is also healthy?\n\n3.1.1 Setting up a model\n\n3.1.1.1 The hypothesis space\nTo start with, we‚Äôll assume that healthy values lie in a contiguous interval. Using the term from the paper, this interval is the consequential region \\(C\\).\nThe hypothesis space consists of all possible consequential regions. For example, [0,100], [10,19], and [44,45], are all valid hypotheses. The full hypothesis space is every valid interval between 0 and 100.\n\n\n3.1.1.2 Prior\nHow much weight should we assign to each hypothesis? We might have reason to favor shorter intervals over longer ones, for example. In the paper, they use an Erlang prior. Alternatively, for simplicity of calculation, we could again assume a uniform prior distribution, placing equal weight on all hypotheses, like we did in the previous chapter. This is tantamount to making no prior assumptions about which intervals are most probable.\n\n\n3.1.1.3 Likelihood\nSuppose you learn that a healthy patient has a hormone level of 60. What was the likelihood of observing this value, assuming we know which hypothesis is correct? That is, what is \\(P(x = 60 | h)\\). It depends on how we assume the patient was chosen.\n\n3.1.1.3.1 Weak vs.¬†strong sampling\nUnder weak sampling, we assume that each observation was sampled from the full range of possibilities, and it was just a coincidence that we happened to get one from the consequential region (a healthy patient). If that‚Äôs true, then the probability of getting any particular value doesn‚Äôt depend on which hypothesis is true:\n\\[\nP(x|h) = \\frac{1}{L}\n\\]\nwhere \\(L\\) is the length of the range of possible values (100 in our case).\nUnder strong sampling, we assume that each observation was specifically chosen as an example of the consequential region \\(C\\). In other words, someone chose a healthy person and tested their hormone levels as an example for you. In this case, the probability of seeing a particular value depends on the size of the region:\n\\[\nP(x|h) = \\begin{cases}\n  \\frac{1}{|h|} & \\text{if } x \\in h \\\\\n  0 & \\text{otherwise}\n  \\end{cases}\n\\] where \\(|h|\\) is the size of \\(h\\), i.e., the number of values contained in \\(h\\). If you have multiple observations \\(X = \\{x_1, x_2, \\ldots, x_n \\}\\), then \\(P(X|h) = (1/|h|)^n\\). This is because we will assume each sample is independent like a coin flip.\nThe result of the strong sampling assumption is the size principle: among hypotheses that include all of the observed examples, those that are smallest will receive higher posterior probability because they will have higher likelihoods.\n\n\n\nSample hypotheses. The thickness of the lines indicates their likelihood, depicting the size principle. Image from Griffiths & Tenenbaum (2001).\n\n\n\n\n\n3.1.1.4 Posterior\nWe can now simply apply Bayes‚Äôs rule to compute the probability of each hypothesis, given an observation (or set of observations).\n\\[\nP(h|X) = \\frac{P(X|h) P(h)}{\\sum_{h_i} P(X|h_i) P(h_i)}\n\\]\n\n\n\n3.1.2 Generalizing\nWe aren‚Äôt quite finished. Remember that we really want to know the probability of some new value \\(y\\) also being a healthy hormone level. But at this point all we have done is assign a probability to each interval being the consequential region.\nWhat we want to do is essentially a two-step process:\n\nFor each hypothesis \\(h\\), check to see if \\(y\\) is in it.\nIf it is, check how probable it is that \\(h\\) is the consequential region \\(C\\), given our observations \\(X\\).\n\nThis basic idea is sometimes known as hypothesis averaging because we don‚Äôt actually care which hypothesis is the right one, so we‚Äôll just average over all hypotheses, weighted by how probable they are. Specifically, we‚Äôll compute:\n\\[\nP(y \\in C|X) = \\sum_h P(y \\in C | h) P(h | X)\n\\]\nThe second term on the right is what we computed earlier using Bayes‚Äôs rule.\nWhat about the first term? This time, we‚Äôll assume weak sampling because there‚Äôs no reason to assume that this new value \\(y\\) was chosen to be a healthy value or not.\n\n\n3.1.3 Homework 2: Finish the details\n\n\n\n\n\nI haven‚Äôt provided all the details for this model because your assignment is to finish the implementation yourself, run some simulations, and collect a small amount of real data to compare the model to."
  },
  {
    "objectID": "03-generalization.html#the-number-game",
    "href": "03-generalization.html#the-number-game",
    "title": "3¬† Generalization",
    "section": "3.2 The number game üî¢",
    "text": "3.2 The number game üî¢\nIn most domains, requiring concepts to be restricted to contiguous intervals is not realistic. Numbers are one example. Consider the space of possible number concepts you could make up for integers between 1 to 100. In addition to concepts like ‚Äúnumbers between 20 and 50‚Äù, there are many other plausible concepts like ‚Äúmultiples of 10‚Äù, ‚Äúeven numbers‚Äù, or ‚Äúpowers of 3‚Äù.\nConsider the following problem: You are given one or more examples \\(X\\) of numbers that fit some rule and you want to know how probable it is that a new number \\(y\\) also fits the rule.\nThe model we discussed before can be naturally extended to this problem. For the likelihood, we can make the same strong sampling assumption as before.\nThe prior is where things get a little trickier. Intuitively, some concepts like ‚Äúeven numbers‚Äù seem more probable than concepts like ‚Äúmultiples of 7‚Äù. This now becomes a psychological question: Which rules will people find to be more intuitively plausible? There is no single way to decide this, but we could run a survey to find out: Give people a long list of rules and ask them to judge how intuitively natural they seem. We could then construct a prior probability distribution using this data.\nAlternatively, we could come up with some definition of ‚Äúcomplexity‚Äù in hypotheses and assume that less complex hypotheses will receive higher prior probability.\nOnce we have chosen a prior probability distribution \\(P(h)\\), we can now proceed just as we did before."
  },
  {
    "objectID": "03-generalization.html#inductive-generalizations-about-animal-properties",
    "href": "03-generalization.html#inductive-generalizations-about-animal-properties",
    "title": "3¬† Generalization",
    "section": "3.3 Inductive generalizations about animal properties üê¥",
    "text": "3.3 Inductive generalizations about animal properties üê¥\nNow let‚Äôs consider an even more complex generalization problem, based on a 2002 paper by Neville Sanjana and Josh Tenenbaum2. This is the problem of generalizing properties from a set of example animals to other animals. The paper uses the following example:\nChimps have blicketitis\nSquirrels have blicketitis\n--------------------------\nHorses have blicketitis\nThe way to read this is as follows: The premises state that chimps and squirrels have blicketitis. The conclusion is that horses have blicketitis. The inductive generalization question is how probable is the conclusion given the premises? Intuitively, the conclusion in this example seems more plausible than the conclusion in the following example:\nChimps have blicketitis\nGorillas have blicketitis\n--------------------------\nHorses have blicketitis\nThe interesting psychological question is why is it that some generalizations seem more intuitively plausible than others?\n\n3.3.1 Hypothesis space\nThis problem is conceptually similar to the ones we‚Äôve already been discussing. You want to infer what animals have blicketitis after seeing some examples of animals that blicketitis. The first question to answer is: What is the analogue of a consequential region for this problem? Animals don‚Äôt naturally fall on a one-dimensional interval so we‚Äôll need to define a different hypothesis space. One possibility is a hierarchy, which naturally captures the knowledge people have about animals.\n\n3.3.1.1 Clustering\nThe paper first creates a hierarchy of eight animals using similarity data collected from people. Specifically, they asked people to judge how similar all pairs of eight animals were and then calculated the average similarity judgment for each animal.\nThese similarity judgments can be used to construct a tree using a simple clustering algorithm. The algorithm works as follows:\n\nPut all animals in their own cluster.\nWhere there is more than one cluster that hasn‚Äôt been placed in a group, do the following:\n\nIdentify the pair of clusters with the greatest similarity between them.\nGroup those clusters into their own new cluster.\n\n\nThere are several approaches for computing the similarity between two clusters that contain multiple animals. For example, you might use the maximum similarity between any pair of individual animals in the two clusters.\nThe results of this algorithm can be represented as a tree, shown below. Each node in the tree represents a cluster. The hypotheses we will consider will be any combination of 1, 2, or 3 of the clusters determined using the clustering algorithm.\n\n\n\nThe tree of animal species. Image from Sanjana & Tenenbaum (2002).\n\n\n\n\n\n3.3.2 The model\nWe can now define the model. First, let‚Äôs define \\(P(h)\\) where \\(h\\) is a set of clusters. The authors make an assumption analogous to the following:\n\\[\nP(h) \\propto \\frac{1}{\\phi^k}\n\\] where \\(k\\) is the number of clusters in \\(h\\). \\(\\phi\\) is a parameter that we can choose. As long as \\(\\phi &gt; 1\\), \\(P(h)\\) will be smaller for hypotheses consisting of more clusters. This has the effect of assigning higher weight to ‚Äúsimpler‚Äù hypotheses.\nOnce again, we will make the strong sampling assumption for the likelihood. This time, \\(|h|\\) is the number of animal species in \\(h\\) and \\(n\\) is the number of examples in the premises.\nA consequence of both of these assumptions is something like Occam‚Äôs razor which says that the simplest explanation should be preferred. This model will assign 0 likelihood to any hypotheses that don‚Äôt include \\(X\\), thus narrowing the hypothesis space down to just hypotheses that are possible. Of those, it will favor hypotheses with fewer clusters and with fewer animals. In other words, it will favor the simplest hypotheses that are consistent with the examples we‚Äôve seen.\n\n\n3.3.3 Try out the model yourself\nYou can try out a running version of the model by making a copy of the code here. Here, let‚Äôs look at how the model handles a few specific cases.\nLet‚Äôs start with a single example: horses can get blicketitis.\n\nanimalGeneralization([\"horse\"])\n\n\n\n\narray([1.        , 0.52984834, 0.30628906, 0.30628906, 0.19579428,\n       0.19579428, 0.12893614, 0.12893614, 0.0842413 , 0.0842413 ])\n\n\nHere, we see a standard generalization curve. Now, let‚Äôs add a few more animals.\n\nanimalGeneralization([\"horse\", \"cow\", \"mouse\"])\n\n\n\n\narray([1.        , 1.        , 0.57379051, 0.57379051, 0.47852518,\n       0.47852518, 1.        , 0.60980466, 0.1707609 , 0.1707609 ])\n\n\nNow the model has increased probability for most other animals. This makes sense because there is more reason to think blicketitis might affect lots of different animals.\n\nanimalGeneralization([\"horse\", \"cow\", \"mouse\", \"squirrel\"])\n\n\n\n\narray([1.        , 1.        , 0.64983602, 0.64983602, 0.58531333,\n       0.58531333, 1.        , 1.        , 0.18405475, 0.18405475])\n\n\nAdding a squirrel further supported this idea. But what if we add additional examples of animals we‚Äôve already seen?\n\nanimalGeneralization([\"horse\", \"cow\", \"mouse\", \"squirrel\", \"horse\", \"squirrel\"])\n\n\n\n\narray([1.        , 1.        , 0.32551484, 0.32551484, 0.26938358,\n       0.26938358, 1.        , 1.        , 0.06883892, 0.06883892])\n\n\nNow the probabilities for other animals drop, because it‚Äôs starting to look like maybe this disease only affects the four animals we‚Äôve seen so far.\nTo sum up, when we‚Äôve seen a small number of examples, like a single horse, the model will generally prefer simpler hypotheses. But once we‚Äôve seen more data, it will favor more complex hypotheses (like a group of animals from two separate evolutionary clusters) if the data support it.\nAs one final example, let‚Äôs look at the impact of multiple examples of a single animal.\n\nanimalGeneralization([\"gorilla\"])\nanimalGeneralization([\"gorilla\", \"gorilla\"])\nanimalGeneralization([\"gorilla\", \"gorilla\", \"gorilla\"])\n\n\n\n\n\n\n\n\n\n\narray([0.01730482, 0.01730482, 0.01730482, 0.01730482, 0.1259028 ,\n       1.        , 0.01270155, 0.01270155, 0.01111944, 0.01111944])\n\n\nHere, we see that the model becomes increasingly confident that the property is unique to gorillas. This makes intuitive sense and it‚Äôs something that people seem to exhibit in their judgments. But, as they point out in the paper, it‚Äôs not something that non-probabilistic models can easily explain.\n\n\n3.3.4 Results\nThe results in the paper show that this model predicts people‚Äôs judgments quite well, better than alternative models that do not rely on Bayesian inference. These results suggest that the assumptions of the model are very similar to the assumptions that people make when making inductive generalizations.\n\n\n\nComparison between model results and human judgments. Image from Sanjana & Tenenbaum (2002)."
  },
  {
    "objectID": "03-generalization.html#footnotes",
    "href": "03-generalization.html#footnotes",
    "title": "3¬† Generalization",
    "section": "",
    "text": "Tenenbaum, J. B. & Griffiths, T. L. (2001). Generalization, similarity, and Bayesian inference. Behavioral and Brain Sciences, 24, 629-640.‚Ü©Ô∏é\nSanjana, N. & Tenenbaum, J. (2002). Bayesian models of inductive generalization. Advances in Neural Information Processing Systems 15‚Ü©Ô∏é"
  },
  {
    "objectID": "02-bayes.html#footnotes",
    "href": "02-bayes.html#footnotes",
    "title": "2¬† Bayesian inference",
    "section": "",
    "text": "After the protracted and aptly named ‚Äúcolor wars‚Äù of 2113-2114, the previously dominant (and ruthless) orange cabs were wiped from existence. For the purposes of this problem, you may ignore them.‚Ü©Ô∏é"
  },
  {
    "objectID": "04-categorization.html#a-typical-category-learning-experiment",
    "href": "04-categorization.html#a-typical-category-learning-experiment",
    "title": "4¬† Categorization",
    "section": "4.1 A typical category learning experiment üü¢üü®",
    "text": "4.1 A typical category learning experiment üü¢üü®\nIn psychology, category learning experiments have a pretty similar structure. People see some unfamiliar stimuli that differ on several dimensions and can be sorted into different categories. The task is to learn what distinguishes one category from another. The experiments usually consist of two phases: a training phase, and a testing phase.\n\nTraining phase: People see many examples of each category and have to classify them, often simply by guessing at first. They get feedback and gradually learn to tell the different categories apart.\nTesting phase: People see more examples, usually some brand new ones, and have to classify them again. This time they don‚Äôt get feedback. The point of this phase is to test what people actually learned about the meaning of the categories.\n\nThe stimuli could be abstract shapes, cartoon insects, race cars with different features, or really anything. What matters is that they have clearly distinguishable features."
  },
  {
    "objectID": "04-categorization.html#representing-stimuli-in-a-model",
    "href": "04-categorization.html#representing-stimuli-in-a-model",
    "title": "4¬† Categorization",
    "section": "4.2 Representing stimuli in a model üß©",
    "text": "4.2 Representing stimuli in a model üß©\nFor modeling purposes, all of these types of stimuli (with discrete-valued features) can be represented in a matrix, with the following properties:\n\nEach dimension of the matrix represents a different feature of the stimuli.\nThe length of each dimension represents how many different values that feature can have.\nA single item can then be represented by a binary matrix with 1s in the cells indicating which feature values it has and 0s everywhere else.\n\nFor example, in Robert Nosofsky‚Äôs (1986) test of the GCM1, the stimuli were semicircles with lines through them. The two feature dimensions were (1) circle size and (2) line orientation. Each feature had four possible values.\nUsing our binary matrix representation, here is one possible stimulus from the Nosofsky experiment:\n\nimport numpy as np\n\nstimulus = np.array([[0, 0, 0, 0],\n                     [0, 0, 0, 0],\n                     [0, 0, 1, 0],\n                     [0, 0, 0, 0]], dtype=int)\nprint(stimulus)\n\n[[0 0 0 0]\n [0 0 0 0]\n [0 0 1 0]\n [0 0 0 0]]\n\n\nThe Nosofksy stimuli are pretty simple: they can have exactly one value on each dimension, so this representation will always only have a single 1 in the matrix.\nIf we wanted to know how many of each stimulus to present our model, it would be pretty cumbersome to maintain a long list of arrays like this one. So we could instead maintain a single matrix that stores counts of stimuli. That is, the values at each element will represent the number of stimuli observed with those feature values.\nFor example, let‚Äôs consider the ‚Äúdimensional‚Äù condition of Nosofsky‚Äôs experiment, in which subjects saw only the stimuli on the ‚Äúdiagonals‚Äù of the matrix. Let‚Äôs assume they saw each one 100 times (actually, they did a mind-numbing 1200 trials after 2600 practice trials ü•¥). We could represent that like so:\n\ntraining_set = np.array([[100, 0, 0, 100],\n                         [0, 100, 100, 0],\n                         [0, 100, 100, 0],\n                         [100, 0, 0, 100]], dtype=int)\nprint(training_set)\n\n[[100   0   0 100]\n [  0 100 100   0]\n [  0 100 100   0]\n [100   0   0 100]]\n\n\nThis still isn‚Äôt ideal though. Because it doesn‚Äôt represent the category labels that subjects got during their training. We don‚Äôt know which examples belonged to which categories.\nThinking ahead to how the GCM works, let‚Äôs instead represent the training examples more like a list, so that we can iterate through them:\n\ntDimensional = np.zeros((8,3), dtype=int)\ntDimensional[0] = [1,1,1]\ntDimensional[1] = [1,2,2]\ntDimensional[2] = [1,3,2]\ntDimensional[3] = [1,4,1]\ntDimensional[4] = [2,1,4]\ntDimensional[5] = [2,2,3]\ntDimensional[6] = [2,3,3]\ntDimensional[7] = [2,4,4]\n\nprint(tDimensional)\n\n[[1 1 1]\n [1 2 2]\n [1 3 2]\n [1 4 1]\n [2 1 4]\n [2 2 3]\n [2 3 3]\n [2 4 4]]\n\n\nIn this format, each row of the matrix is an array with three elements. The first element is the category label: 1 or 2. The second two elements are the indices into the stimulus matrix. Note: Here I‚Äôve deviated from Python norms to be consistent with the numbering in the table in the original Nosofsky paper, shown below. When working with the model, remember that indexing in Python starts with 0.\n\n\n\nTraining examples from the dimensional condition. Image from Nosofsky (1986)."
  },
  {
    "objectID": "04-categorization.html#the-generalized-context-model-gcm",
    "href": "04-categorization.html#the-generalized-context-model-gcm",
    "title": "4¬† Categorization",
    "section": "4.3 The Generalized Context Model (GCM)",
    "text": "4.3 The Generalized Context Model (GCM)\nHere, I will describe a special case of the model here that applies to tasks (like the one in the paper) with only two categories. However, this model can easily be extended to any number of categories. The model is defined by two key equations.\nThe first equation defines the probability of classifying Stimulus \\(S_i\\) into Category \\(C_1\\) (i.e., Category 1):\n\\[\n\\begin{equation}\nP(C_1|S_i) = \\frac{b_1 \\sum_{j \\in C_1} \\eta_{ij}}{b_1 \\sum_{j \\in C_1} \\eta_{ij} + (1-b_1) \\sum_{k \\in C_2} \\eta_{ik}}\n\\end{equation}\n\\tag{4.1}\\]\nThe sum \\(\\sum_{j \\in C_1}\\) is a sum over all stimuli \\(S_j\\) that belong to Category 1. The sum \\(\\sum_{j \\in C_1}\\) is a sum over all stimuli \\(S_k\\) that belong to Category 2.\nEquation¬†4.1 has a parameter \\(b_1\\) which is defined as the response bias for Category 1. \\(b_1\\) can range from 0 to 1 and captures the possibility that a subject is biased to respond with one category or another. This is similar but not identical to a prior distribution. When \\(b_1\\) is small, the model is biased toward Category 2; when \\(b_1\\) is large, the model is biased toward Category 1. Note that when \\(b = 0.5\\), it cancels out of the equation.\nWe could also include a response bias for Category 2, but the model assumes that \\(\\sum_i b_i = 1\\). Therefore, when there are only two categories, \\(b_2 = 1 - b_1\\) which is what you see in the second term of the denominator of Equation¬†4.1.\n\\(\\eta_{ij}\\) is a function that defines how similar to \\(S_i\\) and \\(S_j\\) are. The GCM assumes that stimuli can be represented as points in a multi-dimensional space (in this case, a two-dimensional space) and the similarity is defined as a function of the distance between those two points:\n\\[\n\\eta_{ij} = e^{-c^2 \\left[w_1 (f_{i1} - f_{j1})^2 + (1-w_1)(f_{i2} - f_{j2})^2 \\right]}\n\\]\nwhere \\(f_{i1}\\) and \\(f_{i2}\\) refer to the feature values on dimensions 1 and 2 of \\(S_i\\). This equation has two parameters: \\(c\\) and \\(w_1\\). \\(c\\) is a scaling parameter that affects how steeply the exponential curve is. This will allow us to account for how different people might have different ideas of how close two stimuli have must be to be called similar. \\(w_1\\) is called the attentional weight for dimension 1. This parameter captures how much weight is placed on dimension 1 over dimension 2. Just like \\(b_1\\), \\(w_1\\) can range from 0 to 1, and the larger it is, the more weight is placed on dimension 1. Similarly, we could add a \\(w_2\\), but the attention weights are constrained to sum to 1.\n\n4.3.1 Generating model predictions\nIn order to generate predictions from this model, we need two things. First, we need to define what the training stimuli are, and what their feature values are. This is what will allow us to compute the sums in Equation¬†4.1.\nNext, we need to specify the values of the three parameters \\(b_1\\), \\(c\\), and \\(w_1\\). Nosofsky does this by first collecting data from people in the experiment. That is, for every stimulus \\(S_i\\) we can record the proportion of times people classified it as Category 1. In other words, we can collect empirical estimates of \\(P(C_1|S_i)\\) for all values of \\(S_i\\).\nThen Nosofsky uses a maximum likelihood procedure for fitting the model to the data. Conceptually, the idea is to find the set of values of the three parameters that produce model predictions that are as close as possible to the empirical data. (This is exactly how linear regression works, where you have some data \\((\\bar{x},\\bar{y})\\) and want to find the best-fitting values of \\(a\\) and \\(b\\) for the function \\(y = ax + b\\) that describes the relationship between \\(x\\) and \\(y\\).)\nWe can define model fit by mean squared error (MSE). MSE is defined as\n\\[\n\\frac{1}{n} \\sum_i (y_i - x_i)^2\n\\]\nwhere \\(y\\) are the data and \\(x\\) are the model predictions. A lower MSE means a better model fit. Finding the best values of the parameters can be achieved through an exhaustive search of all values. For example, we might consider all possible values of each parameter in increments of 0.1. We could then write a program that performs the following basic algorithm:\n\nSet minimum MSE to \\(\\inf\\).\nFor all possible values of \\(b_1\\), \\(c\\), and \\(w_1\\):\n\nGenerate model predictions \\(P(R_1|S_i)\\) for all \\(S_i\\).\nCompute MSE between the empirical data and model predictions generated in the previous step.\nIf this MSE is smaller than minimum MSE, set minimum MSE to the current value and store the current values of \\(b_1\\), \\(c\\), and \\(w_1\\).\n\nReturn the stored values of \\(b_1\\), \\(c\\), and \\(w_1\\).\n\n\n\n4.3.2 Cross-validation\nTo avoid over-fitting models to our collected data, best practice is to use an approach that splits the data into training and evaluation (or test) sets and judge performance only on the test sets.\nThe basic idea, known as cross-validation, comes in many forms. The principle is straightforward, but the details are outside the scope of this book. There are existing packages in Python that perform many of the key cross-validation steps automatically.\n\n\n4.3.3 Homework 3\n\n\n\n\n\nIn your next homework you will implement the GCM described above. In the homework, you will have to encode training and test stimuli and implement the equations and model-fitting algorithm above. Additionally, to get some experience with the behavioral side of cognitive science, you will create a working version of a category learning experiment in order to collect some data that you can use to compare with the GCM‚Äôs predictions."
  },
  {
    "objectID": "04-categorization.html#footnotes",
    "href": "04-categorization.html#footnotes",
    "title": "4¬† Categorization",
    "section": "",
    "text": "Nosofsky, R. M. (1986). Attention, similarity, and the identification-categorization relationship. Journal of Experimental Psychology: General, 115(1), 39-57.‚Ü©Ô∏é"
  },
  {
    "objectID": "05-hierarchical-bayes.html#beta-binomial",
    "href": "05-hierarchical-bayes.html#beta-binomial",
    "title": "5¬† Hierarchical generalization",
    "section": "5.1 The Beta-Binomial model ü™ô",
    "text": "5.1 The Beta-Binomial model ü™ô\n\n\n\nPhoto by ZSun Fu on Unsplash.\n\n\nWe can answer this question with a model called the Beta-Binomial model, named for the probability distributions it uses. First, let‚Äôs set up the basic assumptions of the model.\nLet \\(P(\\text{heads}) = \\theta\\). We don‚Äôt know what \\(\\theta\\) is. After observing a sequence of coin flips \\(D\\), we want to estimate \\(\\theta\\). This can be accomplished by directly applying Bayes‚Äôs rule:\n\\[\nP(\\theta|D) = \\frac{P(D|\\theta) P(\\theta)}{P(D)}\n\\]\nThe data \\(D\\) in this case corresponds to the number of \\(k\\) heads out of \\(n\\) total flips. This follows a Binomial distribution, which describes the probability of getting \\(k\\) successes out of \\(n\\) trials, when the probability of success on each trial is \\(\\theta\\). We will define heads as a ‚Äúsuccess‚Äù.\n\\[\n\\begin{align}\nP(D|\\theta) = P(k|\\theta,n) &= \\text{Bin}(k; n, \\theta) \\\\\n&= \\binom{n}{k} \\theta^{k} (1-\\theta)^{n-k}\n\\end{align}\n\\]\nThe notation for the \\(\\text{Bin}(\\cdot)\\) function indicates that this is a distribution over \\(k\\) (number of successes) and the distribution has the parameters \\(n\\) (the total number of trials) and \\(\\theta\\) (the probability of a success on each trial).\nWe can define the prior, \\(P(\\theta)\\), however we like. Because \\(\\theta\\) is a random variable that can take on any value from 0 to 1, we cannot just say \\(P(\\theta) = 0.5\\) like we could in earlier examples. Instead, \\(P(\\theta)\\) must be a probability distribution that assigns probabilities to any value from 0 to 1. If we know nothing about \\(\\theta\\), we could use a Uniform(\\([0,1]\\)) or non-informative prior that assigns equal probability to all values of \\(\\theta\\).\nAlternatively, a convenient choice (for reasons explained below) for \\(P(\\theta)\\) is the Beta distribution:\n\\[\nP(\\theta) = \\text{Beta}(\\theta;\\alpha,\\beta)\n\\]\nThe Beta distribution has two parameters: \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\). Let‚Äôs create a function that will allow us to visualize the Beta distribution.\n\nfrom scipy import stats\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_theme()\n\ndef plot_beta(a, b):\n  x = np.linspace(0,1,num=500)\n  px = stats.beta.pdf(x, a, b)\n  \n  fig, ax = plt.subplots()\n  #ax.plot(x, px)\n  sns.lineplot(x = x, y = px)\n  ax.set_xlabel(r'$\\theta$')\n  plt.show()\n\nplot_beta takes two arguments: a (\\(\\alpha\\)), and b(\\(\\beta\\)) and plots a Beta distribution with those parameter values.\nLet‚Äôs see what it looks like with a few different values.\n\nplot_beta(1,1)\n\n\n\n\nWhen \\(\\alpha = \\beta = 1\\), the Beta distribution is identical to a Uniform(\\([0,1]\\)) distribution.\n\nplot_beta(3,3)\n\n\n\n\nWhen \\(\\alpha\\) and \\(\\beta\\) are greater than 1 and equal, we get a distribution with a peak around 0.5. If we had strong prior expectations that the coin was unbiased, we could increase the parameters even more:\n\nplot_beta(50,50)\n\n\n\n\nWhat about when \\(\\alpha\\) and \\(\\beta\\) are not equal?\n\nplot_beta(4,2)\n\n\n\n\nThis allows us to capture skewed priors, perhaps capturing a belief that the coin has a specific bias.\nNow, what if \\(\\alpha\\) and \\(\\beta\\) are less than 1?\n\nplot_beta(0.5,0.5)\n\n\n\n\nThis might capture the belief that the coin is strongly biased, but we aren‚Äôt sure in which direction.\n\n5.1.1 Conjugate distributions\nThe Beta distribution is the conjugate distribution for the Binomial distribution. This means that when the likelihood is a Binomial distribution and the prior is a Beta distribution, then the posterior is also a Beta distribution. Specifically, after making these assumptions,\n\\[\nP(\\theta|D) = \\text{Beta}(\\theta; \\alpha + k, \\beta + n-k)\n\\]\nThe parameters of the posterior distribution are (1) the sum of \\(\\alpha\\) from the prior and the number of observed heads and (2) the sum of \\(\\beta\\) from the prior and the number of observed tails. This means that the parameters \\(\\alpha\\) and \\(\\beta\\) of the Beta prior have a natural interpretation as ‚Äúvirtual flips‚Äù. For example, the larger \\(\\alpha\\) is compared to \\(\\beta\\), the more biased toward heads we expect \\(\\theta\\) to be. Additionally, the larger \\(\\alpha\\) and \\(\\beta\\) are, the more certain (less diffuse) the prior is.\n\n\n5.1.2 Parameter estimation\nBecause we used a conjugate distribution, we can use our same plot_beta function to generate posterior probability distributions after some coin flips.\nSuppose you start with a fairly strong belief that a coin is fair, represented by this distribution:\n\nplot_beta(30,30)\n\n\n\n\nNow, suppose you flip a coin 20 times and it comes up heads every time. What should you think about the bias of the coin now? According to our model:\n\nplot_beta(30+20,30)\n\n\n\n\nAs you can see, this should cause you to shift your beliefs somewhat.\nThis wasn‚Äôt totally realistic, though. If you picked a coin off the ground, your prior beliefs about it being biased would probably look more like this:\n\nplot_beta(2000,2000)\n\n\n\n\nWhat happens if you now flipped this coin 20 times and it came up heads every time?\n\nplot_beta(2000+20,2000)\n\n\n\n\nYou might be mildly surprised, but those 20 flips wouldn‚Äôt be enough to budge your estimate about the bias of the coin by much.\nFinally, let‚Äôs imagine a situation in which you had a weak prior belief that a coin was biased:\n\nplot_beta(5,1)\n\n\n\n\nNow you flip the coin 100 times and it comes up heads 48 times. What should your updated beliefs be?\n\nplot_beta(5+48,1+52)\n\n\n\n\nAs you can see, the posterior distribution shows that you should think this coin is probably fair now. This illustrates how sufficient evidence can override prior beliefs.\n\n\n5.1.3 Hypothesis averaging\nIn Chapter 3, we solved the generalization problem by summing over all hypotheses, weighted by their posterior probabilities. Here, we can do something similar.\nSuppose we want to know the probability of the next flip coming up heads. In other words, we want to know \\(P(\\text{heads}|D)\\). We can do that by averaging over all possible values of \\(\\theta\\):\n\\[\nP(\\text{heads}|D) = \\int_\\theta P(\\text{heads}|\\theta) \\cdot P(\\theta|D) d\\theta = \\int_\\theta \\theta \\cdot P(\\theta|D) d\\theta\n\\]"
  },
  {
    "objectID": "05-hierarchical-bayes.html#overhypotheses",
    "href": "05-hierarchical-bayes.html#overhypotheses",
    "title": "5¬† Hierarchical generalization",
    "section": "5.2 Overhypotheses üôÜ",
    "text": "5.2 Overhypotheses üôÜ\nNow consider a slightly different situation. You flip 19 different coins in a row, each one time, and they all come up heads. Now you pick up a 20th coin from the same bag as the previous 19 coins. What do you think is the probability of that 20th coin coming up heads? Is it higher than 0.5?\nIf you answered yes, it‚Äôs probably because you formed an overhypothesis about the bias of the coins. After flipping all those coins, you may have concluded that this particular set of coins is more likely than usual to be biased. As a result, your estimate about the probability of the 20th coin coming up heads was higher than it otherwise would be.\n\n5.2.1 The shape bias\nThis coins example is pretty artificial, but the concept of overhypotheses is one that you find in language learning. A phenomenon known as the shape bias refers to the fact that even young children are more likely to generalize a new word based on its shape rather than other properties like color or texture.\n\n\n\nA common task used to test for the shape bias.\n\n\nThis makes sense because objects tend to have common shapes and are less likely to have common colors or textures.\n\n\n5.2.2 Modeling the learning of overhypotheses through hierarchical Bayesian learning\nCharles Kemp, Andy Perfors, and Josh Tenenbaum developed a model of this kind of learning. They focused on bags of black and white marbles rather than flipping coins. They imagine a problem in which you have many bags of marbles that you draw from. After drawing from many bags, you draw a single marble from a new bag and make a prediction about the proportion of black and white marbles in that bag.\nThe details of the model are outside the scope of this book. But the basic idea is that the model learns at two levels simultaneously. At the higher level, the model learns the parameters \\(\\alpha\\) and \\(\\beta\\) of a Beta distribution that characterizes the proportion of black and white marbles in each bag. As we saw above, a Beta distribution can have a peak around a particular proportion, or it can be peaked around both 0 and 1, meaning that each bag is likely to be nearly all black or all white.\nAt the lower level, the model learns the specific distribution of marbles within a bag. If you draw 20 marbles and 5 of them are black, you may have some uncertainty about the overall proportion in the bag, but your best estimate will be around 5/20 (or 1/4).\nWhere the model excels is being able to draw inferences across bags. If you see many bags that are full of only black or only white marbles, and then you draw a single black marble out of a new bag, you are likely to be very confident that the rest of the marbles in that bag are black.\nBut if you see many bags that have mixed proportions of black and white marbles, and then you draw a single black marble out of a new bag, you will be far less confident about the proportion of black marbles in that bag. A model that doesn‚Äôt make inferences at multiple levels would struggle to draw this distinction."
  },
  {
    "objectID": "06-sampling-assumptions.html#word-learning",
    "href": "06-sampling-assumptions.html#word-learning",
    "title": "6¬† Sampling assumptions",
    "section": "6.1 Word learning üí¨",
    "text": "6.1 Word learning üí¨\nSuppose you see the following collection of objects on a table.\n\n\n\nFrom Xu & Tenenbaum (2007).\n\n\nNow consider the following situation:\n\nA teacher picks three blue circles from the pile and calls them ‚Äúwugs‚Äù.\n\nWhat is a wug? Are all circles wugs? Or just blue circles?\nConsider a different situation:\n\nA teacher picks one blue circle from the pile and calls it a ‚Äúwug‚Äù. The teacher then asks a child to choose two more (the child picks two blue circles). The teacher confirms that they are also wugs.\n\nWhat is a wug? Are all circles wugs? Or just blue circles?\nIn both situations, the observed data is the same: three blue circles labeled as wugs. What‚Äôs different is the process by which the data were generated. In one case, a knowledgeable teacher picked all the wugs; in the other, the teacher only picked one example and a (probably not knowledgeable) child picked the others.\nTo use some terms we‚Äôve seen before, the teacher is using strong sampling. The child is using something closer to weak sampling.\n\n\n\nThe hierarchy of objects. From Xu & Tenenbaum (2007).\n\n\nThis is based on an actual study by Fei Xu and Josh Tenenbaum, who also developed a model of the task. They asked whether children and adults would generalize the word ‚Äúwug‚Äù to the basic-level category (circles with lines in them) or the subordinate category (blue circles with lines in them). Here are their results:\n\n\n\nExperimental results from Xu & Tenenbaum (2007).\n\n\nPeople clearly distinguished between the teacher-driven situation and the child-driven (learner-driven) situation, and were much more likely to generalize ‚Äúwugs‚Äù to the subordinate category when the teacher picked the objects."
  },
  {
    "objectID": "06-sampling-assumptions.html#pedagogical-sampling",
    "href": "06-sampling-assumptions.html#pedagogical-sampling",
    "title": "6¬† Sampling assumptions",
    "section": "6.2 Pedagogical sampling üßë‚Äçüè´",
    "text": "6.2 Pedagogical sampling üßë‚Äçüè´\nThe teacher choosing wugs could be said to be using pedagogical sampling. The teacher deliberately used knowledge of the wug concept to select informative examples to help the child learn the concept as quickly as possible.\nThis is an idea explored by Patrick Shafto, Noah Goodman, and Thomas Griffiths (as well as Elizabeth Bonawitz and other researchers in related work). To put it in formal terms:\n\\[\n\\begin{equation}\nP_{\\text{teacher}}(d|h) \\propto (P_{\\text{learner}}(h|d))^\\alpha\n\\end{equation}\n\\tag{6.1}\\]\nHere, \\(\\alpha\\) is a parameter that controls how optimized the teacher‚Äôs choices are. As \\(\\alpha \\rightarrow \\infty\\), the teacher will choose examples \\(d\\) that maximize the learner‚Äôs posterior probability.\nWe can figure out how the learner will update their beliefs about a concept by directly applying Bayes rule:\n\\[\n\\begin{equation}\n  P_{\\text{learner}}(h|d) = \\frac{P_{\\text{teacher}}(d|h)P(h)}{\\sum_{h_i} P_{\\text{teacher}}(d|h_i)P(h_i)}\n\\end{equation}\n\\tag{6.2}\\]\nThis equation makes it clear that the teacher and learner are inextricably linked: The teacher chooses examples to maximize the learner‚Äôs understanding, and the learner updates beliefs based on expectations of how the teacher is choosing examples. The model is recursive.\nAs the researchers explain in their paper, the two equations above are a system of equations that can be rearranged to create the following equation defining how the teacher should choose examples:\n\\[\n\\begin{equation}\n  P_{\\text{teacher}}(d|h) \\propto \\left( \\frac{P_{\\text{teacher}}(d|h) P(h)}{\\sum_{h_i} P_{\\text{teacher}}(d|h_i) P(h_i)} \\right)^\\alpha\n\\end{equation}\n\\tag{6.3}\\]\nHow to solve this equation? In the paper, they use an iterative algorithm, that works like this:\n\nInitialize \\(P_{\\text{teacher}}(d|h)\\) using weak sampling.\nIterate through the following steps until \\(P_{\\text{teacher}}(d|h)\\) stabilizes (doesn‚Äôt change from one iteration to the next):\n\nFor each possible \\(h\\) and \\(d\\), compute \\(P_{\\text{learner}}(h|d)\\) using Equation¬†6.2 and \\(P_{\\text{teacher}}(d|h)\\) values from the previous iteration.\nFor each possible \\(d\\) and \\(h\\), update \\(P_{\\text{teacher}}(d|h)\\) using \\(P_{\\text{learner}}(h|d)\\) values from the previous step, where \\(P_{\\text{teacher}}(d|h_i) = P_{\\text{learner}}(h_i|d) / \\sum_{d_j} P_{\\text{learner}}(h_i|d_j)\\) (Equation¬†6.1).\n\n\n\n6.2.1 The rectangle game\nThe researchers tested out their model in an experiment using a simple task called the rectangle game.\nIn the game, concepts are rectangular boundaries in a two-dimensional space. The teacher knows the boundary and the learner has to figure it out from examples given by the teacher.\n\n\n\nThe rectangle game. From Shafto et al.¬†(2014).\n\n\nIn one version, the teacher can only provide positive examples (‚ÄúHere‚Äôs an example that‚Äôs inside the boundary‚Äù). In another version, the teacher can also provide negative examples (‚ÄúHere‚Äôs an example that‚Äôs not inside the boundary‚Äù).\nIn an experiment, they asked people to play the role of the learner in the rectangle game. There were three conditions:\n\nTeaching-pedagogical learning: People first acted as teachers, then as learners.\nPedagogical learning: People acted just as learners and were told the examples they saw were chosen by a teacher.\nNon-pedagogical learning: People acted just as learners and were told the examples they saw were not chosen by a teacher.\n\nAfter showing people examples, they asked them to draw a rectangle of their best guess about what the boundary was.\nOne key prediction of the model is that the most informative locations for examples under pedagogical sampling are at the corners of the rectangles. To test whether the learners understood this, they measured the proportion of examples that ended up in the corners of the rectangles people drew. The results are shown below.\n\n\n\nResults from Shafto et al.‚Äôs (2014) Experiment 1.\n\n\nAs you can see, when people thought a knowledgeable teacher was providing the examples (the teaching-pedagogical and pedagogical learning conditions), they were much more likely to draw tight boundaries around the examples, compared to when they didn‚Äôt think a teacher was providing the examples (the non-pedagogical learning condition). This suggests that people were indeed sensitive to the process by which the data were generated and incorporated that understanding into their inferences.\n\n\n6.2.2 Why the hypothesis space matters\nWhat happens when teachers and learners have different assumptions about the the problem they‚Äôre looking at?\nIn the rectangle game, it‚Äôs pretty obvious what the space of hypotheses is: it‚Äôs practically pre-specified as part of the game. But in most real-world teaching situations, teachers have much more knowledge not shared by the learners than just the specific concept they are trying to convey. How might this interfere with learning?\nThis was the question that a group of researchers led by Rosie Aboody tested. They applied the model we just looked at to a problem where the space of hypotheses was more ambiguous than in the rectangle game. In a study of 220 people (20 teachers and 200 learners), only about half of the learners managed to fully learn the concept (a rule in this case). They used the model to better understand what was going on.\n\n\n\nFrom Aboody et al.¬†(2023).\n\n\nThey input the teachers‚Äô chosen examples into the model to see if the model successfully learned the correct rule. What they varied was the hypothesis space of possible rules the model considered. When the hypothesis space was small (55 or fewer total rules), the model inferred the correct rule for 75% of the teachers‚Äô sets of examples (by correct, they mean placing 95% or more posterior probability on the correct rule). It wasn‚Äôt until they increased the hypothesis space to 95 total rules that the percentage of times the teachers‚Äô examples led the model to correctly infer the rule dropped to 55%.\nThe researchers concluded from this that the learners were probably considering a larger, more complex set of rules than the teachers were. This misaligned set of assumptions likely caused the low learning rate.\nIn the next chapter, we‚Äôll work through one more application of people‚Äôs sensitivity to different sampling assumptions: understanding the pragmatics of speech."
  },
  {
    "objectID": "07-RSA.html#surprisal",
    "href": "07-RSA.html#surprisal",
    "title": "7¬† Language pragmatics",
    "section": "7.1 Surprisal üòØ",
    "text": "7.1 Surprisal üòØ\nWe‚Äôll focus on one concept from information theory: surprisal. Surprisal captures how unexpected an observation is. Intuitively, the unexpectedness of an observation should depend how probable it is. Namely, the more probable it is, the less unexpected it is.\nWe want a function \\(f(p)\\) that takes a probability and gives us its unexpectedness. Here are some reasonable constraints on \\(f\\):\n\nWhen an observation has probability 0, it is infinitely unexpected: \\(f(0) = \\infty\\).\nWhen an observation has probability 1, it is not unexpected at all: \\(f(1) = 0\\).\nWhen we have two independent observations with probabilities \\(p_1\\) and \\(p_2\\), such that the probability of both occurring is \\(p_1 \\cdot p_2\\), we want the total unexpectedness of observing both to be the sum of the unexpectedness of each observation: \\(f(p_1 \\cdot p_2) = f(p_1) + f(p_2)\\).\n\nA function that satisfies these constraints is:\n\\[\nf(p) = -log(p)\n\\] Let‚Äôs verify.\n\n\\(-log(p) \\rightarrow \\infty\\) as \\(p \\rightarrow 0\\) ‚úÖ\n\\(-log(1) = 0\\) ‚úÖ\n\\(-log(p_1 \\cdot p_2) = -(log(p_1) + log(p_2)) = (-log(p_1))+(-log(p_2))\\) ‚úÖ\n\nHere‚Äôs a plot of this function:\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\np = np.linspace(0.001,1,num=500)\nfp = -np.log(p)\n\nsns.set_theme()\nfig, ax = plt.subplots()\nsns.lineplot(x = p, y = fp)\nax.set_xlabel(\"p\")\nax.set_ylabel(\"surprisal\")\nplt.show()\n\n\n\n\nWhat does surprisal have to do with language?\nWhen you‚Äôre communicating with someone, surprisal can help you decide what information to give the other person. For example, imagine that you observe a number between 1 and 10 and you want to share it with someone else. You could either say:\n\nI saw an even number.\n\nOr you could say:\n\nI saw two.\n\nWhich would you say? The second one, duh. But why?\nIn formal terms, you‚Äôre trying to send a signal such that the number you observed is not surprising for the other person after receiving your signal. That is, you‚Äôre trying to minimize the surprisal of \\(P(2|\\text{signal})\\).\nExercise: Compute the surprisal of each of these two signals."
  },
  {
    "objectID": "07-RSA.html#rsa-model",
    "href": "07-RSA.html#rsa-model",
    "title": "7¬† Language pragmatics",
    "section": "7.2 The Rational Speech Act model üç™",
    "text": "7.2 The Rational Speech Act model üç™\nLet‚Äôs apply the concept of surprisal to the following example. There were initially two cookies in a cookie jar, you may have eaten some, and you‚Äôre telling your friend how many you had. You can say:\n\n‚ÄúI had some cookies.‚Äù (You ate one or two.)\n‚ÄúI had all the cookies.‚Äù (You ate two.)\n‚ÄúI had no cookies.‚Äù (You ate none.)\nNothing ‚Äì stay silent. (You could have eaten no cookies, one cookie, or two cookies.)\n\nLet‚Äôs define this situation using a binary matrix. Each row is a signal (what you say) and each column is a state (how many cookies were eaten).\n\nstate_matrix = np.array([[0,1,1],\n                         [0,0,1],\n                         [1,0,0],\n                         [1,1,1]])\n                         \nsignals = [\"some\", \"all\", \"no\", \"silent\"]\nstates = [\"0\",\"1\",\"2\"]\n\nWe‚Äôll define a function that prints out this matrix as a heat map.\n\ndef print_state_matrix(m, x_labels=None, y_labels=None):\n  fig, ax = plt.subplots()\n  if (x_labels and y_labels):\n    sns.heatmap(m, annot=True, xticklabels=x_labels, yticklabels=y_labels)\n  else:\n    sns.heatmap(m, annot=True)\n  \n  plt.show()\n\n# Print out the matrix\nprint_state_matrix(state_matrix, states, signals)\n\n\n\n\nNow let‚Äôs calculate how a literal listener would interpret each of these statements. Each row in the matrix above (one for each signal) will define a distribution over the states. That is, for state \\(h\\) and signal \\(d\\), we want to compute:\n\\[\nP_{\\text{listener}}(h|d) \\propto P(d|h) P(h)\n\\]\nThe literal listener assumes that, given a state, a signal consistent with that state is chosen at random by the speaker. This is already captured in the matrix above: signals consistent with a state have a value of 1 in their cells and signals inconsistent have a 0. We will also assume that the listener has a uniform prior over states. Therefore, we can calculate the literal listener‚Äôs probabilities by just normalizing the rows of the matrix.\nWe‚Äôll do this more than once, so let‚Äôs write a function to do it.\n\ndef normalize_rows(m):\n  normalized_m = np.zeros((m.shape))\n  row_num = 0\n  for row in m:\n    normalized_m[row_num,:] = row / sum(row)\n    row_num += 1\n    \n  return(normalized_m)\n\n\nliteral_listener = normalize_rows(state_matrix)\nprint_state_matrix(literal_listener, states, signals)\n\n\n\n\nNow let‚Äôs calculate the pragmatic speaker. The pragmatic speaker chooses a signal given a state. Therefore, this time, the columns of the matrix will be probability distributions.\nUnlike the literal listener, the pragmatic speaker is trying to convey some information, so they will choose a signal with the greatest expected utility for the literal listener. Expected utility in this case is defined as the expected increase in understanding about the true state of the world. Remember, we already have a way of quantifying this: surprisal. So we‚Äôll define utility as negative surprisal of the state given the signal from the point of view of the literal listener: \\(-(-log(P_{\\text{listener}}(h|d)))\\).\n\n# First we apply a softmax decision function\nalpha = 1 # this paramater controls how close to maximizing the speaker is\n\n# To avoid an error, let's avoid the cells with P(0)\nnonzero_probs = literal_listener != 0\n\npragmatic_speaker = literal_listener\npragmatic_speaker[nonzero_probs] = np.exp(np.log(literal_listener[nonzero_probs])*alpha)\n\nfor col in range(pragmatic_speaker.shape[1]):\n  pragmatic_speaker[:,col] = (pragmatic_speaker[:,col] /\n    sum(pragmatic_speaker[:,col]))\n\nprint_state_matrix(pragmatic_speaker, states, signals)\n\n\n\n\nIn the above code, I used something known as a ‚Äúsoftmax‚Äù rather than a true maximizing function. The exponential function captures the idea that the speaker will choose probabilistically, generally choosing options with more utility. This is a common assumption in both psychology and economics because we often don‚Äôt have complete information when making decisions (or when modeling other people‚Äôs decisions), so assuming a pure maximizing function isn‚Äôt always the best choice.\nHowever, I also included a parameter alpha that controls how close to maximizing the speaker is. The larger alpha is, the closer the speaker will get to maximizing utility.\nNow we can calculate the probabilities for the pragmatic listener. This will be similar to the literal listener in that the pragmatic listener receives a signal and computes the probability of each state given a signal. The difference is that the pragmatic listener doesn‚Äôt assume the signals are chosen at random, but that the signals are chosen by the pragmatic speaker above.\nWe can compute the pragmatic listener by normalizing the matrix above along the rows.\n\npragmatic_listener = normalize_rows(pragmatic_speaker)\nprint_state_matrix(pragmatic_listener, states, signals)\n\n\n\n\nThe pragmatic listener correctly infers a scalar implicature, a concept from pragmatics in linguistics. Even though ‚ÄúI had some cookies‚Äù is consistent with eating all of the cookies, a pragmatic listener will infer that if a person says they ate some then they did not eat all (otherwise they would have said so)."
  },
  {
    "objectID": "07-RSA.html#does-this-model-match-human-behavior",
    "href": "07-RSA.html#does-this-model-match-human-behavior",
    "title": "7¬† Language pragmatics",
    "section": "7.3 Does this model match human behavior? üîµ",
    "text": "7.3 Does this model match human behavior? üîµ\nIn a word, yes. One of the first papers to look at this was by Michael Frank and Noah Goodman. They applied this model to the simple task of picking out a shape from a small set and tested the model‚Äôs predictions in a behavioral experiment.\n\n7.3.1 The task\n\nThe speaker: ‚ÄúImagine you are talking to someone and you want to refer to the middle object. What word would you use: blue or circle?‚Äù üü¶ üîµ üü©\nThe listener: ‚ÄúImagine someone is talking to you and uses the word blue to refer to one of these objects. Which object are they talking about?‚Äù üü¶ üîµ üü©\n\nIn the task and experiment, the researchers varied the actual set of objects in a systematic way.\n\n\n7.3.2 The model\nTheir model is virtually identical to the cookie jar model we just worked through. They assume that speakers try to choose words to maximize the listener‚Äôs utility, as measured by surprisal.\nA literal listener in this case would assume that any object that is consistent with the speaker‚Äôs chosen word could have been the one they were referring to.\nWith these assumptions, the researchers derive the probability of the speaker choosing each word (details in the paper) as:\n\\[\nP(w|r_s,C) = \\frac{|w|^{-1}}{\\sum_{w^\\prime \\in W} {|w^\\prime|}^{-1}}\n\\]\n\n\\(w\\): The speaker‚Äôs chosen word.\n\\(r_s\\): The object the speaker meant to refer to.\n\\(C\\): The set of objects.\n\\(|w|\\): The number of objects that \\(w\\) could apply to.\n\\(W\\): The set of words that apply to the object that the speaker meant to refer to.\n\nRegarding \\(W\\), imagine that the speaker wanted to refer to the blue circle. In that case, \\(W = \\{ \\text{blue}, \\text{circle} \\}\\), because either word could apply to the object.\nAccording to this model, speakers will tend to choose words that more uniquely identify an object in a set. For example, in the set above, because blue applies to two objects (blue square and blue circle), but circle only applies to one, circle will get higher probability.\n\n\n7.3.3 Homework 4: Implement the model\n\n\n\n\n\nI haven‚Äôt provided all the details for this model because your assignment is to finish the implementation yourself, run some simulations, and collect a small amount of real data to compare the model to."
  }
]