<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 5 Hierarchical generalization | Introduction to Computational Psychology</title>
<meta name="author" content="Alan Jern">
<meta name="description" content="In previous examples, there were always a finite number of hypotheses that we were making inferences about (number of black balls, fair or trick coin, yellow or green taxi). Sometimes, we want to...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 5 Hierarchical generalization | Introduction to Computational Psychology">
<meta property="og:type" content="book">
<meta property="og:url" content="https://alanjern.github.io/computational-psych-book/hierarchical-generalization.html">
<meta property="og:image" content="https://alanjern.github.io/computational-psych-book/images/cover/marble_notext.jpg">
<meta property="og:description" content="In previous examples, there were always a finite number of hypotheses that we were making inferences about (number of black balls, fair or trick coin, yellow or green taxi). Sometimes, we want to...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 5 Hierarchical generalization | Introduction to Computational Psychology">
<meta name="twitter:description" content="In previous examples, there were always a finite number of hypotheses that we were making inferences about (number of black balls, fair or trick coin, yellow or green taxi). Sometimes, we want to...">
<meta name="twitter:image" content="https://alanjern.github.io/computational-psych-book/images/cover/marble_notext.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.9/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><link rel="apple-touch-icon" sizes="180x180" href="images/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicons/favicon-16x16.png">
<link rel="manifest" href="images/favicons/site.webmanifest">
<link rel="shortcut icon" href="images/favicons/favicon.ico">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="images/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Introduction to Computational Psychology</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Why computational modeling?</a></li>
<li><a class="" href="bayes.html"><span class="header-section-number">2</span> Bayesian inference</a></li>
<li><a class="" href="generalization.html"><span class="header-section-number">3</span> Generalization</a></li>
<li><a class="" href="categorization.html"><span class="header-section-number">4</span> Categorization</a></li>
<li><a class="active" href="hierarchical-generalization.html"><span class="header-section-number">5</span> Hierarchical generalization</a></li>
<li><a class="" href="sampling-assumptions.html"><span class="header-section-number">6</span> Sampling assumptions</a></li>
<li><a class="" href="pragmatics.html"><span class="header-section-number">7</span> Language pragmatics</a></li>
<li><a class="" href="social-cognition.html"><span class="header-section-number">8</span> Social cognition</a></li>
<li><a class="" href="iterated-learning.html"><span class="header-section-number">9</span> Iterated learning</a></li>
<li><a class="" href="causal-inference.html"><span class="header-section-number">10</span> Causal inference</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/alanjern/computational-psych-book">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="hierarchical-generalization" class="section level1" number="5">
<h1>
<span class="header-section-number">5</span> Hierarchical generalization<a class="anchor" aria-label="anchor" href="#hierarchical-generalization"><i class="fas fa-link"></i></a>
</h1>
<p>In <a href="bayes.html#bayes">previous examples</a>, there were always a finite number of hypotheses that we were making inferences about (number of black balls, fair or trick coin, yellow or green taxi). Sometimes, we want to consider an infinite set of hypotheses. For example, after flipping a coin, what is the probability of that coin coming up heads? The answer to this question could be any number in the interval [0,1].</p>
<div id="beta-binomial" class="section level2" number="5.1">
<h2>
<span class="header-section-number">5.1</span> The Beta-Binomial model ü™ô<a class="anchor" aria-label="anchor" href="#beta-binomial"><i class="fas fa-link"></i></a>
</h2>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-33"></span>
<img src="images/05/coin-in-hand.jpg" alt="Photo by ZSun Fu on Unsplash." width="75%"><p class="caption">
Figure 5.1: Photo by ZSun Fu on Unsplash.
</p>
</div>
<p>We can answer this question with a model called the Beta-Binomial model, named for the probability distributions it uses. First, let‚Äôs set up the basic assumptions of the model.</p>
<p>Let <span class="math inline">\(P(\text{heads}) = \theta\)</span>. We don‚Äôt know what <span class="math inline">\(\theta\)</span> is. After observing a sequence of coin flips <span class="math inline">\(D\)</span>, we want to estimate <span class="math inline">\(\theta\)</span>. This can be accomplished by directly applying Bayes‚Äôs rule:</p>
<p><span class="math display">\[
P(\theta|D) = \frac{P(D|\theta) P(\theta)}{P(D)}
\]</span></p>
<p>The data <span class="math inline">\(D\)</span> in this case corresponds to the number of <span class="math inline">\(k\)</span> heads out of <span class="math inline">\(n\)</span> total flips. This follows a <a href="https://en.wikipedia.org/wiki/Binomial_distribution">Binomial distribution</a>, which describes the probability of getting <span class="math inline">\(k\)</span> successes out of <span class="math inline">\(n\)</span> trials, when the probability of success on each trial is <span class="math inline">\(\theta\)</span>. We will define heads as a ‚Äúsuccess.‚Äù</p>
<p><span class="math display">\[
\begin{align}
P(D|\theta) = P(k|\theta,n) &amp;= \text{Bin}(k; n, \theta) \\
&amp;= \binom{n}{k} \theta^{k} (1-\theta)^{n-k}
\end{align}
\]</span></p>
<p>The notation for the <span class="math inline">\(\text{Bin}(\cdot)\)</span> function indicates that this is a distribution over <span class="math inline">\(k\)</span> (number of successes) and the distribution has the parameters <span class="math inline">\(n\)</span> (the total number of trials) and <span class="math inline">\(\theta\)</span> (the probability of a success on each trial).</p>
<p>We can define the prior, <span class="math inline">\(P(\theta)\)</span>, however we like. Because <span class="math inline">\(\theta\)</span> is a random variable that can take on any value from 0 to 1, we cannot just say <span class="math inline">\(P(\theta) = 0.5\)</span> like we could in earlier examples. Instead, <span class="math inline">\(P(\theta)\)</span> must be a probability distribution that assigns probabilities to any value from 0 to 1. If we know nothing about <span class="math inline">\(\theta\)</span>, we could use a Uniform(<span class="math inline">\([0,1]\)</span>) or non-informative prior that assigns equal probability to all values of <span class="math inline">\(\theta\)</span>.</p>
<p>Alternatively, a convenient choice (for reasons explained below) for <span class="math inline">\(P(\theta)\)</span> is the Beta distribution:</p>
<p><span class="math display">\[
P(\theta) = \text{Beta}(\theta;\alpha,\beta)
\]</span></p>
<p>The <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a> has two parameters: <span class="math inline">\(\alpha &gt; 0\)</span> and <span class="math inline">\(\beta &gt; 0\)</span>. Let‚Äôs create a function that will allow us to visualize the Beta distribution.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="hierarchical-generalization.html#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb58-2"><a href="hierarchical-generalization.html#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb58-3"><a href="hierarchical-generalization.html#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb58-4"><a href="hierarchical-generalization.html#cb58-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-5"><a href="hierarchical-generalization.html#cb58-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_beta(a, b):</span>
<span id="cb58-6"><a href="hierarchical-generalization.html#cb58-6" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,num<span class="op">=</span><span class="dv">500</span>)</span>
<span id="cb58-7"><a href="hierarchical-generalization.html#cb58-7" aria-hidden="true" tabindex="-1"></a>  px <span class="op">=</span> stats.beta.pdf(x, a, b)</span>
<span id="cb58-8"><a href="hierarchical-generalization.html#cb58-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb58-9"><a href="hierarchical-generalization.html#cb58-9" aria-hidden="true" tabindex="-1"></a>  fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb58-10"><a href="hierarchical-generalization.html#cb58-10" aria-hidden="true" tabindex="-1"></a>  ax.plot(x, px)</span>
<span id="cb58-11"><a href="hierarchical-generalization.html#cb58-11" aria-hidden="true" tabindex="-1"></a>  plt.show()</span></code></pre></div>
<p><code>plot_beta</code> takes two arguments: <code>a</code> (<span class="math inline">\(\alpha\)</span>), and <code>b</code>(<span class="math inline">\(\beta\)</span>) and plots a Beta distribution with those parameter values.</p>
<p>Let‚Äôs see what it looks like with a few different values.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="hierarchical-generalization.html#cb59-1" aria-hidden="true" tabindex="-1"></a>plot_beta(<span class="dv">1</span>,<span class="dv">1</span>)</span></code></pre></div>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-35-1.png" width="672"></div>
<p>When <span class="math inline">\(\alpha = \beta = 1\)</span>, the Beta distribution is identical to a Uniform(<span class="math inline">\([0,1]\)</span>) distribution.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="hierarchical-generalization.html#cb60-1" aria-hidden="true" tabindex="-1"></a>plot_beta(<span class="dv">3</span>,<span class="dv">3</span>)</span></code></pre></div>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-36-3.png" width="672"></div>
<p>When <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are greater than 1 and equal, we get a distribution with a peak around 0.5. If we had strong prior expectations that the coin was unbiased, we could increase the parameters even more:</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="hierarchical-generalization.html#cb61-1" aria-hidden="true" tabindex="-1"></a>plot_beta(<span class="dv">50</span>,<span class="dv">50</span>)</span></code></pre></div>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-37-5.png" width="672"></div>
<p>What about when <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are not equal?</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="hierarchical-generalization.html#cb62-1" aria-hidden="true" tabindex="-1"></a>plot_beta(<span class="dv">4</span>,<span class="dv">2</span>)</span></code></pre></div>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-38-7.png" width="672"></div>
<p>This allows us to capture skewed priors, perhaps capturing a belief that the coin has a specific bias.</p>
<p>Now, what if <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are less than 1?</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="hierarchical-generalization.html#cb63-1" aria-hidden="true" tabindex="-1"></a>plot_beta(<span class="fl">0.5</span>,<span class="fl">0.5</span>)</span></code></pre></div>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-39-9.png" width="672"></div>
<p>This might capture the belief that the coin is strongly biased, but we aren‚Äôt sure in which direction.</p>
<div id="conjugate-distributions" class="section level3" number="5.1.1">
<h3>
<span class="header-section-number">5.1.1</span> Conjugate distributions<a class="anchor" aria-label="anchor" href="#conjugate-distributions"><i class="fas fa-link"></i></a>
</h3>
<p>The Beta distribution is the <em>conjugate distribution</em> for the Binomial distribution. This means that when the likelihood is a Binomial distribution and the prior is a Beta distribution, then the posterior is also a Beta distribution. Specifically, after making these assumptions,</p>
<p><span class="math display">\[
P(\theta|D) = \text{Beta}(\theta; \alpha + k, \beta + n-k)
\]</span></p>
<p>The parameters of the posterior distribution are (1) the sum of <span class="math inline">\(\alpha\)</span> from the prior and the number of observed heads and (2) the sum of <span class="math inline">\(\beta\)</span> from the prior and the number of observed tails. This means that the parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> of the Beta prior have a natural interpretation as ‚Äúvirtual flips.‚Äù For example, the larger <span class="math inline">\(\alpha\)</span> is compared to <span class="math inline">\(\beta\)</span>, the more biased toward heads we expect <span class="math inline">\(\theta\)</span> to be. Additionally, the larger <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are, the more certain (less diffuse) the prior is.</p>
</div>
<div id="parameter-estimation" class="section level3" number="5.1.2">
<h3>
<span class="header-section-number">5.1.2</span> Parameter estimation<a class="anchor" aria-label="anchor" href="#parameter-estimation"><i class="fas fa-link"></i></a>
</h3>
<p>Because we used a conjugate distribution, we can use our same <code>plot_beta</code> function to generate posterior probability distributions after some coin flips.</p>
<p>Suppose we start with a fairly strong belief that a coin is fair, represented by this distribution:</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="hierarchical-generalization.html#cb64-1" aria-hidden="true" tabindex="-1"></a>plot_beta(<span class="dv">30</span>,<span class="dv">30</span>)</span></code></pre></div>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-40-11.png" width="672"></div>
<p>Now, suppose you flip a coin 20 times and it comes up heads every time. What should you think about the bias of the coin now? According to our model:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="hierarchical-generalization.html#cb65-1" aria-hidden="true" tabindex="-1"></a>plot_beta(<span class="dv">30</span><span class="op">+</span><span class="dv">20</span>,<span class="dv">30</span>)</span></code></pre></div>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-41-13.png" width="672"></div>
<p>As you can see, this should cause you to shift your beliefs somewhat.</p>
<p>This wasn‚Äôt totally realistic, though. If you picked a coin off the ground, your prior beliefs about it being biased would probably look more like this:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="hierarchical-generalization.html#cb66-1" aria-hidden="true" tabindex="-1"></a>plot_beta(<span class="dv">2000</span>,<span class="dv">2000</span>)</span></code></pre></div>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-42-15.png" width="672"></div>
<p>What happens if we <em>now</em> flipped this coin 20 times and it came up heads every time?</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb67-1"><a href="hierarchical-generalization.html#cb67-1" aria-hidden="true" tabindex="-1"></a>plot_beta(<span class="dv">2000</span><span class="op">+</span><span class="dv">20</span>,<span class="dv">2000</span>)</span></code></pre></div>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-43-17.png" width="672"></div>
<p>You might be mildly surprised, but those 20 flips wouldn‚Äôt be enough to budge your estimate about the bias of the coin by much.</p>
<p>Finally, let‚Äôs imagine a situation in which you had a weak prior belief that a coin was biased:</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="hierarchical-generalization.html#cb68-1" aria-hidden="true" tabindex="-1"></a>plot_beta(<span class="dv">5</span>,<span class="dv">1</span>)</span></code></pre></div>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-44-19.png" width="672"></div>
<p>Now you flip the coin 100 times and it comes up heads 48 times. What should your updated beliefs be?</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb69-1"><a href="hierarchical-generalization.html#cb69-1" aria-hidden="true" tabindex="-1"></a>plot_beta(<span class="dv">5</span><span class="op">+</span><span class="dv">48</span>,<span class="dv">1</span><span class="op">+</span><span class="dv">52</span>)</span></code></pre></div>
<pre><code>## &lt;string&gt;:5: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).</code></pre>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-45-21.png" width="672"></div>
<p>As you can see, the posterior distribution shows that you should think this coin is probably fair now. This illustrates how sufficient evidence can override prior beliefs.</p>
</div>
<div id="hypothesis-averaging" class="section level3" number="5.1.3">
<h3>
<span class="header-section-number">5.1.3</span> Hypothesis averaging<a class="anchor" aria-label="anchor" href="#hypothesis-averaging"><i class="fas fa-link"></i></a>
</h3>
<p><a href="generalization.html#generalizing">In Chapter 3</a>, we solved the generalization problem by summing over all hypotheses, weighted by their posterior probabilities. Here, we can do something similar.</p>
<p>Suppose we want to know the probability of the next flip coming up heads. In other words, we want to know <span class="math inline">\(P(\text{heads}|D)\)</span>. We can do that by averaging over all possible values of <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
P(\text{heads}|D) = \int_\theta P(\text{heads}|\theta) \cdot P(\theta|D) d\theta = \int_\theta \theta \cdot P(\theta|D) d\theta
\]</span></p>
</div>
</div>
<div id="overhypotheses" class="section level2" number="5.2">
<h2>
<span class="header-section-number">5.2</span> Overhypotheses üôÜ<a class="anchor" aria-label="anchor" href="#overhypotheses"><i class="fas fa-link"></i></a>
</h2>
<p>Now consider a slightly different situation. You flip 19 different coins in a row, each one time, and they all come up heads. Now you pick up a 20th coin from the same bag as the previous 19 coins. What do you think is the probability of that 20th coin coming up heads? Is it higher than 0.5?</p>
<p>If you answered yes, it‚Äôs probably because you formed an <em>overhypothesis</em> about the bias of the coins. After flipping all those coins, you may have concluded that this particular set of coins is more likely than usual to be biased. As a result, your estimate about the probability of the 20th coin coming up heads was higher than it otherwise would be.</p>
<div id="the-shape-bias" class="section level3" number="5.2.1">
<h3>
<span class="header-section-number">5.2.1</span> The shape bias<a class="anchor" aria-label="anchor" href="#the-shape-bias"><i class="fas fa-link"></i></a>
</h3>
<p>This coins example is pretty artificial, but the notion of overhypotheses is one that you find in language learning. A phenomenon known as <a href="https://doi.org/10.1016/0885-2014(88)90014-7">the shape bias</a> refers to the fact that even young children are more likely to generalize a new word based on its shape rather than other properties like color or texture.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-46"></span>
<img src="images/05/shape_bias.gif" alt="A common task used to test for the shape bias."><p class="caption">
Figure 5.2: A common task used to test for the shape bias.
</p>
</div>
<p>This makes sense because objects tend to have common shapes and are less likely to have common colors or textures.</p>
</div>
<div id="modeling-the-learning-of-overhypotheses-through-hierarchical-bayesian-learning" class="section level3" number="5.2.2">
<h3>
<span class="header-section-number">5.2.2</span> Modeling the learning of overhypotheses through hierarchical Bayesian learning<a class="anchor" aria-label="anchor" href="#modeling-the-learning-of-overhypotheses-through-hierarchical-bayesian-learning"><i class="fas fa-link"></i></a>
</h3>
<p>Charles Kemp, Andy Perfors, and Josh Tenenbaum developed <a href="http://www.charleskemp.com/papers/KempPTDevSci.pdf">a model</a> of this kind of learning. They focused on bags of black and white marbles rather than flipping coins. They imagine a problem in which you have many bags of marbles that you draw from. After drawing from many bags, you draw a single marble from a new bag and make a prediction about the proportion of black and white marbles in that bag.</p>
<p>The details of the model are outside the scope of this book. But the basic idea is that the model learns at two levels simultaneously. At the higher level, the model learns the parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> of a Beta distribution that characterizes the proportion of black and white marbles in each bag. As we saw above, a Beta distribution can have a peak around a particular proportion, or it can be peaked around both 0 and 1, meaning that each bag is likely to be nearly all black or all white.</p>
<p>At the lower level, the model learns the specific distribution of marbles within a bag. If you draw 20 marbles and 5 of them are black, you may have some uncertainty about the overall proportion in the bag, but your best estimate will be around 5/20 or 1/4.</p>
<p>Where the model excels is being able to draw inferences <em>across</em> bags. If you see many bags that are full of <em>only</em> black or <em>only</em> white marbles, and then you draw a single black marble out of a new bag, you are likely to be very confident that the rest of the marbles in that bag are black.</p>
<p>But if you see many bags that have mixed proportions of black and white marbles, and then you draw a single black marble out of a new bag, you will be far less confident about the proportion of black marbles in that bag. A model that doesn‚Äôt make inferences at multiple levels would struggle to draw this distinction.</p>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="categorization.html"><span class="header-section-number">4</span> Categorization</a></div>
<div class="next"><a href="sampling-assumptions.html"><span class="header-section-number">6</span> Sampling assumptions</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#hierarchical-generalization"><span class="header-section-number">5</span> Hierarchical generalization</a></li>
<li>
<a class="nav-link" href="#beta-binomial"><span class="header-section-number">5.1</span> The Beta-Binomial model ü™ô</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#conjugate-distributions"><span class="header-section-number">5.1.1</span> Conjugate distributions</a></li>
<li><a class="nav-link" href="#parameter-estimation"><span class="header-section-number">5.1.2</span> Parameter estimation</a></li>
<li><a class="nav-link" href="#hypothesis-averaging"><span class="header-section-number">5.1.3</span> Hypothesis averaging</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#overhypotheses"><span class="header-section-number">5.2</span> Overhypotheses üôÜ</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#the-shape-bias"><span class="header-section-number">5.2.1</span> The shape bias</a></li>
<li><a class="nav-link" href="#modeling-the-learning-of-overhypotheses-through-hierarchical-bayesian-learning"><span class="header-section-number">5.2.2</span> Modeling the learning of overhypotheses through hierarchical Bayesian learning</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/alanjern/computational-psych-book/blob/main/05-hierarchical-bayes.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/alanjern/computational-psych-book/edit/main/05-hierarchical-bayes.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Introduction to Computational Psychology</strong>" was written by Alan Jern. It was last built on 2022-05-04.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
